# 第七课：关键词检索优化 - 学生实验指导

## 🎯 实验目标

通过本实验，你将学会：
1. 在lesson06基础上添加PostgreSQL全文检索功能
2. 使用jieba进行中文分词处理
3. 实现基础的关键词搜索接口
4. 理解关键词检索与向量检索的区别

**实验时长**：15分钟

---

## 📋 环境准备

### 步骤1：切换到lesson07分支

```bash
# 进入rag-system项目目录
cd /Users/richzhao/dev/llm_courses/courses/8_llm_in_action/unit6_RAG实战/rag-system

# 切换到lesson07分支（基于lesson06）
git checkout lesson07

# 验证当前分支
git branch
# 应该显示 * lesson07
```

### 步骤2：安装jieba分词库

```bash
# 激活虚拟环境
source .venv/bin/activate

# 安装jieba
pip install jieba==0.42.1

# 验证安装
python -c "import jieba; print('jieba安装成功')"
```

---

## 🔧 实验一：数据库扩展（5分钟）

### 步骤1：添加全文检索字段

连接到PostgreSQL数据库：
```bash
# 启动数据库服务（如果未启动）
docker-compose up -d

# 连接数据库
psql -h localhost -p 5432 -U rag_user -d rag_db
```

在数据库中执行：
```sql
-- 1. 查看现有文档
SELECT id, title, LEFT(content, 50) as preview FROM documents LIMIT 3;

-- 2. 添加全文检索字段
ALTER TABLE documents ADD COLUMN IF NOT EXISTS content_vector tsvector;

-- 3. 更新现有数据的向量字段
UPDATE documents 
SET content_vector = to_tsvector('simple', COALESCE(title, '') || ' ' || COALESCE(content, ''));

-- 4. 创建GIN索引
CREATE INDEX IF NOT EXISTS idx_content_vector ON documents USING gin(content_vector);

-- 5. 验证设置
SELECT id, title, content_vector FROM documents LIMIT 1;
```

---

## 🔧 实验二：中文分词测试（5分钟）

### 步骤1：创建分词测试脚本

创建文件 `test_jieba.py`：
```python
import jieba

def test_segmentation():
    """测试jieba分词效果"""
    test_texts = [
        "人工智能和机器学习",
        "PostgreSQL数据库全文检索",
        "RAG系统向量检索技术"
    ]
    
    print("🔍 jieba分词测试")
    print("=" * 40)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n测试 {i}: {text}")
        
        # 精确模式
        words1 = jieba.lcut(text)
        print(f"精确模式: {' / '.join(words1)}")
        
        # 搜索模式
        words2 = jieba.lcut_for_search(text)
        print(f"搜索模式: {' / '.join(words2)}")
        
        # 为PostgreSQL准备的文本
        search_ready = ' '.join(words2)
        print(f"搜索文本: {search_ready}")

if __name__ == "__main__":
    test_segmentation()
```

运行测试：
```bash
python test_jieba.py
```

---

## 🔧 实验三：关键词搜索实现（5分钟）

### 步骤1：创建搜索函数

创建文件 `keyword_search.py`：
```python
import jieba
import psycopg2
from typing import List, Dict

# 数据库连接配置
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'rag_db',
    'user': 'rag_user',
    'password': 'rag_password'
}

def preprocess_query(query: str) -> str:
    """预处理查询文本"""
    # 使用jieba分词
    words = jieba.lcut_for_search(query)
    
    # 过滤空词和单字符
    filtered_words = [w.strip() for w in words if len(w.strip()) > 1]
    
    # 构建tsquery格式
    if not filtered_words:
        return ''
    
    # 使用 & 连接（AND操作）
    tsquery_parts = [f"'{word}':*" for word in filtered_words]
    return ' & '.join(tsquery_parts)

def keyword_search(query: str, limit: int = 5) -> List[Dict]:
    """执行关键词搜索"""
    # 预处理查询
    processed_query = preprocess_query(query)
    if not processed_query:
        return []
    
    # 连接数据库
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    try:
        # 执行搜索
        sql = """
        SELECT 
            id, title, content,
            ts_rank(content_vector, to_tsquery('simple', %s)) as score
        FROM documents 
        WHERE content_vector @@ to_tsquery('simple', %s)
        ORDER BY score DESC
        LIMIT %s
        """
        
        cursor.execute(sql, (processed_query, processed_query, limit))
        results = cursor.fetchall()
        
        # 格式化结果
        formatted_results = []
        for row in results:
            formatted_results.append({
                'id': row[0],
                'title': row[1],
                'content': row[2][:200] + '...' if len(row[2]) > 200 else row[2],
                'score': round(row[3], 4)
            })
        
        return formatted_results
    
    finally:
        cursor.close()
        conn.close()

def test_search():
    """测试搜索功能"""
    test_queries = [
        "Python",
        "数据库",
        "机器学习"
    ]
    
    print("🔍 关键词搜索测试")
    print("=" * 40)
    
    for query in test_queries:
        print(f"\n搜索: {query}")
        print("-" * 20)
        
        results = keyword_search(query, limit=3)
        
        if results:
            for i, result in enumerate(results, 1):
                print(f"{i}. [{result['score']}] {result['title']}")
                print(f"   {result['content'][:100]}...")
        else:
            print("   无搜索结果")

if __name__ == "__main__":
    test_search()
```

### 步骤2：运行搜索测试

```bash
python keyword_search.py
```

---

## 📝 实验验证

### 验证清单

- [ ] 成功添加了content_vector字段
- [ ] 创建了GIN索引
- [ ] jieba分词正常工作
- [ ] 关键词搜索返回结果
- [ ] 搜索结果按相关性排序

### 测试不同查询

在PostgreSQL中直接测试：
```sql
-- 测试1：英文关键词
SELECT id, title, ts_rank(content_vector, to_tsquery('simple', 'python')) as score
FROM documents 
WHERE content_vector @@ to_tsquery('simple', 'python')
ORDER BY score DESC LIMIT 3;

-- 测试2：中文关键词（需要先分词）
SELECT id, title, ts_rank(content_vector, to_tsquery('simple', '数据库')) as score
FROM documents 
WHERE content_vector @@ to_tsquery('simple', '数据库')
ORDER BY score DESC LIMIT 3;
```

---

## 🔗 与lesson06的对比

### 相同点
- 使用相同的数据库和文档表
- 保持相同的项目结构
- 继续使用Docker服务

### 新增功能
- 添加了PostgreSQL全文检索字段
- 集成了jieba中文分词
- 实现了关键词搜索功能

### 下节课预告
- lesson08将学习如何融合关键词检索和向量检索
- 实现混合检索策略
- 优化搜索结果的准确性

---

## 🤔 思考题

1. 关键词搜索和向量搜索各适合什么场景？
2. 为什么需要对中文进行分词处理？
3. 如何评估搜索结果的质量？

---

## 📚 参考资料

- [PostgreSQL全文检索](https://www.postgresql.org/docs/current/textsearch.html)
- [jieba分词库](https://github.com/fxsjy/jieba)
- [GIN索引说明](https://www.postgresql.org/docs/current/gin.html)