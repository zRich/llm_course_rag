# ç¬¬å››è¯¾ï¼šPDFè§£æä¸Chunkæ‹†åˆ† - å­¦ç”Ÿå®éªŒæŒ‡å¯¼

## ä»£ç åŸºç¡€å‡†å¤‡

åœ¨å¼€å§‹æœ¬èŠ‚è¯¾çš„å®éªŒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åŸºäºä¸Šä¸€èŠ‚è¯¾çš„ä»£ç ç»§ç»­å¼€å‘ã€‚ç°åœ¨æˆ‘ä»¬ä½¿ç”¨Gitåˆ†æ”¯ç®¡ç†æ¥è·å–ä»£ç ã€‚

### æ­¥éª¤1ï¼šè¿›å…¥é¡¹ç›®ç›®å½•å¹¶åˆ‡æ¢åˆ†æ”¯

```bash
# è¿›å…¥rag-systemé¡¹ç›®ç›®å½•
cd rag-system

# åˆ‡æ¢åˆ°lesson04åˆ†æ”¯
git checkout lesson04

# éªŒè¯å½“å‰åˆ†æ”¯
git branch
# åº”è¯¥æ˜¾ç¤º * lesson04
```

### æ­¥éª¤2ï¼šéªŒè¯ä»£ç çŠ¶æ€

```bash
# æ£€æŸ¥é¡¹ç›®ç»“æ„
ls -la
# åº”è¯¥çœ‹åˆ°ï¼šsrc/ scripts/ alembic/ docker-compose.yml ç­‰æ–‡ä»¶å’Œç›®å½•

# æ£€æŸ¥PDFè§£æç›¸å…³æ–‡ä»¶
ls -la src/document/
# åº”è¯¥çœ‹åˆ°ï¼šparser.py pdf_parser.py ç­‰æ–‡ä»¶
```

### æ­¥éª¤3ï¼šéªŒè¯æ•°æ®æ¨¡å‹ç¯å¢ƒ

```bash
# å¯åŠ¨ä¾èµ–æœåŠ¡
docker-compose up -d

# è¿è¡Œæ•°æ®åº“è¿ç§»
alembic upgrade head

# éªŒè¯æ•°æ®æ¨¡å‹
python -c "from src.models import User, Document; print('æ•°æ®æ¨¡å‹å¯¼å…¥æˆåŠŸ')"
```

**è¯´æ˜**ï¼šlesson04åˆ†æ”¯åŒ…å«äº†lesson03çš„æ‰€æœ‰ä»£ç ï¼Œå¹¶æ–°å¢äº†PDFè§£æå’ŒChunkæ‹†åˆ†ç›¸å…³çš„æ¨¡å—å’Œé…ç½®ã€‚

---

## ğŸ¯ å®éªŒç›®æ ‡

é€šè¿‡æœ¬æ¬¡å®éªŒï¼Œä½ å°†å­¦ä¼šï¼š
- ä½¿ç”¨PyMuPDFè§£æPDFæ–‡æ¡£
- æå–æ–‡æ¡£çš„æ–‡æœ¬ã€å›¾ç‰‡å’Œå…ƒæ•°æ®
- å®ç°æ™ºèƒ½çš„æ–‡æ¡£ç»“æ„è¯†åˆ«
- è®¾è®¡å’Œå®ç°å¤šç§Chunkæ‹†åˆ†ç­–ç•¥
- å¤„ç†å¤æ‚æ–‡æ¡£æ ¼å¼å’Œè¾¹ç•Œæƒ…å†µ

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–
```bash
# å®‰è£…æ–‡æ¡£å¤„ç†ç›¸å…³ä¾èµ–
uv add pymupdf python-docx python-pptx
uv add langdetect pillow
uv add nltk spacy

# ä¸‹è½½NLTKæ•°æ®ï¼ˆç”¨äºå¥å­åˆ†å‰²ï¼‰
python -c "import nltk; nltk.download('punkt')"
```

### å‡†å¤‡æµ‹è¯•æ–‡æ¡£
```bash
# åˆ›å»ºæµ‹è¯•æ–‡æ¡£ç›®å½•
mkdir -p test_documents

# å‡†å¤‡ä¸åŒç±»å‹çš„æµ‹è¯•æ–‡æ¡£
# - å­¦æœ¯è®ºæ–‡PDF
# - æŠ€æœ¯æ‰‹å†ŒPDF  
# - Wordæ–‡æ¡£
# - PowerPointæ–‡æ¡£
# - çº¯æ–‡æœ¬æ–‡ä»¶
```

---

## ğŸ”¬ å®éªŒä¸€ï¼šPyMuPDFåŸºç¡€æ“ä½œ

### 1.1 åˆ›å»ºæ–‡æ¡£è§£æå™¨åŸºç±»

**åˆ›å»º `src/document/parser.py`ï¼š**
```python
"""æ–‡æ¡£è§£æå™¨æ¨¡å—"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging
from dataclasses import dataclass
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """æ–‡æ¡£å…ƒæ•°æ®"""
    title: str = ""
    author: str = ""
    subject: str = ""
    creator: str = ""
    producer: str = ""
    creation_date: Optional[datetime] = None
    modification_date: Optional[datetime] = None
    page_count: int = 0
    file_size: int = 0
    file_name: str = ""
    file_extension: str = ""
    language: str = "unknown"
    has_images: bool = False
    has_tables: bool = False
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

@dataclass
class DocumentContent:
    """æ–‡æ¡£å†…å®¹"""
    text: str
    metadata: DocumentMetadata
    pages: List[Dict[str, Any]] = None
    images: List[Dict[str, Any]] = None
    tables: List[Dict[str, Any]] = None
    structure: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.pages is None:
            self.pages = []
        if self.images is None:
            self.images = []
        if self.tables is None:
            self.tables = []
        if self.structure is None:
            self.structure = {}

class DocumentParser(ABC):
    """æ–‡æ¡£è§£æå™¨åŸºç±»"""
    
    def __init__(self):
        self.supported_extensions = set()
    
    @abstractmethod
    def parse(self, file_path: Path) -> DocumentContent:
        """è§£ææ–‡æ¡£"""
        pass
    
    def can_parse(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒè§£æè¯¥æ–‡ä»¶"""
        return file_path.suffix.lower() in self.supported_extensions
    
    def validate_file(self, file_path: Path) -> bool:
        """éªŒè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        if not file_path.exists():
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        if not file_path.is_file():
            logger.error(f"ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {file_path}")
            return False
        
        if file_path.stat().st_size == 0:
            logger.error(f"æ–‡ä»¶ä¸ºç©º: {file_path}")
            return False
        
        return True
```

### 1.2 å®ç°PDFè§£æå™¨

**åˆ›å»º `src/document/pdf_parser.py`ï¼š**
```python
"""PDFæ–‡æ¡£è§£æå™¨"""

import fitz  # PyMuPDF
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
import re

from .parser import DocumentParser, DocumentContent, DocumentMetadata

class PDFParser(DocumentParser):
    """PDFæ–‡æ¡£è§£æå™¨"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = {".pdf"}
    
    def parse(self, file_path: Path) -> DocumentContent:
        """è§£æPDFæ–‡æ¡£"""
        if not self.validate_file(file_path):
            raise ValueError(f"æ— æ•ˆçš„PDFæ–‡ä»¶: {file_path}")
        
        try:
            doc = fitz.open(str(file_path))
            
            # æå–å…ƒæ•°æ®
            metadata = self._extract_metadata(doc, file_path)
            
            # æå–å†…å®¹
            text, pages = self._extract_text_and_pages(doc)
            
            # æå–å›¾ç‰‡
            images = self._extract_images(doc)
            
            # è¯†åˆ«æ–‡æ¡£ç»“æ„
            structure = self._analyze_structure(doc)
            
            # æ£€æµ‹è¡¨æ ¼
            tables = self._detect_tables(doc)
            
            doc.close()
            
            return DocumentContent(
                text=text,
                metadata=metadata,
                pages=pages,
                images=images,
                tables=tables,
                structure=structure
            )
            
        except Exception as e:
            logger.error(f"PDFè§£æå¤±è´¥: {e}")
            raise
    
    def _extract_metadata(self, doc: fitz.Document, file_path: Path) -> DocumentMetadata:
        """æå–PDFå…ƒæ•°æ®"""
        metadata = doc.metadata
        
        # è§£ææ—¥æœŸ
        creation_date = self._parse_pdf_date(metadata.get("creationDate", ""))
        modification_date = self._parse_pdf_date(metadata.get("modDate", ""))
        
        # æ£€æµ‹è¯­è¨€
        language = self._detect_language(doc)
        
        return DocumentMetadata(
            title=metadata.get("title", "").strip(),
            author=metadata.get("author", "").strip(),
            subject=metadata.get("subject", "").strip(),
            creator=metadata.get("creator", "").strip(),
            producer=metadata.get("producer", "").strip(),
            creation_date=creation_date,
            modification_date=modification_date,
            page_count=doc.page_count,
            file_size=os.path.getsize(file_path),
            file_name=file_path.name,
            file_extension=file_path.suffix,
            language=language,
            has_images=self._has_images(doc),
            has_tables=self._has_tables(doc),
            keywords=self._extract_keywords(metadata.get("keywords", ""))
        )
    
    def _parse_pdf_date(self, date_str: str) -> Optional[datetime]:
        """è§£æPDFæ—¥æœŸæ ¼å¼"""
        if not date_str:
            return None
        
        # PDFæ—¥æœŸæ ¼å¼: D:YYYYMMDDHHmmSSOHH'mm'
        date_pattern = r"D:(\d{4})(\d{2})(\d{2})(\d{2})(\d{2})(\d{2})"
        match = re.match(date_pattern, date_str)
        
        if match:
            year, month, day, hour, minute, second = map(int, match.groups())
            try:
                return datetime(year, month, day, hour, minute, second)
            except ValueError:
                pass
        
        return None
    
    def _detect_language(self, doc: fitz.Document) -> str:
        """æ£€æµ‹æ–‡æ¡£è¯­è¨€"""
        # æå–å‰å‡ é¡µæ–‡æœ¬è¿›è¡Œè¯­è¨€æ£€æµ‹
        sample_text = ""
        max_pages = min(3, doc.page_count)
        
        for page_num in range(max_pages):
            page_text = doc[page_num].get_text()
            sample_text += page_text[:1000]  # æ¯é¡µæœ€å¤š1000å­—ç¬¦
        
        if not sample_text.strip():
            return "unknown"
        
        try:
            from langdetect import detect
            return detect(sample_text)
        except Exception:
            # ç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', sample_text))
            english_chars = len(re.findall(r'[a-zA-Z]', sample_text))
            
            if chinese_chars > english_chars:
                return "zh"
            elif english_chars > 0:
                return "en"
            else:
                return "unknown"
    
    def _extract_text_and_pages(self, doc: fitz.Document) -> tuple[str, List[Dict]]:
        """æå–æ–‡æœ¬å’Œé¡µé¢ä¿¡æ¯"""
        full_text = ""
        pages = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            
            # æå–é¡µé¢æ–‡æœ¬
            page_text = page.get_text()
            full_text += page_text + "\n\n"
            
            # è·å–é¡µé¢è¯¦ç»†ä¿¡æ¯
            text_dict = page.get_text("dict")
            
            page_info = {
                "page_number": page_num + 1,
                "text": page_text,
                "bbox": page.rect,
                "width": page.rect.width,
                "height": page.rect.height,
                "blocks": self._process_text_blocks(text_dict.get("blocks", [])),
                "fonts": self._extract_fonts(text_dict.get("blocks", [])),
                "word_count": len(page_text.split()),
                "char_count": len(page_text)
            }
            
            pages.append(page_info)
        
        return full_text.strip(), pages
    
    def _process_text_blocks(self, blocks: List[Dict]) -> List[Dict]:
        """å¤„ç†æ–‡æœ¬å—"""
        processed_blocks = []
        
        for block in blocks:
            if block.get("type") == 0:  # æ–‡æœ¬å—
                block_info = {
                    "bbox": block.get("bbox", []),
                    "text": "",
                    "lines": []
                }
                
                for line in block.get("lines", []):
                    line_text = ""
                    line_info = {
                        "bbox": line.get("bbox", []),
                        "spans": []
                    }
                    
                    for span in line.get("spans", []):
                        span_text = span.get("text", "")
                        line_text += span_text
                        
                        span_info = {
                            "text": span_text,
                            "bbox": span.get("bbox", []),
                            "font": span.get("font", ""),
                            "size": span.get("size", 0),
                            "flags": span.get("flags", 0),
                            "color": span.get("color", 0)
                        }
                        
                        line_info["spans"].append(span_info)
                    
                    line_info["text"] = line_text
                    block_info["lines"].append(line_info)
                    block_info["text"] += line_text + "\n"
                
                block_info["text"] = block_info["text"].strip()
                processed_blocks.append(block_info)
        
        return processed_blocks
    
    def _extract_fonts(self, blocks: List[Dict]) -> Dict[str, int]:
        """æå–å­—ä½“ä¿¡æ¯"""
        fonts = {}
        
        for block in blocks:
            if block.get("type") == 0:
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        font = span.get("font", "")
                        size = span.get("size", 0)
                        
                        if font:
                            font_key = f"{font}_{size}"
                            fonts[font_key] = fonts.get(font_key, 0) + 1
        
        return fonts
    
    def _extract_images(self, doc: fitz.Document) -> List[Dict]:
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            image_list = page.get_images()
            
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    
                    if pix.n - pix.alpha < 4:  # ç¡®ä¿æ˜¯RGBæˆ–ç°åº¦å›¾
                        image_info = {
                            "page_number": page_num + 1,
                            "index": img_index,
                            "xref": xref,
                            "width": pix.width,
                            "height": pix.height,
                            "colorspace": pix.colorspace.name if pix.colorspace else "unknown",
                            "has_alpha": bool(pix.alpha),
                            "size_bytes": len(pix.tobytes()),
                            "bbox": img[1:5] if len(img) > 4 else None
                        }
                        
                        images.append(image_info)
                    
                    pix = None
                    
                except Exception as e:
                    logger.warning(f"æå–å›¾ç‰‡å¤±è´¥ (é¡µé¢ {page_num + 1}, å›¾ç‰‡ {img_index}): {e}")
        
        return images
    
    def _analyze_structure(self, doc: fitz.Document) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        structure = {
            "headings": [],
            "paragraphs": [],
            "lists": [],
            "outline": []
        }
        
        # æå–å¤§çº²
        try:
            outline = doc.get_toc()
            structure["outline"] = [
                {
                    "level": item[0],
                    "title": item[1],
                    "page": item[2]
                }
                for item in outline
            ]
        except Exception:
            pass
        
        # åˆ†ææ ‡é¢˜å’Œæ®µè½
        for page_num in range(doc.page_count):
            page = doc[page_num]
            text_dict = page.get_text("dict")
            
            page_headings, page_paragraphs = self._identify_headings_and_paragraphs(
                text_dict.get("blocks", []), page_num + 1
            )
            
            structure["headings"].extend(page_headings)
            structure["paragraphs"].extend(page_paragraphs)
        
        return structure
    
    def _identify_headings_and_paragraphs(self, blocks: List[Dict], page_num: int) -> tuple[List[Dict], List[Dict]]:
        """è¯†åˆ«æ ‡é¢˜å’Œæ®µè½"""
        headings = []
        paragraphs = []
        
        # è®¡ç®—å¹³å‡å­—ä½“å¤§å°
        font_sizes = []
        for block in blocks:
            if block.get("type") == 0:
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        size = span.get("size", 0)
                        if size > 0:
                            font_sizes.append(size)
        
        avg_font_size = sum(font_sizes) / len(font_sizes) if font_sizes else 12
        
        for block in blocks:
            if block.get("type") == 0:
                block_text = ""
                max_font_size = 0
                is_bold = False
                
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        text = span.get("text", "")
                        size = span.get("size", 0)
                        flags = span.get("flags", 0)
                        
                        block_text += text
                        max_font_size = max(max_font_size, size)
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºç²—ä½“ (flags & 16)
                        if flags & 16:
                            is_bold = True
                
                block_text = block_text.strip()
                
                if block_text:
                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜
                    if (max_font_size > avg_font_size * 1.2 or 
                        is_bold or 
                        self._looks_like_heading(block_text)):
                        
                        heading = {
                            "text": block_text,
                            "page_number": page_num,
                            "font_size": max_font_size,
                            "is_bold": is_bold,
                            "level": self._determine_heading_level(max_font_size, avg_font_size),
                            "bbox": block.get("bbox", [])
                        }
                        headings.append(heading)
                    else:
                        paragraph = {
                            "text": block_text,
                            "page_number": page_num,
                            "word_count": len(block_text.split()),
                            "char_count": len(block_text),
                            "bbox": block.get("bbox", [])
                        }
                        paragraphs.append(paragraph)
        
        return headings, paragraphs
    
    def _looks_like_heading(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åƒæ ‡é¢˜"""
        # çŸ­æ–‡æœ¬æ›´å¯èƒ½æ˜¯æ ‡é¢˜
        if len(text) < 100 and len(text.split()) < 10:
            # ä»¥æ•°å­—å¼€å¤´çš„å¯èƒ½æ˜¯ç« èŠ‚æ ‡é¢˜
            if re.match(r'^\d+\.', text.strip()):
                return True
            
            # å…¨å¤§å†™çš„çŸ­æ–‡æœ¬
            if text.isupper() and len(text) > 3:
                return True
            
            # ä»¥å¸¸è§æ ‡é¢˜è¯å¼€å¤´
            heading_words = ['ç¬¬', 'ç« ', 'Chapter', 'Section', 'æ‘˜è¦', 'Abstract', 'å¼•è¨€', 'Introduction']
            for word in heading_words:
                if text.strip().startswith(word):
                    return True
        
        return False
    
    def _determine_heading_level(self, font_size: float, avg_font_size: float) -> int:
        """ç¡®å®šæ ‡é¢˜å±‚çº§"""
        ratio = font_size / avg_font_size
        
        if ratio >= 1.8:
            return 1
        elif ratio >= 1.5:
            return 2
        elif ratio >= 1.3:
            return 3
        elif ratio >= 1.1:
            return 4
        else:
            return 5
    
    def _detect_tables(self, doc: fitz.Document) -> List[Dict]:
        """æ£€æµ‹è¡¨æ ¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        tables = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            
            # è·å–é¡µé¢ä¸Šçš„æ‰€æœ‰çŸ©å½¢ï¼ˆå¯èƒ½æ˜¯è¡¨æ ¼è¾¹æ¡†ï¼‰
            drawings = page.get_drawings()
            
            # ç®€å•çš„è¡¨æ ¼æ£€æµ‹é€»è¾‘
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è¡¨æ ¼æ£€æµ‹ç®—æ³•
            if len(drawings) > 10:  # å¦‚æœæœ‰å¾ˆå¤šç»˜å›¾å…ƒç´ ï¼Œå¯èƒ½åŒ…å«è¡¨æ ¼
                table_info = {
                    "page_number": page_num + 1,
                    "estimated_tables": 1,  # ç®€åŒ–å¤„ç†
                    "drawing_count": len(drawings)
                }
                tables.append(table_info)
        
        return tables
    
    def _has_images(self, doc: fitz.Document) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        for page_num in range(min(3, doc.page_count)):  # åªæ£€æŸ¥å‰3é¡µ
            page = doc[page_num]
            if page.get_images():
                return True
        return False
    
    def _has_tables(self, doc: fitz.Document) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼"""
        for page_num in range(min(3, doc.page_count)):  # åªæ£€æŸ¥å‰3é¡µ
            page = doc[page_num]
            drawings = page.get_drawings()
            if len(drawings) > 5:  # ç®€å•åˆ¤æ–­
                return True
        return False
    
    def _extract_keywords(self, keywords_str: str) -> List[str]:
        """æå–å…³é”®è¯"""
        if not keywords_str:
            return []
        
        # åˆ†å‰²å…³é”®è¯ï¼ˆå¸¸è§åˆ†éš”ç¬¦ï¼‰
        keywords = re.split(r'[,;ï¼Œï¼›\s]+', keywords_str)
        return [kw.strip() for kw in keywords if kw.strip()]
```

### 1.3 æµ‹è¯•PDFè§£æå™¨

**åˆ›å»º `test_pdf_parser.py`ï¼š**
```python
"""æµ‹è¯•PDFè§£æå™¨"""

from pathlib import Path
from src.document.pdf_parser import PDFParser
import json

def test_pdf_parser():
    """æµ‹è¯•PDFè§£æåŠŸèƒ½"""
    parser = PDFParser()
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…çš„PDFæ–‡ä»¶ï¼‰
    test_files = [
        "test_documents/sample1.pdf",
        "test_documents/sample2.pdf",
        "test_documents/sample3.pdf"
    ]
    
    for file_path in test_files:
        pdf_path = Path(file_path)
        
        if not pdf_path.exists():
            print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
        
        print(f"\nğŸ“„ è§£ææ–‡ä»¶: {pdf_path.name}")
        print("=" * 50)
        
        try:
            # è§£ææ–‡æ¡£
            content = parser.parse(pdf_path)
            
            # æ˜¾ç¤ºå…ƒæ•°æ®
            print("ğŸ“‹ æ–‡æ¡£å…ƒæ•°æ®:")
            print(f"  æ ‡é¢˜: {content.metadata.title}")
            print(f"  ä½œè€…: {content.metadata.author}")
            print(f"  é¡µæ•°: {content.metadata.page_count}")
            print(f"  æ–‡ä»¶å¤§å°: {content.metadata.file_size:,} å­—èŠ‚")
            print(f"  è¯­è¨€: {content.metadata.language}")
            print(f"  åŒ…å«å›¾ç‰‡: {content.metadata.has_images}")
            print(f"  åŒ…å«è¡¨æ ¼: {content.metadata.has_tables}")
            
            # æ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡
            print(f"\nğŸ“ æ–‡æœ¬ç»Ÿè®¡:")
            print(f"  æ€»å­—ç¬¦æ•°: {len(content.text):,}")
            print(f"  æ€»å•è¯æ•°: {len(content.text.split()):,}")
            print(f"  æ€»é¡µæ•°: {len(content.pages)}")
            
            # æ˜¾ç¤ºç»“æ„ä¿¡æ¯
            if content.structure["headings"]:
                print(f"\nğŸ“‘ æ ‡é¢˜ç»“æ„:")
                for heading in content.structure["headings"][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"  H{heading['level']}: {heading['text'][:50]}...")
            
            # æ˜¾ç¤ºå¤§çº²
            if content.structure["outline"]:
                print(f"\nğŸ“– æ–‡æ¡£å¤§çº²:")
                for item in content.structure["outline"][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    indent = "  " * (item["level"] - 1)
                    print(f"{indent}- {item['title']} (é¡µ {item['page']})")
            
            # æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
            if content.images:
                print(f"\nğŸ–¼ï¸ å›¾ç‰‡ä¿¡æ¯:")
                print(f"  å›¾ç‰‡æ€»æ•°: {len(content.images)}")
                for img in content.images[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  é¡µ {img['page_number']}: {img['width']}x{img['height']} ({img['colorspace']})")
            
            # æ˜¾ç¤ºå‰200å­—ç¬¦çš„æ–‡æœ¬å†…å®¹
            print(f"\nğŸ“„ æ–‡æœ¬é¢„è§ˆ:")
            preview_text = content.text[:200].replace('\n', ' ').strip()
            print(f"  {preview_text}...")
            
            print("\nâœ… è§£ææˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")

def test_specific_features():
    """æµ‹è¯•ç‰¹å®šåŠŸèƒ½"""
    parser = PDFParser()
    
    # æµ‹è¯•æ–‡ä»¶éªŒè¯
    print("\nğŸ” æµ‹è¯•æ–‡ä»¶éªŒè¯:")
    print(f"  å­˜åœ¨çš„æ–‡ä»¶: {parser.validate_file(Path('test_documents/sample1.pdf'))}")
    print(f"  ä¸å­˜åœ¨çš„æ–‡ä»¶: {parser.validate_file(Path('nonexistent.pdf'))}")
    
    # æµ‹è¯•æ”¯æŒçš„æ ¼å¼
    print(f"\nğŸ“‹ æ”¯æŒçš„æ ¼å¼: {parser.supported_extensions}")
    print(f"  PDFæ–‡ä»¶æ”¯æŒ: {parser.can_parse(Path('test.pdf'))}")
    print(f"  Wordæ–‡ä»¶æ”¯æŒ: {parser.can_parse(Path('test.docx'))}")

if __name__ == "__main__":
    print("å¼€å§‹PDFè§£æå™¨æµ‹è¯•...")
    
    try:
        test_specific_features()
        test_pdf_parser()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_pdf_parser.py
```

---

## ğŸ”¬ å®éªŒäºŒï¼šå¤šæ ¼å¼æ–‡æ¡£è§£æå™¨

### 2.1 å®ç°Wordæ–‡æ¡£è§£æå™¨

**åˆ›å»º `src/document/docx_parser.py`ï¼š**
```python
"""Wordæ–‡æ¡£è§£æå™¨"""

from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import os

from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT

from .parser import DocumentParser, DocumentContent, DocumentMetadata

class DocxParser(DocumentParser):
    """Wordæ–‡æ¡£è§£æå™¨"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = {".docx", ".doc"}
    
    def parse(self, file_path: Path) -> DocumentContent:
        """è§£æWordæ–‡æ¡£"""
        if not self.validate_file(file_path):
            raise ValueError(f"æ— æ•ˆçš„Wordæ–‡ä»¶: {file_path}")
        
        try:
            doc = Document(str(file_path))
            
            # æå–å…ƒæ•°æ®
            metadata = self._extract_metadata(doc, file_path)
            
            # æå–å†…å®¹
            text, structure = self._extract_content_and_structure(doc)
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            images = self._extract_images(doc)
            
            # æå–è¡¨æ ¼
            tables = self._extract_tables(doc)
            
            return DocumentContent(
                text=text,
                metadata=metadata,
                images=images,
                tables=tables,
                structure=structure
            )
            
        except Exception as e:
            logger.error(f"Wordæ–‡æ¡£è§£æå¤±è´¥: {e}")
            raise
    
    def _extract_metadata(self, doc: Document, file_path: Path) -> DocumentMetadata:
        """æå–Wordæ–‡æ¡£å…ƒæ•°æ®"""
        core_props = doc.core_properties
        
        return DocumentMetadata(
            title=core_props.title or "",
            author=core_props.author or "",
            subject=core_props.subject or "",
            creator=core_props.author or "",
            creation_date=core_props.created,
            modification_date=core_props.modified,
            page_count=self._estimate_page_count(doc),
            file_size=os.path.getsize(file_path),
            file_name=file_path.name,
            file_extension=file_path.suffix,
            language=core_props.language or "unknown",
            has_images=self._has_images(doc),
            has_tables=self._has_tables(doc),
            keywords=self._extract_keywords(core_props.keywords or "")
        )
    
    def _extract_content_and_structure(self, doc: Document) -> tuple[str, Dict[str, Any]]:
        """æå–å†…å®¹å’Œç»“æ„"""
        full_text = ""
        paragraphs = []
        headings = []
        lists = []
        
        for i, paragraph in enumerate(doc.paragraphs):
            text = paragraph.text.strip()
            if not text:
                continue
            
            full_text += text + "\n\n"
            
            # åˆ†ææ®µè½æ ·å¼
            style_name = paragraph.style.name if paragraph.style else "Normal"
            
            para_info = {
                "index": i,
                "text": text,
                "style": style_name,
                "alignment": self._get_alignment(paragraph),
                "word_count": len(text.split()),
                "char_count": len(text)
            }
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜
            if self._is_heading(paragraph, style_name):
                heading = {
                    "text": text,
                    "level": self._get_heading_level(style_name),
                    "style": style_name,
                    "index": i
                }
                headings.append(heading)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ—è¡¨é¡¹
            elif self._is_list_item(text):
                list_item = {
                    "text": text,
                    "type": "bullet" if text.startswith("â€¢") else "numbered",
                    "index": i
                }
                lists.append(list_item)
            
            paragraphs.append(para_info)
        
        structure = {
            "paragraphs": paragraphs,
            "headings": headings,
            "lists": lists,
            "outline": self._build_outline(headings)
        }
        
        return full_text.strip(), structure
    
    def _get_alignment(self, paragraph) -> str:
        """è·å–æ®µè½å¯¹é½æ–¹å¼"""
        alignment_map = {
            WD_PARAGRAPH_ALIGNMENT.LEFT: "left",
            WD_PARAGRAPH_ALIGNMENT.CENTER: "center",
            WD_PARAGRAPH_ALIGNMENT.RIGHT: "right",
            WD_PARAGRAPH_ALIGNMENT.JUSTIFY: "justify"
        }
        return alignment_map.get(paragraph.alignment, "left")
    
    def _is_heading(self, paragraph, style_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜"""
        # åŸºäºæ ·å¼ååˆ¤æ–­
        heading_styles = ["Heading 1", "Heading 2", "Heading 3", "Heading 4", "Heading 5", "Heading 6"]
        if style_name in heading_styles:
            return True
        
        # åŸºäºæ–‡æœ¬ç‰¹å¾åˆ¤æ–­
        text = paragraph.text.strip()
        if len(text) < 100 and len(text.split()) < 10:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç²—ä½“æ ¼å¼
            for run in paragraph.runs:
                if run.bold:
                    return True
        
        return False
    
    def _get_heading_level(self, style_name: str) -> int:
        """è·å–æ ‡é¢˜å±‚çº§"""
        if "Heading" in style_name:
            try:
                return int(style_name.split()[-1])
            except (ValueError, IndexError):
                pass
        return 1
    
    def _is_list_item(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ—è¡¨é¡¹"""
        return (
            text.startswith("â€¢") or
            text.startswith("-") or
            re.match(r'^\d+\.', text) or
            re.match(r'^[a-zA-Z]\)', text)
        )
    
    def _build_outline(self, headings: List[Dict]) -> List[Dict]:
        """æ„å»ºæ–‡æ¡£å¤§çº²"""
        outline = []
        for heading in headings:
            outline.append({
                "level": heading["level"],
                "title": heading["text"],
                "index": heading["index"]
            })
        return outline
    
    def _extract_images(self, doc: Document) -> List[Dict]:
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        
        # éå†æ‰€æœ‰æ®µè½ä¸­çš„å›¾ç‰‡
        for i, paragraph in enumerate(doc.paragraphs):
            for run in paragraph.runs:
                if run.element.xpath('.//pic:pic'):
                    # æ‰¾åˆ°å›¾ç‰‡
                    image_info = {
                        "paragraph_index": i,
                        "type": "inline",
                        "description": "Word embedded image"
                    }
                    images.append(image_info)
        
        return images
    
    def _extract_tables(self, doc: Document) -> List[Dict]:
        """æå–è¡¨æ ¼"""
        tables = []
        
        for i, table in enumerate(doc.tables):
            rows_data = []
            
            for row in table.rows:
                row_data = []
                for cell in row.cells:
                    row_data.append(cell.text.strip())
                rows_data.append(row_data)
            
            table_info = {
                "index": i,
                "rows": len(table.rows),
                "columns": len(table.columns) if table.rows else 0,
                "data": rows_data,
                "text": self._table_to_text(rows_data)
            }
            
            tables.append(table_info)
        
        return tables
    
    def _table_to_text(self, rows_data: List[List[str]]) -> str:
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬"""
        text_lines = []
        for row in rows_data:
            text_lines.append(" | ".join(row))
        return "\n".join(text_lines)
    
    def _estimate_page_count(self, doc: Document) -> int:
        """ä¼°ç®—é¡µæ•°ï¼ˆåŸºäºå­—ç¬¦æ•°ï¼‰"""
        total_chars = sum(len(p.text) for p in doc.paragraphs)
        # å‡è®¾æ¯é¡µçº¦2000å­—ç¬¦
        return max(1, total_chars // 2000)
    
    def _has_images(self, doc: Document) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        for paragraph in doc.paragraphs:
            for run in paragraph.runs:
                if run.element.xpath('.//pic:pic'):
                    return True
        return False
    
    def _has_tables(self, doc: Document) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼"""
        return len(doc.tables) > 0
    
    def _extract_keywords(self, keywords_str: str) -> List[str]:
        """æå–å…³é”®è¯"""
        if not keywords_str:
            return []
        
        keywords = re.split(r'[,;ï¼Œï¼›\s]+', keywords_str)
        return [kw.strip() for kw in keywords if kw.strip()]
```

### 2.2 å®ç°æ–‡æœ¬æ–‡æ¡£è§£æå™¨

**åˆ›å»º `src/document/txt_parser.py`ï¼š**
```python
"""æ–‡æœ¬æ–‡æ¡£è§£æå™¨"""

from pathlib import Path
from typing import Dict, List, Any
import os
import chardet
import re

from .parser import DocumentParser, DocumentContent, DocumentMetadata

class TxtParser(DocumentParser):
    """æ–‡æœ¬æ–‡æ¡£è§£æå™¨"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = {".txt", ".md", ".rst"}
    
    def parse(self, file_path: Path) -> DocumentContent:
        """è§£ææ–‡æœ¬æ–‡æ¡£"""
        if not self.validate_file(file_path):
            raise ValueError(f"æ— æ•ˆçš„æ–‡æœ¬æ–‡ä»¶: {file_path}")
        
        try:
            # æ£€æµ‹ç¼–ç 
            encoding = self._detect_encoding(file_path)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding=encoding) as f:
                text = f.read()
            
            # æå–å…ƒæ•°æ®
            metadata = self._extract_metadata(text, file_path, encoding)
            
            # åˆ†æç»“æ„
            structure = self._analyze_structure(text, file_path.suffix)
            
            return DocumentContent(
                text=text,
                metadata=metadata,
                structure=structure
            )
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {e}")
            raise
    
    def _detect_encoding(self, file_path: Path) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # è¯»å–å‰10KB
            
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            
            # å¸¸è§ç¼–ç çš„æ˜ å°„
            encoding_map = {
                'GB2312': 'gbk',
                'ISO-8859-1': 'utf-8'  # é€šå¸¸æ˜¯è¯¯åˆ¤
            }
            
            return encoding_map.get(encoding, encoding or 'utf-8')
            
        except Exception:
            return 'utf-8'
    
    def _extract_metadata(self, text: str, file_path: Path, encoding: str) -> DocumentMetadata:
        """æå–æ–‡æœ¬æ–‡æ¡£å…ƒæ•°æ®"""
        lines = text.split('\n')
        
        # å°è¯•ä»æ–‡ä»¶å¼€å¤´æå–æ ‡é¢˜
        title = ""
        if lines:
            first_line = lines[0].strip()
            if first_line and len(first_line) < 100:
                title = first_line
        
        # æ£€æµ‹è¯­è¨€
        language = self._detect_language(text)
        
        return DocumentMetadata(
            title=title,
            page_count=1,  # æ–‡æœ¬æ–‡ä»¶è§†ä¸º1é¡µ
            file_size=os.path.getsize(file_path),
            file_name=file_path.name,
            file_extension=file_path.suffix,
            language=language,
            has_images=False,
            has_tables=self._has_tables(text)
        )
    
    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            from langdetect import detect
            return detect(text[:1000])  # åªæ£€æµ‹å‰1000å­—ç¬¦
        except Exception:
            # ç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text[:1000]))
            english_chars = len(re.findall(r'[a-zA-Z]', text[:1000]))
            
            if chinese_chars > english_chars:
                return "zh"
            elif english_chars > 0:
                return "en"
            else:
                return "unknown"
    
    def _analyze_structure(self, text: str, file_extension: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬ç»“æ„"""
        lines = text.split('\n')
        
        if file_extension == '.md':
            return self._analyze_markdown_structure(lines)
        elif file_extension == '.rst':
            return self._analyze_rst_structure(lines)
        else:
            return self._analyze_plain_text_structure(lines)
    
    def _analyze_markdown_structure(self, lines: List[str]) -> Dict[str, Any]:
        """åˆ†æMarkdownç»“æ„"""
        headings = []
        paragraphs = []
        lists = []
        code_blocks = []
        
        in_code_block = False
        current_paragraph = []
        
        for i, line in enumerate(lines):
            line = line.rstrip()
            
            # ä»£ç å—
            if line.startswith('```'):
                in_code_block = not in_code_block
                if not in_code_block and current_paragraph:
                    code_blocks.append({
                        "start_line": i - len(current_paragraph),
                        "end_line": i,
                        "content": '\n'.join(current_paragraph)
                    })
                    current_paragraph = []
                continue
            
            if in_code_block:
                current_paragraph.append(line)
                continue
            
            # æ ‡é¢˜
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                if title:
                    headings.append({
                        "text": title,
                        "level": level,
                        "line_number": i + 1
                    })
            
            # åˆ—è¡¨é¡¹
            elif re.match(r'^\s*[-*+]\s', line) or re.match(r'^\s*\d+\.\s', line):
                lists.append({
                    "text": line.strip(),
                    "type": "numbered" if re.match(r'^\s*\d+\.', line) else "bullet",
                    "line_number": i + 1
                })
            
            # æ®µè½
            elif line.strip():
                if current_paragraph:
                    current_paragraph.append(line)
                else:
                    current_paragraph = [line]
            
            # ç©ºè¡Œï¼Œç»“æŸå½“å‰æ®µè½
            elif current_paragraph:
                paragraphs.append({
                    "text": '\n'.join(current_paragraph),
                    "start_line": i - len(current_paragraph) + 1,
                    "end_line": i,
                    "word_count": len(' '.join(current_paragraph).split())
                })
                current_paragraph = []
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph:
            paragraphs.append({
                "text": '\n'.join(current_paragraph),
                "start_line": len(lines) - len(current_paragraph),
                "end_line": len(lines),
                "word_count": len(' '.join(current_paragraph).split())
            })
        
        return {
            "headings": headings,
            "paragraphs": paragraphs,
            "lists": lists,
            "code_blocks": code_blocks,
            "outline": self._build_outline(headings)
        }
    
    def _analyze_rst_structure(self, lines: List[str]) -> Dict[str, Any]:
        """åˆ†æreStructuredTextç»“æ„"""
        # ç®€åŒ–çš„RSTè§£æ
        headings = []
        paragraphs = []
        
        for i, line in enumerate(lines):
            # RSTæ ‡é¢˜æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            if i < len(lines) - 1:
                next_line = lines[i + 1]
                if (len(next_line.strip()) > 0 and 
                    len(set(next_line.strip())) == 1 and 
                    next_line.strip()[0] in '=-~^"'):
                    
                    headings.append({
                        "text": line.strip(),
                        "level": self._get_rst_heading_level(next_line.strip()[0]),
                        "line_number": i + 1
                    })
        
        return {
            "headings": headings,
            "paragraphs": paragraphs,
            "lists": [],
            "outline": self._build_outline(headings)
        }
    
    def _get_rst_heading_level(self, char: str) -> int:
        """è·å–RSTæ ‡é¢˜å±‚çº§"""
        level_map = {
            '=': 1,
            '-': 2,
            '~': 3,
            '^': 4,
            '"': 5
        }
        return level_map.get(char, 1)
    
    def _analyze_plain_text_structure(self, lines: List[str]) -> Dict[str, Any]:
        """åˆ†æçº¯æ–‡æœ¬ç»“æ„"""
        paragraphs = []
        current_paragraph = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            if line:
                current_paragraph.append(line)
            elif current_paragraph:
                paragraphs.append({
                    "text": ' '.join(current_paragraph),
                    "start_line": i - len(current_paragraph) + 1,
                    "end_line": i,
                    "word_count": len(' '.join(current_paragraph).split())
                })
                current_paragraph = []
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph:
            paragraphs.append({
                "text": ' '.join(current_paragraph),
                "start_line": len(lines) - len(current_paragraph),
                "end_line": len(lines),
                "word_count": len(' '.join(current_paragraph).split())
            })
        
        return {
            "headings": [],
            "paragraphs": paragraphs,
            "lists": [],
            "outline": []
        }
    
    def _build_outline(self, headings: List[Dict]) -> List[Dict]:
        """æ„å»ºæ–‡æ¡£å¤§çº²"""
        return [{
            "level": h["level"],
            "title": h["text"],
            "line": h["line_number"]
        } for h in headings]
    
    def _has_tables(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ï¼ˆç®€å•æ£€æµ‹ï¼‰"""
        lines = text.split('\n')
        
        # æ£€æµ‹Markdownè¡¨æ ¼
        for line in lines:
            if '|' in line and line.count('|') >= 2:
                return True
        
        return False
```

### 2.3 åˆ›å»ºç»Ÿä¸€çš„æ–‡æ¡£è§£æç®¡ç†å™¨

**åˆ›å»º `src/document/document_manager.py`ï¼š**
```python
"""æ–‡æ¡£è§£æç®¡ç†å™¨"""

from pathlib import Path
from typing import Dict, List, Optional, Type
import logging

from .parser import DocumentParser, DocumentContent
from .pdf_parser import PDFParser
from .docx_parser import DocxParser
from .txt_parser import TxtParser

logger = logging.getLogger(__name__)

class DocumentManager:
    """æ–‡æ¡£è§£æç®¡ç†å™¨"""
    
    def __init__(self):
        self.parsers: Dict[str, DocumentParser] = {}
        self._register_default_parsers()
    
    def _register_default_parsers(self):
        """æ³¨å†Œé»˜è®¤è§£æå™¨"""
        parsers = [
            PDFParser(),
            DocxParser(),
            TxtParser()
        ]
        
        for parser in parsers:
            self.register_parser(parser)
    
    def register_parser(self, parser: DocumentParser):
        """æ³¨å†Œè§£æå™¨"""
        for ext in parser.supported_extensions:
            self.parsers[ext.lower()] = parser
            logger.info(f"æ³¨å†Œè§£æå™¨: {ext} -> {parser.__class__.__name__}")
    
    def get_parser(self, file_path: Path) -> Optional[DocumentParser]:
        """è·å–é€‚åˆçš„è§£æå™¨"""
        extension = file_path.suffix.lower()
        return self.parsers.get(extension)
    
    def can_parse(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥è§£æè¯¥æ–‡ä»¶"""
        return self.get_parser(file_path) is not None
    
    def parse_document(self, file_path: Path) -> DocumentContent:
        """è§£ææ–‡æ¡£"""
        file_path = Path(file_path)
        
        parser = self.get_parser(file_path)
        if not parser:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        logger.info(f"ä½¿ç”¨ {parser.__class__.__name__} è§£æ: {file_path.name}")
        return parser.parse(file_path)
    
    def batch_parse(self, file_paths: List[Path], 
                   skip_errors: bool = True) -> List[DocumentContent]:
        """æ‰¹é‡è§£ææ–‡æ¡£"""
        results = []
        
        for file_path in file_paths:
            try:
                content = self.parse_document(file_path)
                results.append(content)
                logger.info(f"âœ… è§£ææˆåŠŸ: {file_path.name}")
            except Exception as e:
                logger.error(f"âŒ è§£æå¤±è´¥: {file_path.name} - {e}")
                if not skip_errors:
                    raise
        
        return results
    
    def get_supported_extensions(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å"""
        return list(self.parsers.keys())
    
    def get_parser_info(self) -> Dict[str, str]:
        """è·å–è§£æå™¨ä¿¡æ¯"""
        info = {}
        for ext, parser in self.parsers.items():
            info[ext] = parser.__class__.__name__
        return info
```

### 2.4 æµ‹è¯•å¤šæ ¼å¼è§£æå™¨

**åˆ›å»º `test_document_manager.py`ï¼š**
```python
"""æµ‹è¯•æ–‡æ¡£ç®¡ç†å™¨"""

from pathlib import Path
from src.document.document_manager import DocumentManager

def test_document_manager():
    """æµ‹è¯•æ–‡æ¡£ç®¡ç†å™¨"""
    manager = DocumentManager()
    
    print("ğŸ“‹ æ–‡æ¡£ç®¡ç†å™¨ä¿¡æ¯:")
    print(f"æ”¯æŒçš„æ ¼å¼: {manager.get_supported_extensions()}")
    print(f"è§£æå™¨æ˜ å°„: {manager.get_parser_info()}")
    
    # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    test_files = [
        "test_documents/sample.pdf",
        "test_documents/sample.docx",
        "test_documents/sample.txt",
        "test_documents/sample.md",
        "test_documents/README.md"
    ]
    
    print("\nğŸ” æ£€æŸ¥æ–‡ä»¶æ”¯æŒæƒ…å†µ:")
    for file_path in test_files:
        path = Path(file_path)
        supported = manager.can_parse(path)
        parser = manager.get_parser(path)
        parser_name = parser.__class__.__name__ if parser else "None"
        
        print(f"  {path.name}: {supported} ({parser_name})")
    
    # æ‰¹é‡è§£ææµ‹è¯•
    print("\nğŸ“„ æ‰¹é‡è§£ææµ‹è¯•:")
    existing_files = [Path(f) for f in test_files if Path(f).exists()]
    
    if existing_files:
        try:
            results = manager.batch_parse(existing_files)
            
            print(f"\nâœ… æˆåŠŸè§£æ {len(results)} ä¸ªæ–‡ä»¶:")
            for i, content in enumerate(results):
                print(f"\næ–‡ä»¶ {i+1}: {content.metadata.file_name}")
                print(f"  æ ‡é¢˜: {content.metadata.title}")
                print(f"  å¤§å°: {content.metadata.file_size:,} å­—èŠ‚")
                print(f"  è¯­è¨€: {content.metadata.language}")
                print(f"  å­—ç¬¦æ•°: {len(content.text):,}")
                print(f"  å•è¯æ•°: {len(content.text.split()):,}")
                
                if content.structure.get("headings"):
                    print(f"  æ ‡é¢˜æ•°: {len(content.structure['headings'])}")
                
                if content.images:
                    print(f"  å›¾ç‰‡æ•°: {len(content.images)}")
                
                if content.tables:
                    print(f"  è¡¨æ ¼æ•°: {len(content.tables)}")
                
                # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
                preview = content.text[:100].replace('\n', ' ').strip()
                print(f"  é¢„è§ˆ: {preview}...")
        
        except Exception as e:
            print(f"âŒ æ‰¹é‡è§£æå¤±è´¥: {e}")
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")

def test_specific_formats():
    """æµ‹è¯•ç‰¹å®šæ ¼å¼"""
    manager = DocumentManager()
    
    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬æ–‡ä»¶
    test_md_content = """
# æµ‹è¯•Markdownæ–‡æ¡£

## ç¬¬ä¸€ç«  ä»‹ç»

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯Markdownè§£æåŠŸèƒ½ã€‚

### 1.1 åŠŸèƒ½ç‰¹æ€§

- æ”¯æŒæ ‡é¢˜è§£æ
- æ”¯æŒåˆ—è¡¨è¯†åˆ«
- æ”¯æŒä»£ç å—

```python
def hello_world():
    print("Hello, World!")
```

## ç¬¬äºŒç«  æ€»ç»“

è§£æåŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚
"""
    
    # å†™å…¥æµ‹è¯•æ–‡ä»¶
    test_file = Path("test_markdown.md")
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(test_md_content)
    
    try:
        print("\nğŸ“ æµ‹è¯•Markdownè§£æ:")
        content = manager.parse_document(test_file)
        
        print(f"æ ‡é¢˜: {content.metadata.title}")
        print(f"è¯­è¨€: {content.metadata.language}")
        print(f"å­—ç¬¦æ•°: {len(content.text):,}")
        
        if content.structure.get("headings"):
            print(f"\nğŸ“‘ æ ‡é¢˜ç»“æ„:")
            for heading in content.structure["headings"]:
                indent = "  " * (heading["level"] - 1)
                print(f"{indent}H{heading['level']}: {heading['text']}")
        
        if content.structure.get("code_blocks"):
            print(f"\nğŸ’» ä»£ç å—: {len(content.structure['code_blocks'])} ä¸ª")
        
        if content.structure.get("lists"):
            print(f"\nğŸ“‹ åˆ—è¡¨é¡¹: {len(content.structure['lists'])} ä¸ª")
    
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if test_file.exists():
            test_file.unlink()

if __name__ == "__main__":
    print("å¼€å§‹æ–‡æ¡£ç®¡ç†å™¨æµ‹è¯•...")
    
    try:
        test_document_manager()
        test_specific_formats()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_document_manager.py
```

---

## ğŸ”¬ å®éªŒä¸‰ï¼šæ™ºèƒ½Chunkæ‹†åˆ†ç­–ç•¥

### 3.1 åˆ›å»ºChunkæ‹†åˆ†å™¨åŸºç±»

**åˆ›å»º `src/chunking/chunker.py`ï¼š**
```python
"""æ–‡æ¡£åˆ†å—å™¨æ¨¡å—"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ChunkMetadata:
    """åˆ†å—å…ƒæ•°æ®"""
    chunk_id: str
    source_file: str
    page_number: Optional[int] = None
    start_char: int = 0
    end_char: int = 0
    chunk_type: str = "text"  # text, heading, list, table, code
    heading_level: Optional[int] = None
    parent_heading: Optional[str] = None
    word_count: int = 0
    char_count: int = 0
    language: str = "unknown"
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—"""
    content: str
    metadata: ChunkMetadata
    
    def __post_init__(self):
        # è‡ªåŠ¨è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.metadata.word_count = len(self.content.split())
        self.metadata.char_count = len(self.content)

class ChunkingStrategy(ABC):
    """åˆ†å—ç­–ç•¥åŸºç±»"""
    
    def __init__(self, 
                 max_chunk_size: int = 1000,
                 overlap_size: int = 100,
                 min_chunk_size: int = 50):
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size
        self.min_chunk_size = min_chunk_size
    
    @abstractmethod
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """å°†æ–‡æœ¬åˆ†å—"""
        pass
    
    def _create_chunk_id(self, source_file: str, index: int) -> str:
        """åˆ›å»ºåˆ†å—ID"""
        return f"{source_file}_{index:04d}"
    
    def _should_merge_chunks(self, chunk1: DocumentChunk, chunk2: DocumentChunk) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶åˆ†å—"""
        total_size = chunk1.metadata.char_count + chunk2.metadata.char_count
        return total_size <= self.max_chunk_size
    
    def _merge_chunks(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """åˆå¹¶è¿‡å°çš„åˆ†å—"""
        if not chunks:
            return chunks
        
        merged = []
        current_chunk = chunks[0]
        
        for next_chunk in chunks[1:]:
            if (current_chunk.metadata.char_count < self.min_chunk_size and
                self._should_merge_chunks(current_chunk, next_chunk)):
                
                # åˆå¹¶åˆ†å—
                merged_content = current_chunk.content + "\n\n" + next_chunk.content
                merged_metadata = ChunkMetadata(
                    chunk_id=current_chunk.metadata.chunk_id,
                    source_file=current_chunk.metadata.source_file,
                    page_number=current_chunk.metadata.page_number,
                    start_char=current_chunk.metadata.start_char,
                    end_char=next_chunk.metadata.end_char,
                    chunk_type="merged",
                    parent_heading=current_chunk.metadata.parent_heading
                )
                
                current_chunk = DocumentChunk(merged_content, merged_metadata)
            else:
                merged.append(current_chunk)
                current_chunk = next_chunk
        
        merged.append(current_chunk)
        return merged
```

### 3.2 å®ç°åŸºäºå¥å­çš„åˆ†å—ç­–ç•¥

**åˆ›å»º `src/chunking/sentence_chunker.py`ï¼š**
```python
"""åŸºäºå¥å­çš„åˆ†å—å™¨"""

import re
from typing import List, Dict, Any
import nltk
from nltk.tokenize import sent_tokenize

from .chunker import ChunkingStrategy, DocumentChunk, ChunkMetadata

# ç¡®ä¿NLTKæ•°æ®å·²ä¸‹è½½
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class SentenceChunker(ChunkingStrategy):
    """åŸºäºå¥å­çš„åˆ†å—å™¨"""
    
    def __init__(self, 
                 max_chunk_size: int = 1000,
                 overlap_size: int = 100,
                 min_chunk_size: int = 50,
                 language: str = 'english'):
        super().__init__(max_chunk_size, overlap_size, min_chunk_size)
        self.language = language
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """åŸºäºå¥å­åˆ†å—"""
        if not text.strip():
            return []
        
        metadata = metadata or {}
        source_file = metadata.get('source_file', 'unknown')
        
        # åˆ†å‰²å¥å­
        sentences = self._split_sentences(text)
        
        # åˆ›å»ºåˆ†å—
        chunks = []
        current_chunk_sentences = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            # å¦‚æœå½“å‰å¥å­åŠ å…¥åè¶…è¿‡æœ€å¤§å¤§å°ï¼Œå…ˆä¿å­˜å½“å‰åˆ†å—
            if (current_size + sentence_size > self.max_chunk_size and 
                current_chunk_sentences):
                
                chunk_content = ' '.join(current_chunk_sentences)
                chunk = self._create_chunk(
                    chunk_content, 
                    source_file, 
                    len(chunks),
                    metadata
                )
                chunks.append(chunk)
                
                # å¤„ç†é‡å 
                if self.overlap_size > 0:
                    overlap_sentences = self._get_overlap_sentences(
                        current_chunk_sentences, self.overlap_size
                    )
                    current_chunk_sentences = overlap_sentences + [sentence]
                    current_size = sum(len(s) for s in current_chunk_sentences)
                else:
                    current_chunk_sentences = [sentence]
                    current_size = sentence_size
            else:
                current_chunk_sentences.append(sentence)
                current_size += sentence_size
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk_sentences:
            chunk_content = ' '.join(current_chunk_sentences)
            chunk = self._create_chunk(
                chunk_content, 
                source_file, 
                len(chunks),
                metadata
            )
            chunks.append(chunk)
        
        # åˆå¹¶è¿‡å°çš„åˆ†å—
        return self._merge_chunks(chunks)
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # ä½¿ç”¨NLTKè¿›è¡Œå¥å­åˆ†å‰²
        try:
            sentences = sent_tokenize(text, language=self.language)
        except Exception:
            # å¦‚æœNLTKå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼
            sentences = self._simple_sentence_split(text)
        
        # æ¸…ç†å’Œè¿‡æ»¤å¥å­
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„å¥å­
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _simple_sentence_split(self, text: str) -> List[str]:
        """ç®€å•çš„å¥å­åˆ†å‰²"""
        # åŸºäºæ ‡ç‚¹ç¬¦å·åˆ†å‰²
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ]\s+'
        sentences = re.split(sentence_endings, text)
        
        # é‡æ–°æ·»åŠ æ ‡ç‚¹ç¬¦å·
        result = []
        endings = re.findall(sentence_endings, text)
        
        for i, sentence in enumerate(sentences[:-1]):
            if i < len(endings):
                result.append(sentence + endings[i].strip())
            else:
                result.append(sentence)
        
        # æ·»åŠ æœ€åä¸€ä¸ªå¥å­
        if sentences[-1].strip():
            result.append(sentences[-1])
        
        return result
    
    def _get_overlap_sentences(self, sentences: List[str], overlap_size: int) -> List[str]:
        """è·å–é‡å çš„å¥å­"""
        if not sentences:
            return []
        
        # ä»åå¾€å‰é€‰æ‹©å¥å­ï¼Œç›´åˆ°è¾¾åˆ°é‡å å¤§å°
        overlap_sentences = []
        current_size = 0
        
        for sentence in reversed(sentences):
            if current_size + len(sentence) <= overlap_size:
                overlap_sentences.insert(0, sentence)
                current_size += len(sentence)
            else:
                break
        
        return overlap_sentences
    
    def _create_chunk(self, content: str, source_file: str, index: int, 
                     metadata: Dict[str, Any]) -> DocumentChunk:
        """åˆ›å»ºæ–‡æ¡£åˆ†å—"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=metadata.get('page_number'),
            chunk_type="sentence",
            language=metadata.get('language', 'unknown'),
            parent_heading=metadata.get('parent_heading')
        )
        
        return DocumentChunk(content, chunk_metadata)
```

### 3.5 åˆ›å»ºåˆ†å—ç®¡ç†å™¨

**åˆ›å»º `src/chunking/chunk_manager.py`ï¼š**
```python
"""åˆ†å—ç®¡ç†å™¨"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import logging

from ..document.document_manager import DocumentManager
from .chunker import DocumentChunk
from .sentence_chunker import SentenceChunker
from .semantic_chunker import SemanticChunker
from .structure_chunker import StructureChunker

logger = logging.getLogger(__name__)

class ChunkManager:
    """åˆ†å—ç®¡ç†å™¨"""
    
    def __init__(self):
        self.document_manager = DocumentManager()
        self.chunkers = {
            'sentence': SentenceChunker(),
            'semantic': SemanticChunker(),
            'structure': StructureChunker()
        }
    
    def chunk_file(self, file_path: Path, strategy: str = 'sentence', 
                   **kwargs) -> List[DocumentChunk]:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†å—"""
        if strategy not in self.chunkers:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å—ç­–ç•¥: {strategy}")
        
        # è§£ææ–‡æ¡£
        document = self.document_manager.parse_document(file_path)
        if not document:
            return []
        
        # é€‰æ‹©åˆ†å—å™¨
        chunker = self.chunkers[strategy]
        
        # æ›´æ–°åˆ†å—å™¨å‚æ•°
        if kwargs:
            for key, value in kwargs.items():
                if hasattr(chunker, key):
                    setattr(chunker, key, value)
        
        # æ‰§è¡Œåˆ†å—
        if strategy == 'structure' and hasattr(chunker, 'chunk_document'):
            chunks = chunker.chunk_document(document)
        else:
            metadata = {
                'source_file': file_path.name,
                'language': document.metadata.language,
                'page_number': 1
            }
            chunks = chunker.chunk_text(document.text, metadata)
        
        logger.info(f"æ–‡ä»¶ {file_path.name} åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—")
        return chunks
    
    def chunk_directory(self, directory: Path, strategy: str = 'sentence',
                       **kwargs) -> Dict[str, List[DocumentChunk]]:
        """å¯¹ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶è¿›è¡Œåˆ†å—"""
        results = {}
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and self.document_manager.is_supported(file_path):
                try:
                    chunks = self.chunk_file(file_path, strategy, **kwargs)
                    if chunks:
                        results[str(file_path)] = chunks
                except Exception as e:
                    logger.error(f"åˆ†å—æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return results
    
    def save_chunks(self, chunks: List[DocumentChunk], output_file: Path):
        """ä¿å­˜åˆ†å—ç»“æœåˆ°æ–‡ä»¶"""
        chunk_data = []
        
        for chunk in chunks:
            chunk_dict = {
                'content': chunk.content,
                'metadata': {
                    'chunk_id': chunk.metadata.chunk_id,
                    'source_file': chunk.metadata.source_file,
                    'page_number': chunk.metadata.page_number,
                    'start_char': chunk.metadata.start_char,
                    'end_char': chunk.metadata.end_char,
                    'chunk_type': chunk.metadata.chunk_type,
                    'heading_level': chunk.metadata.heading_level,
                    'parent_heading': chunk.metadata.parent_heading,
                    'word_count': chunk.metadata.word_count,
                    'char_count': chunk.metadata.char_count,
                    'language': chunk.metadata.language,
                    'keywords': chunk.metadata.keywords
                }
            }
            chunk_data.append(chunk_dict)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†å—ç»“æœå·²ä¿å­˜åˆ° {output_file}")
    
    def load_chunks(self, input_file: Path) -> List[DocumentChunk]:
        """ä»æ–‡ä»¶åŠ è½½åˆ†å—ç»“æœ"""
        with open(input_file, 'r', encoding='utf-8') as f:
            chunk_data = json.load(f)
        
        chunks = []
        for item in chunk_data:
            from .chunker import ChunkMetadata
            
            metadata = ChunkMetadata(
                chunk_id=item['metadata']['chunk_id'],
                source_file=item['metadata']['source_file'],
                page_number=item['metadata'].get('page_number'),
                start_char=item['metadata'].get('start_char', 0),
                end_char=item['metadata'].get('end_char', 0),
                chunk_type=item['metadata'].get('chunk_type', 'text'),
                heading_level=item['metadata'].get('heading_level'),
                parent_heading=item['metadata'].get('parent_heading'),
                word_count=item['metadata'].get('word_count', 0),
                char_count=item['metadata'].get('char_count', 0),
                language=item['metadata'].get('language', 'unknown'),
                keywords=item['metadata'].get('keywords', [])
            )
            
            chunk = DocumentChunk(item['content'], metadata)
            chunks.append(chunk)
        
        logger.info(f"ä» {input_file} åŠ è½½äº† {len(chunks)} ä¸ªåˆ†å—")
        return chunks
    
    def get_chunk_statistics(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """è·å–åˆ†å—ç»Ÿè®¡ä¿¡æ¯"""
        if not chunks:
            return {}
        
        total_chunks = len(chunks)
        total_chars = sum(chunk.metadata.char_count for chunk in chunks)
        total_words = sum(chunk.metadata.word_count for chunk in chunks)
        
        chunk_sizes = [chunk.metadata.char_count for chunk in chunks]
        avg_chunk_size = total_chars / total_chunks
        min_chunk_size = min(chunk_sizes)
        max_chunk_size = max(chunk_sizes)
        
        chunk_types = {}
        for chunk in chunks:
            chunk_type = chunk.metadata.chunk_type
            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        return {
            'total_chunks': total_chunks,
            'total_characters': total_chars,
            'total_words': total_words,
            'average_chunk_size': round(avg_chunk_size, 2),
            'min_chunk_size': min_chunk_size,
            'max_chunk_size': max_chunk_size,
            'chunk_types': chunk_types
        }
```

### 3.6 åˆ›å»ºåˆ†å—æµ‹è¯•è„šæœ¬

**åˆ›å»º `test_chunking.py`ï¼š**
```python
"""åˆ†å—åŠŸèƒ½æµ‹è¯•"""

from pathlib import Path
from src.chunking.chunk_manager import ChunkManager

def test_chunking_strategies():
    """æµ‹è¯•ä¸åŒçš„åˆ†å—ç­–ç•¥"""
    print("ğŸ”¬ æµ‹è¯•åˆ†å—ç­–ç•¥...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_content = """
# äººå·¥æ™ºèƒ½ç®€ä»‹

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

## å‘å±•å†å²

äººå·¥æ™ºèƒ½çš„å‘å±•å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚1950å¹´ï¼Œè‹±å›½æ•°å­¦å®¶è‰¾ä¼¦Â·å›¾çµå‘è¡¨äº†ã€Šè®¡ç®—æœºå™¨ä¸æ™ºèƒ½ã€‹ä¸€æ–‡ï¼Œæå‡ºäº†è‘—åçš„"å›¾çµæµ‹è¯•"ã€‚

### æ—©æœŸå‘å±•

1956å¹´å¤å­£ï¼Œåœ¨ç¾å›½è¾¾ç‰¹èŒ…æ–¯å­¦é™¢ä¸¾è¡Œçš„ä¸€æ¬¡ä¼šè®®ä¸Šï¼Œ"äººå·¥æ™ºèƒ½"è¿™ä¸ªæœ¯è¯­è¢«æ­£å¼æå‡ºã€‚è¿™æ¬¡ä¼šè®®è¢«è®¤ä¸ºæ˜¯äººå·¥æ™ºèƒ½å­¦ç§‘çš„è¯ç”Ÿæ ‡å¿—ã€‚

### ç°ä»£å‘å±•

è¿›å…¥21ä¸–çºªä»¥æ¥ï¼Œéšç€è®¡ç®—èƒ½åŠ›çš„æå‡å’Œå¤§æ•°æ®çš„å‡ºç°ï¼Œäººå·¥æ™ºèƒ½è¿æ¥äº†æ–°çš„å‘å±•æœºé‡ã€‚æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ ç­‰æŠ€æœ¯å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

## ä¸»è¦åº”ç”¨é¢†åŸŸ

äººå·¥æ™ºèƒ½åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼š

1. **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ†æç­‰ã€‚
2. **è®¡ç®—æœºè§†è§‰**ï¼šå›¾åƒè¯†åˆ«ã€äººè„¸è¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚
3. **æœºå™¨å­¦ä¹ **ï¼šæ•°æ®æŒ–æ˜ã€é¢„æµ‹åˆ†æã€æ¨èç³»ç»Ÿç­‰ã€‚
4. **æœºå™¨äººæŠ€æœ¯**ï¼šå·¥ä¸šæœºå™¨äººã€æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚

## æœªæ¥å±•æœ›

äººå·¥æ™ºèƒ½æŠ€æœ¯å°†ç»§ç»­å¿«é€Ÿå‘å±•ï¼Œé¢„è®¡åœ¨æœªæ¥å‡ å¹´å†…å°†åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥æ›´å¤§çš„ä¾¿åˆ©å’Œä»·å€¼ã€‚
    """
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = Path("test_ai_document.md")
    test_file.write_text(test_content, encoding='utf-8')
    
    try:
        chunk_manager = ChunkManager()
        
        # æµ‹è¯•ä¸åŒç­–ç•¥
        strategies = ['sentence', 'semantic', 'structure']
        
        for strategy in strategies:
            print(f"\nğŸ“ æµ‹è¯• {strategy} åˆ†å—ç­–ç•¥:")
            
            # è®¾ç½®ä¸åŒçš„å‚æ•°
            kwargs = {
                'max_chunk_size': 500,
                'overlap_size': 50,
                'min_chunk_size': 100
            }
            
            if strategy == 'semantic':
                kwargs['similarity_threshold'] = 0.3
            
            chunks = chunk_manager.chunk_file(test_file, strategy, **kwargs)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"  åˆ†å—æ•°é‡: {len(chunks)}")
            
            for i, chunk in enumerate(chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"\n  åˆ†å— {i+1}:")
                print(f"    ç±»å‹: {chunk.metadata.chunk_type}")
                print(f"    å­—ç¬¦æ•°: {chunk.metadata.char_count}")
                print(f"    å†…å®¹é¢„è§ˆ: {chunk.content[:100]}...")
                if chunk.metadata.parent_heading:
                    print(f"    çˆ¶æ ‡é¢˜: {chunk.metadata.parent_heading}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = chunk_manager.get_chunk_statistics(chunks)
            print(f"\n  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"    å¹³å‡åˆ†å—å¤§å°: {stats['average_chunk_size']} å­—ç¬¦")
            print(f"    æœ€å°åˆ†å—: {stats['min_chunk_size']} å­—ç¬¦")
            print(f"    æœ€å¤§åˆ†å—: {stats['max_chunk_size']} å­—ç¬¦")
            print(f"    åˆ†å—ç±»å‹åˆ†å¸ƒ: {stats['chunk_types']}")
            
            # ä¿å­˜ç»“æœ
            output_file = Path(f"chunks_{strategy}.json")
            chunk_manager.save_chunks(chunks, output_file)
            print(f"    ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if test_file.exists():
            test_file.unlink()
        
        # æ¸…ç†è¾“å‡ºæ–‡ä»¶
        for strategy in strategies:
            output_file = Path(f"chunks_{strategy}.json")
            if output_file.exists():
                output_file.unlink()

def test_batch_chunking():
    """æµ‹è¯•æ‰¹é‡åˆ†å—"""
    print("\nğŸ”„ æµ‹è¯•æ‰¹é‡åˆ†å—...")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•å’Œæ–‡ä»¶
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
    test_files = {
        "doc1.txt": "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚å®ƒåŒ…å«ä¸€äº›ç®€å•çš„æ–‡æœ¬å†…å®¹ï¼Œç”¨äºæµ‹è¯•åˆ†å—åŠŸèƒ½ã€‚",
        "doc2.md": "# æ ‡é¢˜\n\nè¿™æ˜¯ä¸€ä¸ªMarkdownæ–‡æ¡£ã€‚\n\n## å­æ ‡é¢˜\n\nåŒ…å«ä¸€äº›ç»“æ„åŒ–å†…å®¹ã€‚",
        "doc3.txt": "å¦ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œå†…å®¹ç¨å¾®é•¿ä¸€äº›ã€‚å®ƒç”¨äºéªŒè¯æ‰¹é‡å¤„ç†åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚æˆ‘ä»¬éœ€è¦ç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½èƒ½è¢«æ­£ç¡®å¤„ç†ã€‚"
    }
    
    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        for filename, content in test_files.items():
            (test_dir / filename).write_text(content, encoding='utf-8')
        
        chunk_manager = ChunkManager()
        
        # æ‰¹é‡åˆ†å—
        results = chunk_manager.chunk_directory(
            test_dir, 
            strategy='sentence',
            max_chunk_size=200,
            overlap_size=20
        )
        
        print(f"å¤„ç†äº† {len(results)} ä¸ªæ–‡ä»¶:")
        
        for file_path, chunks in results.items():
            filename = Path(file_path).name
            print(f"  ğŸ“„ {filename}: {len(chunks)} ä¸ªåˆ†å—")
            
            for i, chunk in enumerate(chunks):
                print(f"    åˆ†å— {i+1}: {chunk.metadata.char_count} å­—ç¬¦")
    
    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if test_dir.exists():
            for file in test_dir.iterdir():
                file.unlink()
            test_dir.rmdir()

if __name__ == "__main__":
    print("å¼€å§‹åˆ†å—åŠŸèƒ½æµ‹è¯•...")
    
    try:
        test_chunking_strategies()
        test_batch_chunking()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_chunking.py
```

---

## ğŸ¤” æ€è€ƒé¢˜

1. **åˆ†å—ç­–ç•¥æ¯”è¾ƒ**ï¼š
   - åŸºäºå¥å­ã€è¯­ä¹‰å’Œç»“æ„çš„åˆ†å—ç­–ç•¥å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ
   - åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥é€‰æ‹©å“ªç§åˆ†å—ç­–ç•¥ï¼Ÿ

2. **å‚æ•°è°ƒä¼˜**ï¼š
   - `max_chunk_size`ã€`overlap_size` å’Œ `min_chunk_size` è¿™äº›å‚æ•°å¦‚ä½•å½±å“åˆ†å—æ•ˆæœï¼Ÿ
   - å¦‚ä½•æ ¹æ®ä¸åŒç±»å‹çš„æ–‡æ¡£è°ƒæ•´è¿™äº›å‚æ•°ï¼Ÿ

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å¦‚ä½•ä¼˜åŒ–å¤§æ–‡æ¡£çš„åˆ†å—å¤„ç†æ€§èƒ½ï¼Ÿ
   - è¯­ä¹‰åˆ†å—ä¸­çš„TF-IDFè®¡ç®—æ˜¯å¦å¯ä»¥ç”¨å…¶ä»–æ–¹æ³•æ›¿ä»£ï¼Ÿ

4. **è´¨é‡è¯„ä¼°**ï¼š
   - å¦‚ä½•è¯„ä¼°åˆ†å—è´¨é‡çš„å¥½åï¼Ÿ
   - è®¾è®¡ä¸€ä¸ªåˆ†å—è´¨é‡è¯„ä¼°æŒ‡æ ‡ã€‚

5. **æ‰©å±•åŠŸèƒ½**ï¼š
   - å¦‚ä½•å¤„ç†å¤šè¯­è¨€æ–‡æ¡£çš„åˆ†å—ï¼Ÿ
   - å¦‚ä½•ä¿æŒè¡¨æ ¼å’Œä»£ç å—çš„å®Œæ•´æ€§ï¼Ÿ

---

## âœ… å®éªŒæ£€æŸ¥æ¸…å•

- [ ] **ç¯å¢ƒå‡†å¤‡**
  - [ ] å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åŒ…
  - [ ] å‡†å¤‡æµ‹è¯•æ–‡æ¡£ï¼ˆPDFã€Wordã€æ–‡æœ¬ç­‰ï¼‰
  - [ ] éªŒè¯PyMuPDFå’Œå…¶ä»–è§£æåº“æ­£å¸¸å·¥ä½œ

- [ ] **æ–‡æ¡£è§£æ**
  - [ ] å®ç°PDFè§£æå™¨ï¼Œèƒ½å¤Ÿæå–æ–‡æœ¬ã€å…ƒæ•°æ®å’Œç»“æ„ä¿¡æ¯
  - [ ] å®ç°Wordæ–‡æ¡£è§£æå™¨
  - [ ] å®ç°æ–‡æœ¬æ–‡ä»¶è§£æå™¨
  - [ ] åˆ›å»ºç»Ÿä¸€çš„æ–‡æ¡£ç®¡ç†å™¨

- [ ] **åˆ†å—åŠŸèƒ½**
  - [ ] å®ç°åŸºäºå¥å­çš„åˆ†å—ç­–ç•¥
  - [ ] å®ç°åŸºäºè¯­ä¹‰çš„åˆ†å—ç­–ç•¥
  - [ ] å®ç°åŸºäºç»“æ„çš„åˆ†å—ç­–ç•¥
  - [ ] åˆ›å»ºåˆ†å—ç®¡ç†å™¨

- [ ] **æµ‹è¯•éªŒè¯**
  - [ ] æµ‹è¯•ä¸åŒæ ¼å¼æ–‡æ¡£çš„è§£æ
  - [ ] æµ‹è¯•ä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœ
  - [ ] éªŒè¯åˆ†å—ç»“æœçš„ä¿å­˜å’ŒåŠ è½½
  - [ ] æ£€æŸ¥åˆ†å—ç»Ÿè®¡ä¿¡æ¯çš„å‡†ç¡®æ€§

- [ ] **ä»£ç è´¨é‡**
  - [ ] ä»£ç ç¬¦åˆPEP 8è§„èŒƒ
  - [ ] æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†
  - [ ] åŒ…å«è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²
  - [ ] é€šè¿‡ç±»å‹æ£€æŸ¥

---

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### 1. PyMuPDFå®‰è£…é—®é¢˜
```bash
# å¦‚æœuvå®‰è£…å¤±è´¥ï¼Œå°è¯•ï¼š
conda install -c conda-forge pymupdf
# æˆ–è€…
uv pip install --upgrade pip
uv add PyMuPDF
```

### 2. NLTKæ•°æ®ä¸‹è½½é—®é¢˜
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

### 3. ç¼–ç é—®é¢˜
```python
# ç¡®ä¿æ–‡ä»¶è¯»å–æ—¶æŒ‡å®šæ­£ç¡®ç¼–ç 
with open(file_path, 'r', encoding='utf-8') as f:
    content = f.read()
```

### 4. å†…å­˜ä½¿ç”¨è¿‡å¤š
```python
# å¤„ç†å¤§æ–‡ä»¶æ—¶ä½¿ç”¨æµå¼å¤„ç†
def process_large_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # é€è¡Œå¤„ç†
            yield line.strip()
```

### 5. åˆ†å—è´¨é‡ä¸ä½³
- è°ƒæ•´ `max_chunk_size` å‚æ•°
- å°è¯•ä¸åŒçš„åˆ†å—ç­–ç•¥
- æ£€æŸ¥æ–‡æ¡£é¢„å¤„ç†æ˜¯å¦å……åˆ†
- è€ƒè™‘æ–‡æ¡£ç‰¹å®šçš„åˆ†å—è§„åˆ™

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **PyMuPDFå®˜æ–¹æ–‡æ¡£**ï¼šhttps://pymupdf.readthedocs.io/
2. **python-docxæ–‡æ¡£**ï¼šhttps://python-docx.readthedocs.io/
3. **NLTKæ–‡æ¡£**ï¼šhttps://www.nltk.org/
4. **scikit-learn TF-IDF**ï¼šhttps://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction
5. **æ–‡æ¡£åˆ†å—æœ€ä½³å®è·µ**ï¼šç›¸å…³å­¦æœ¯è®ºæ–‡å’ŒæŠ€æœ¯åšå®¢

---

## ğŸ“ å®éªŒå®Œæˆåçš„Gitæ“ä½œ

### ä¸ºä»€ä¹ˆè¦è¿›è¡ŒGitæäº¤ï¼Ÿ

å®ŒæˆPDFè§£æä¸æ–‡æ¡£åˆ†å—å®éªŒåï¼Œè¿›è¡ŒGitæäº¤éå¸¸é‡è¦ï¼š

1. **æ–‡æ¡£å¤„ç†ä»£ç ä¿å­˜**ï¼šä¿å­˜PDFè§£æå™¨ã€æ–‡æ¡£ç»“æ„è¯†åˆ«å’Œåˆ†å—ç­–ç•¥çš„å®ç°ä»£ç 
2. **ç®—æ³•ç‰ˆæœ¬ç®¡ç†**ï¼šè®°å½•ä¸åŒåˆ†å—ç®—æ³•çš„æ¼”è¿›è¿‡ç¨‹ï¼Œä¾¿äºåç»­ä¼˜åŒ–å’Œè°ƒè¯•
3. **é…ç½®æ–‡ä»¶ç®¡ç†**ï¼šä¿å­˜åˆ†å—å‚æ•°é…ç½®ã€æ–‡æ¡£ç±»å‹æ˜ å°„ç­‰é‡è¦é…ç½®
4. **æµ‹è¯•æ•°æ®ç®¡ç†**ï¼šä¿å­˜æµ‹è¯•ç”¨çš„PDFæ–‡ä»¶å’Œé¢„æœŸè¾“å‡ºç»“æœ
5. **å›¢é˜Ÿåä½œåŸºç¡€**ï¼šä¸ºå›¢é˜Ÿæˆå‘˜æä¾›å®Œæ•´çš„æ–‡æ¡£å¤„ç†åŠŸèƒ½å®ç°
6. **å­¦ä¹ è¿›åº¦è®°å½•**ï¼šæ ‡è®°PDFè§£æå’Œæ™ºèƒ½åˆ†å—æŠ€æœ¯çš„å­¦ä¹ é‡Œç¨‹ç¢‘

### Gitæ“ä½œæ­¥éª¤

#### 1. æ£€æŸ¥å½“å‰ä¿®æ”¹çŠ¶æ€
```bash
git status
```

é¢„æœŸä¼šçœ‹åˆ°ä»¥ä¸‹ç±»å‹çš„æ–‡ä»¶å˜æ›´ï¼š
- `src/document_processor/` - PDFè§£æå™¨ç›¸å…³ä»£ç 
- `src/chunking/` - æ–‡æ¡£åˆ†å—ç­–ç•¥å®ç°
- `src/models/document.py` - æ–‡æ¡£æ•°æ®æ¨¡å‹
- `tests/test_pdf_parser.py` - PDFè§£ææµ‹è¯•
- `tests/test_chunking.py` - åˆ†å—åŠŸèƒ½æµ‹è¯•
- `config/chunking_config.yaml` - åˆ†å—é…ç½®æ–‡ä»¶
- `requirements.txt` - æ–°å¢çš„ä¾èµ–åŒ…

#### 2. æ·»åŠ æ‰€æœ‰ä¿®æ”¹çš„æ–‡ä»¶
```bash
git add .
```

#### 3. æäº¤æ›´æ”¹
```bash
git commit -m "å®Œæˆlesson04å®éªŒï¼šPDFè§£æä¸æ™ºèƒ½åˆ†å—åŠŸèƒ½å®ç°

- å®ç°PDFæ–‡æ¡£è§£æå™¨ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼æå–
- å¼€å‘å¤šç§åˆ†å—ç­–ç•¥ï¼šå›ºå®šé•¿åº¦ã€è¯­ä¹‰åˆ†å—ã€ç»“æ„åŒ–åˆ†å—
- æ·»åŠ æ–‡æ¡£ç»“æ„è¯†åˆ«åŠŸèƒ½ï¼šæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨æ£€æµ‹
- å®ç°åˆ†å—è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–æœºåˆ¶
- å®Œå–„åˆ†å—å…ƒæ•°æ®ç®¡ç†å’Œç´¢å¼•åŠŸèƒ½
- æ·»åŠ comprehensiveæµ‹è¯•ç”¨ä¾‹å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•"
```

#### 4. æŸ¥çœ‹æäº¤å†å²
```bash
git log --oneline -5
```

#### 5. ï¼ˆå¯é€‰ï¼‰æ¨é€åˆ°è¿œç¨‹ä»“åº“
å¦‚æœä½ ä½¿ç”¨è¿œç¨‹Gitä»“åº“ï¼Œå¯ä»¥æ¨é€ä½ çš„æ›´æ”¹ï¼š
```bash
git push origin lesson04
```

### æ–‡æ¡£å¤„ç†é¡¹ç›®çš„ç‰¹æ®Šæ³¨æ„äº‹é¡¹

#### æäº¤å‰éªŒè¯æ¸…å•
1. **è§£æåŠŸèƒ½éªŒè¯**ï¼šç¡®ä¿PDFè§£æå™¨èƒ½æ­£ç¡®å¤„ç†å„ç§æ ¼å¼çš„æ–‡æ¡£
2. **åˆ†å—è´¨é‡æ£€æŸ¥**ï¼šéªŒè¯åˆ†å—ç»“æœçš„è¯­ä¹‰å®Œæ•´æ€§å’Œå¤§å°åˆç†æ€§
3. **æ€§èƒ½æµ‹è¯•**ï¼šç¡®è®¤å¤§æ–‡ä»¶å¤„ç†çš„å†…å­˜ä½¿ç”¨å’Œå¤„ç†é€Ÿåº¦
4. **é…ç½®æ–‡ä»¶æ£€æŸ¥**ï¼šç¡®ä¿åˆ†å—å‚æ•°é…ç½®åˆç†ä¸”æ–‡æ¡£å®Œæ•´
5. **æµ‹è¯•è¦†ç›–ç‡**ï¼šéªŒè¯å…³é”®åŠŸèƒ½éƒ½æœ‰å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹

#### æ•æ„Ÿä¿¡æ¯å¤„ç†
- ç¡®ä¿æµ‹è¯•ç”¨çš„PDFæ–‡ä»¶ä¸åŒ…å«æ•æ„Ÿæˆ–ç‰ˆæƒä¿æŠ¤å†…å®¹
- æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«æ–‡æ¡£å†…å®¹ç‰‡æ®µ
- éªŒè¯ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜ç›®å½•å·²è¢«æ­£ç¡®å¿½ç•¥

### å¸¸è§é—®é¢˜è§£å†³

#### é—®é¢˜1ï¼šå¤§æ–‡ä»¶æ— æ³•æäº¤
```bash
# æ£€æŸ¥æ–‡ä»¶å¤§å°
find . -name "*.pdf" -size +50M

# å°†å¤§æ–‡ä»¶æ·»åŠ åˆ°.gitignore
echo "*.pdf" >> .gitignore
echo "test_data/large_files/" >> .gitignore
```

#### é—®é¢˜2ï¼šæƒ³è¦ä¿®æ”¹æäº¤ä¿¡æ¯
```bash
# ä¿®æ”¹æœ€è¿‘ä¸€æ¬¡æäº¤çš„ä¿¡æ¯
git commit --amend -m "æ–°çš„æäº¤ä¿¡æ¯"
```

#### é—®é¢˜3ï¼šè¯¯æ·»åŠ äº†ä¸éœ€è¦çš„æ–‡ä»¶
```bash
# ä»æš‚å­˜åŒºç§»é™¤æ–‡ä»¶ä½†ä¿ç•™æœ¬åœ°ä¿®æ”¹
git reset HEAD æ–‡ä»¶å

# æˆ–è€…ç§»é™¤æ‰€æœ‰æš‚å­˜çš„æ–‡ä»¶
git reset HEAD .
```

#### é—®é¢˜4ï¼šåˆ†å—ç®—æ³•æ€§èƒ½é—®é¢˜
```bash
# åˆ›å»ºæ€§èƒ½ä¼˜åŒ–åˆ†æ”¯
git checkout -b optimize-chunking

# æäº¤ä¼˜åŒ–åçš„ä»£ç 
git commit -m "ä¼˜åŒ–åˆ†å—ç®—æ³•æ€§èƒ½ï¼šå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œæå‡å¤„ç†é€Ÿåº¦"
```

### æ–‡æ¡£å¤„ç†é¡¹ç›®Gitæœ€ä½³å®è·µ

1. **åˆ†å±‚æäº¤**ï¼šå°†è§£æå™¨ã€åˆ†å—å™¨ã€æµ‹è¯•åˆ†åˆ«æäº¤
2. **ç®—æ³•æ ‡è®°**ï¼šä¸ºé‡è¦çš„ç®—æ³•ç‰ˆæœ¬æ‰“æ ‡ç­¾
3. **é…ç½®ç®¡ç†**ï¼šå°†é…ç½®æ–‡ä»¶å˜æ›´å•ç‹¬æäº¤å¹¶è¯¦ç»†è¯´æ˜
4. **æµ‹è¯•æ•°æ®**ï¼šä½¿ç”¨Git LFSç®¡ç†å¤§å‹æµ‹è¯•æ–‡æ¡£
5. **æ€§èƒ½åŸºå‡†**ï¼šè®°å½•æ€§èƒ½æµ‹è¯•ç»“æœå’Œä¼˜åŒ–å†ç¨‹

### ä¸‹ä¸€æ­¥å­¦ä¹ æŒ‡å¯¼

å®ŒæˆGitæäº¤åï¼Œä½ å¯ä»¥ï¼š
1. å›é¡¾æœ¬æ¬¡å®éªŒçš„åˆ†å—ç­–ç•¥å®ç°
2. å‡†å¤‡è¿›å…¥lesson05ï¼šEmbeddingä¸å‘é‡å…¥åº“
3. æ€è€ƒå¦‚ä½•å°†æ–‡æ¡£åˆ†å—ä¸å‘é‡åŒ–æŠ€æœ¯ç»“åˆ
4. é¢„ä¹ å‘é‡æ•°æ®åº“çš„åŸºæœ¬æ¦‚å¿µå’Œæ“ä½œ

## ğŸ¯ å®éªŒå®Œæˆæ ‡å¿—

å½“ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡æ—¶ï¼Œè¯´æ˜å®éªŒæˆåŠŸï¼š

1. âœ… èƒ½å¤Ÿè§£æå¤šç§æ ¼å¼çš„æ–‡æ¡£ï¼ˆPDFã€Wordã€æ–‡æœ¬ï¼‰
2. âœ… å®ç°äº†ä¸‰ç§ä¸åŒçš„åˆ†å—ç­–ç•¥
3. âœ… åˆ†å—ç»“æœåŒ…å«å®Œæ•´çš„å…ƒæ•°æ®ä¿¡æ¯
4. âœ… èƒ½å¤Ÿä¿å­˜å’ŒåŠ è½½åˆ†å—ç»“æœ
5. âœ… æµ‹è¯•è„šæœ¬è¿è¡Œæ­£å¸¸ï¼Œè¾“å‡ºåˆç†çš„ç»Ÿè®¡ä¿¡æ¯
6. âœ… ä»£ç ç»“æ„æ¸…æ™°ï¼Œé”™è¯¯å¤„ç†å®Œå–„
7. âœ… Gitæäº¤å®Œæˆï¼šå·²å°†æ‰€æœ‰ä»£ç å˜æ›´æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ

**æ­å–œï¼ä½ å·²ç»æŒæ¡äº†æ–‡æ¡£è§£æå’Œæ™ºèƒ½åˆ†å—çš„æ ¸å¿ƒæŠ€æœ¯ï¼** ğŸ‰

### 3.3 å®ç°åŸºäºè¯­ä¹‰çš„åˆ†å—ç­–ç•¥

**åˆ›å»º `src/chunking/semantic_chunker.py`ï¼š**
```python
"""åŸºäºè¯­ä¹‰çš„åˆ†å—å™¨"""

import re
from typing import List, Dict, Any, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from .sentence_chunker import SentenceChunker
from .chunker import DocumentChunk, ChunkMetadata

class SemanticChunker(SentenceChunker):
    """åŸºäºè¯­ä¹‰çš„åˆ†å—å™¨"""
    
    def __init__(self, 
                 max_chunk_size: int = 1000,
                 overlap_size: int = 100,
                 min_chunk_size: int = 50,
                 similarity_threshold: float = 0.5,
                 language: str = 'english'):
        super().__init__(max_chunk_size, overlap_size, min_chunk_size, language)
        self.similarity_threshold = similarity_threshold
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english' if language == 'english' else None,
            ngram_range=(1, 2)
        )
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§åˆ†å—"""
        if not text.strip():
            return []
        
        metadata = metadata or {}
        source_file = metadata.get('source_file', 'unknown')
        
        # åˆ†å‰²å¥å­
        sentences = self._split_sentences(text)
        
        if len(sentences) < 2:
            # å¦‚æœå¥å­å¤ªå°‘ï¼Œç›´æ¥è¿”å›
            return [self._create_chunk(' '.join(sentences), source_file, 0, metadata)]
        
        # è®¡ç®—å¥å­ç›¸ä¼¼æ€§
        similarity_matrix = self._calculate_similarity_matrix(sentences)
        
        # åŸºäºç›¸ä¼¼æ€§åˆ†ç»„
        sentence_groups = self._group_sentences_by_similarity(
            sentences, similarity_matrix
        )
        
        # åˆ›å»ºåˆ†å—
        chunks = []
        for i, group in enumerate(sentence_groups):
            chunk_content = ' '.join(group)
            
            # å¦‚æœåˆ†å—å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(chunk_content) > self.max_chunk_size:
                sub_chunks = self._split_large_chunk(group, source_file, len(chunks), metadata)
                chunks.extend(sub_chunks)
            else:
                chunk = self._create_chunk(chunk_content, source_file, len(chunks), metadata)
                chunks.append(chunk)
        
        # åˆå¹¶è¿‡å°çš„åˆ†å—
        return self._merge_chunks(chunks)
    
    def _calculate_similarity_matrix(self, sentences: List[str]) -> np.ndarray:
        """è®¡ç®—å¥å­ç›¸ä¼¼æ€§çŸ©é˜µ"""
        try:
            # ä½¿ç”¨TF-IDFå‘é‡åŒ–
            tfidf_matrix = self.vectorizer.fit_transform(sentences)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            return similarity_matrix
        
        except Exception as e:
            logger.warning(f"è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„")
            # å¦‚æœå¤±è´¥ï¼Œè¿”å›å•ä½çŸ©é˜µï¼ˆæ¯ä¸ªå¥å­è‡ªæˆä¸€ç»„ï¼‰
            return np.eye(len(sentences))
    
    def _group_sentences_by_similarity(self, sentences: List[str], 
                                     similarity_matrix: np.ndarray) -> List[List[str]]:
        """åŸºäºç›¸ä¼¼æ€§åˆ†ç»„å¥å­"""
        n_sentences = len(sentences)
        visited = [False] * n_sentences
        groups = []
        
        for i in range(n_sentences):
            if visited[i]:
                continue
            
            # å¼€å§‹æ–°çš„ç»„
            current_group = [sentences[i]]
            visited[i] = True
            current_size = len(sentences[i])
            
            # å¯»æ‰¾ç›¸ä¼¼çš„å¥å­
            for j in range(i + 1, n_sentences):
                if visited[j]:
                    continue
                
                # æ£€æŸ¥ç›¸ä¼¼æ€§å’Œå¤§å°é™åˆ¶
                if (similarity_matrix[i][j] >= self.similarity_threshold and
                    current_size + len(sentences[j]) <= self.max_chunk_size):
                    
                    current_group.append(sentences[j])
                    visited[j] = True
                    current_size += len(sentences[j])
            
            groups.append(current_group)
        
        return groups
    
    def _split_large_chunk(self, sentences: List[str], source_file: str, 
                          start_index: int, metadata: Dict[str, Any]) -> List[DocumentChunk]:
        """åˆ†å‰²è¿‡å¤§çš„åˆ†å—"""
        chunks = []
        current_sentences = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > self.max_chunk_size and current_sentences:
                # åˆ›å»ºå½“å‰åˆ†å—
                chunk_content = ' '.join(current_sentences)
                chunk = self._create_chunk(
                    chunk_content, 
                    source_file, 
                    start_index + len(chunks),
                    metadata
                )
                chunks.append(chunk)
                
                # å¤„ç†é‡å 
                if self.overlap_size > 0:
                    overlap_sentences = self._get_overlap_sentences(
                        current_sentences, self.overlap_size
                    )
                    current_sentences = overlap_sentences + [sentence]
                    current_size = sum(len(s) for s in current_sentences)
                else:
                    current_sentences = [sentence]
                    current_size = sentence_size
            else:
                current_sentences.append(sentence)
                current_size += sentence_size
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
        if current_sentences:
            chunk_content = ' '.join(current_sentences)
            chunk = self._create_chunk(
                chunk_content, 
                source_file, 
                start_index + len(chunks),
                metadata
            )
            chunks.append(chunk)
        
        return chunks
    
    def _create_chunk(self, content: str, source_file: str, index: int, 
                     metadata: Dict[str, Any]) -> DocumentChunk:
        """åˆ›å»ºæ–‡æ¡£åˆ†å—"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=metadata.get('page_number'),
            chunk_type="semantic",
            language=metadata.get('language', 'unknown'),
            parent_heading=metadata.get('parent_heading')
        )
        
        return DocumentChunk(content, chunk_metadata)
```

### 3.4 å®ç°åŸºäºç»“æ„çš„åˆ†å—ç­–ç•¥

**åˆ›å»º `src/chunking/structure_chunker.py`ï¼š**
```python
"""åŸºäºæ–‡æ¡£ç»“æ„çš„åˆ†å—å™¨"""

from typing import List, Dict, Any
from ..document.parser import DocumentContent
from .chunker import ChunkingStrategy, DocumentChunk, ChunkMetadata

class StructureChunker(ChunkingStrategy):
    """åŸºäºæ–‡æ¡£ç»“æ„çš„åˆ†å—å™¨"""
    
    def __init__(self, 
                 max_chunk_size: int = 1500,
                 overlap_size: int = 150,
                 min_chunk_size: int = 100,
                 preserve_structure: bool = True):
        super().__init__(max_chunk_size, overlap_size, min_chunk_size)
        self.preserve_structure = preserve_structure
    
    def chunk_document(self, document: DocumentContent) -> List[DocumentChunk]:
        """åŸºäºæ–‡æ¡£ç»“æ„åˆ†å—"""
        chunks = []
        
        # å¤„ç†æ ‡é¢˜å’Œæ®µè½
        if document.structure.get('headings') and document.structure.get('paragraphs'):
            chunks.extend(self._chunk_by_headings(document))
        else:
            # å¦‚æœæ²¡æœ‰ç»“æ„ä¿¡æ¯ï¼ŒæŒ‰æ®µè½åˆ†å—
            chunks.extend(self._chunk_by_paragraphs(document))
        
        # å¤„ç†è¡¨æ ¼
        if document.tables:
            chunks.extend(self._chunk_tables(document))
        
        # å¤„ç†ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼‰
        if document.structure.get('code_blocks'):
            chunks.extend(self._chunk_code_blocks(document))
        
        return chunks
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """ç®€å•æ–‡æœ¬åˆ†å—ï¼ˆå…¼å®¹åŸºç±»æ¥å£ï¼‰"""
        metadata = metadata or {}
        source_file = metadata.get('source_file', 'unknown')
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            if len(current_chunk) + len(paragraph) <= self.max_chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunk = self._create_chunk(
                        current_chunk.strip(), 
                        source_file, 
                        len(chunks), 
                        metadata
                    )
                    chunks.append(chunk)
                
                current_chunk = paragraph + "\n\n"
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk:
            chunk = self._create_chunk(
                current_chunk.strip(), 
                source_file, 
                len(chunks), 
                metadata
            )
            chunks.append(chunk)
        
        return self._merge_chunks(chunks)
    
    def _chunk_by_headings(self, document: DocumentContent) -> List[DocumentChunk]:
        """åŸºäºæ ‡é¢˜ç»“æ„åˆ†å—"""
        chunks = []
        headings = document.structure['headings']
        paragraphs = document.structure['paragraphs']
        
        # ä¸ºæ¯ä¸ªæ ‡é¢˜åˆ›å»ºåˆ†å—
        for i, heading in enumerate(headings):
            chunk_content = heading['text'] + "\n\n"
            current_size = len(chunk_content)
            
            # æŸ¥æ‰¾å±äºè¿™ä¸ªæ ‡é¢˜çš„æ®µè½
            next_heading_page = headings[i + 1]['page_number'] if i + 1 < len(headings) else float('inf')
            
            for paragraph in paragraphs:
                # åˆ¤æ–­æ®µè½æ˜¯å¦å±äºå½“å‰æ ‡é¢˜
                if (paragraph['page_number'] >= heading['page_number'] and
                    paragraph['page_number'] < next_heading_page):
                    
                    para_text = paragraph['text']
                    
                    if current_size + len(para_text) <= self.max_chunk_size:
                        chunk_content += para_text + "\n\n"
                        current_size += len(para_text)
                    else:
                        # å½“å‰åˆ†å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°åˆ†å—
                        if chunk_content.strip():
                            chunk = self._create_structure_chunk(
                                chunk_content.strip(),
                                document.metadata.file_name,
                                len(chunks),
                                heading,
                                "heading_section"
                            )
                            chunks.append(chunk)
                        
                        # å¼€å§‹æ–°åˆ†å—
                        chunk_content = para_text + "\n\n"
                        current_size = len(para_text)
            
            # ä¿å­˜æœ€åä¸€ä¸ªåˆ†å—
            if chunk_content.strip():
                chunk = self._create_structure_chunk(
                    chunk_content.strip(),
                    document.metadata.file_name,
                    len(chunks),
                    heading,
                    "heading_section"
                )
                chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_paragraphs(self, document: DocumentContent) -> List[DocumentChunk]:
        """åŸºäºæ®µè½åˆ†å—"""
        chunks = []
        paragraphs = document.structure.get('paragraphs', [])
        
        current_chunk = ""
        current_size = 0
        
        for paragraph in paragraphs:
            para_text = paragraph['text']
            para_size = len(para_text)
            
            if current_size + para_size <= self.max_chunk_size:
                current_chunk += para_text + "\n\n"
                current_size += para_size
            else:
                # ä¿å­˜å½“å‰åˆ†å—
                if current_chunk.strip():
                    chunk = self._create_chunk(
                        current_chunk.strip(),
                        document.metadata.file_name,
                        len(chunks),
                        {'page_number': paragraph.get('page_number')}
                    )
                    chunks.append(chunk)
                
                # å¼€å§‹æ–°åˆ†å—
                current_chunk = para_text + "\n\n"
                current_size = para_size
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk.strip():
            chunk = self._create_chunk(
                current_chunk.strip(),
                document.metadata.file_name,
                len(chunks),
                {}
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_tables(self, document: DocumentContent) -> List[DocumentChunk]:
        """å¤„ç†è¡¨æ ¼åˆ†å—"""
        chunks = []
        
        for i, table in enumerate(document.tables):
            table_text = table.get('text', '')
            if not table_text:
                continue
            
            chunk_metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(document.metadata.file_name, f"table_{i}"),
                source_file=document.metadata.file_name,
                page_number=table.get('page_number'),
                chunk_type="table",
                language=document.metadata.language
            )
            
            chunk = DocumentChunk(table_text, chunk_metadata)
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_code_blocks(self, document: DocumentContent) -> List[DocumentChunk]:
        """å¤„ç†ä»£ç å—åˆ†å—"""
        chunks = []
        code_blocks = document.structure.get('code_blocks', [])
        
        for i, code_block in enumerate(code_blocks):
            code_content = code_block.get('content', '')
            if not code_content:
                continue
            
            chunk_metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(document.metadata.file_name, f"code_{i}"),
                source_file=document.metadata.file_name,
                chunk_type="code",
                language="code"
            )
            
            chunk = DocumentChunk(code_content, chunk_metadata)
            chunks.append(chunk)
        
        return chunks
    
    def _create_structure_chunk(self, content: str, source_file: str, index: int,
                              heading: Dict[str, Any], chunk_type: str) -> DocumentChunk:
        """åˆ›å»ºç»“æ„åŒ–åˆ†å—"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=heading.get('page_number'),
            chunk_type=chunk_type,
            heading_level=heading.get('level'),
            parent_heading=heading.get('text')
        )
        
        return DocumentChunk(content, chunk_metadata)
    
    def _create_chunk(self, content: str, source_file: str, index: int, 
                     metadata: Dict[str, Any]) -> DocumentChunk:
        """åˆ›å»ºæ™®é€šåˆ†å—"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=metadata.get('page_number'),
            chunk_type="paragraph"
        )
        
        return DocumentChunk(content, chunk_metadata)
```