# 第四课：PDF解析与Chunk拆分 - 学生实验指导

## 代码基础准备

在开始本节课的实验之前，我们需要基于上一节课的代码继续开发。现在我们使用Git分支管理来获取代码。

### 步骤1：进入项目目录并切换分支

```bash
# 进入rag-system项目目录
cd rag-system

# 切换到lesson04分支
git checkout lesson04

# 验证当前分支
git branch
# 应该显示 * lesson04
```

### 步骤2：验证代码状态

```bash
# 检查项目结构
ls -la
# 应该看到：src/ scripts/ alembic/ docker-compose.yml 等文件和目录

# 检查PDF解析相关文件
ls -la src/document/
# 应该看到：parser.py pdf_parser.py 等文件
```

### 步骤3：验证数据模型环境

```bash
# 启动依赖服务
docker-compose up -d

# 运行数据库迁移
alembic upgrade head

# 验证数据模型
python -c "from src.models import User, Document; print('数据模型导入成功')"
```

**说明**：lesson04分支包含了lesson03的所有代码，并新增了PDF解析和Chunk拆分相关的模块和配置。

---

## 🎯 实验目标

通过本次实验，你将学会：
- 使用PyMuPDF解析PDF文档
- 提取文档的文本、图片和元数据
- 实现智能的文档结构识别
- 设计和实现多种Chunk拆分策略
- 处理复杂文档格式和边界情况

---

## 🛠️ 环境准备

### 安装依赖
```bash
# 安装文档处理相关依赖
uv add pymupdf python-docx python-pptx
uv add langdetect pillow
uv add nltk spacy

# 下载NLTK数据（用于句子分割）
python -c "import nltk; nltk.download('punkt')"
```

### 准备测试文档
```bash
# 创建测试文档目录
mkdir -p test_documents

# 准备不同类型的测试文档
# - 学术论文PDF
# - 技术手册PDF  
# - Word文档
# - PowerPoint文档
# - 纯文本文件
```

---

## 🔬 实验一：PyMuPDF基础操作

### 1.1 创建文档解析器基类

**创建 `src/document/parser.py`：**
```python
"""文档解析器模块"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging
from dataclasses import dataclass
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """文档元数据"""
    title: str = ""
    author: str = ""
    subject: str = ""
    creator: str = ""
    producer: str = ""
    creation_date: Optional[datetime] = None
    modification_date: Optional[datetime] = None
    page_count: int = 0
    file_size: int = 0
    file_name: str = ""
    file_extension: str = ""
    language: str = "unknown"
    has_images: bool = False
    has_tables: bool = False
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

@dataclass
class DocumentContent:
    """文档内容"""
    text: str
    metadata: DocumentMetadata
    pages: List[Dict[str, Any]] = None
    images: List[Dict[str, Any]] = None
    tables: List[Dict[str, Any]] = None
    structure: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.pages is None:
            self.pages = []
        if self.images is None:
            self.images = []
        if self.tables is None:
            self.tables = []
        if self.structure is None:
            self.structure = {}

class DocumentParser(ABC):
    """文档解析器基类"""
    
    def __init__(self):
        self.supported_extensions = set()
    
    @abstractmethod
    def parse(self, file_path: Path) -> DocumentContent:
        """解析文档"""
        pass
    
    def can_parse(self, file_path: Path) -> bool:
        """检查是否支持解析该文件"""
        return file_path.suffix.lower() in self.supported_extensions
    
    def validate_file(self, file_path: Path) -> bool:
        """验证文件是否有效"""
        if not file_path.exists():
            logger.error(f"文件不存在: {file_path}")
            return False
        
        if not file_path.is_file():
            logger.error(f"不是有效文件: {file_path}")
            return False
        
        if file_path.stat().st_size == 0:
            logger.error(f"文件为空: {file_path}")
            return False
        
        return True
```

### 1.2 实现PDF解析器

**创建 `src/document/pdf_parser.py`：**
```python
"""PDF文档解析器"""

import fitz  # PyMuPDF
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
import re

from .parser import DocumentParser, DocumentContent, DocumentMetadata

class PDFParser(DocumentParser):
    """PDF文档解析器"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = {".pdf"}
    
    def parse(self, file_path: Path) -> DocumentContent:
        """解析PDF文档"""
        if not self.validate_file(file_path):
            raise ValueError(f"无效的PDF文件: {file_path}")
        
        try:
            doc = fitz.open(str(file_path))
            
            # 提取元数据
            metadata = self._extract_metadata(doc, file_path)
            
            # 提取内容
            text, pages = self._extract_text_and_pages(doc)
            
            # 提取图片
            images = self._extract_images(doc)
            
            # 识别文档结构
            structure = self._analyze_structure(doc)
            
            # 检测表格
            tables = self._detect_tables(doc)
            
            doc.close()
            
            return DocumentContent(
                text=text,
                metadata=metadata,
                pages=pages,
                images=images,
                tables=tables,
                structure=structure
            )
            
        except Exception as e:
            logger.error(f"PDF解析失败: {e}")
            raise
    
    def _extract_metadata(self, doc: fitz.Document, file_path: Path) -> DocumentMetadata:
        """提取PDF元数据"""
        metadata = doc.metadata
        
        # 解析日期
        creation_date = self._parse_pdf_date(metadata.get("creationDate", ""))
        modification_date = self._parse_pdf_date(metadata.get("modDate", ""))
        
        # 检测语言
        language = self._detect_language(doc)
        
        return DocumentMetadata(
            title=metadata.get("title", "").strip(),
            author=metadata.get("author", "").strip(),
            subject=metadata.get("subject", "").strip(),
            creator=metadata.get("creator", "").strip(),
            producer=metadata.get("producer", "").strip(),
            creation_date=creation_date,
            modification_date=modification_date,
            page_count=doc.page_count,
            file_size=os.path.getsize(file_path),
            file_name=file_path.name,
            file_extension=file_path.suffix,
            language=language,
            has_images=self._has_images(doc),
            has_tables=self._has_tables(doc),
            keywords=self._extract_keywords(metadata.get("keywords", ""))
        )
    
    def _parse_pdf_date(self, date_str: str) -> Optional[datetime]:
        """解析PDF日期格式"""
        if not date_str:
            return None
        
        # PDF日期格式: D:YYYYMMDDHHmmSSOHH'mm'
        date_pattern = r"D:(\d{4})(\d{2})(\d{2})(\d{2})(\d{2})(\d{2})"
        match = re.match(date_pattern, date_str)
        
        if match:
            year, month, day, hour, minute, second = map(int, match.groups())
            try:
                return datetime(year, month, day, hour, minute, second)
            except ValueError:
                pass
        
        return None
    
    def _detect_language(self, doc: fitz.Document) -> str:
        """检测文档语言"""
        # 提取前几页文本进行语言检测
        sample_text = ""
        max_pages = min(3, doc.page_count)
        
        for page_num in range(max_pages):
            page_text = doc[page_num].get_text()
            sample_text += page_text[:1000]  # 每页最多1000字符
        
        if not sample_text.strip():
            return "unknown"
        
        try:
            from langdetect import detect
            return detect(sample_text)
        except Exception:
            # 简单的中英文检测
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', sample_text))
            english_chars = len(re.findall(r'[a-zA-Z]', sample_text))
            
            if chinese_chars > english_chars:
                return "zh"
            elif english_chars > 0:
                return "en"
            else:
                return "unknown"
    
    def _extract_text_and_pages(self, doc: fitz.Document) -> tuple[str, List[Dict]]:
        """提取文本和页面信息"""
        full_text = ""
        pages = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            
            # 提取页面文本
            page_text = page.get_text()
            full_text += page_text + "\n\n"
            
            # 获取页面详细信息
            text_dict = page.get_text("dict")
            
            page_info = {
                "page_number": page_num + 1,
                "text": page_text,
                "bbox": page.rect,
                "width": page.rect.width,
                "height": page.rect.height,
                "blocks": self._process_text_blocks(text_dict.get("blocks", [])),
                "fonts": self._extract_fonts(text_dict.get("blocks", [])),
                "word_count": len(page_text.split()),
                "char_count": len(page_text)
            }
            
            pages.append(page_info)
        
        return full_text.strip(), pages
    
    def _process_text_blocks(self, blocks: List[Dict]) -> List[Dict]:
        """处理文本块"""
        processed_blocks = []
        
        for block in blocks:
            if block.get("type") == 0:  # 文本块
                block_info = {
                    "bbox": block.get("bbox", []),
                    "text": "",
                    "lines": []
                }
                
                for line in block.get("lines", []):
                    line_text = ""
                    line_info = {
                        "bbox": line.get("bbox", []),
                        "spans": []
                    }
                    
                    for span in line.get("spans", []):
                        span_text = span.get("text", "")
                        line_text += span_text
                        
                        span_info = {
                            "text": span_text,
                            "bbox": span.get("bbox", []),
                            "font": span.get("font", ""),
                            "size": span.get("size", 0),
                            "flags": span.get("flags", 0),
                            "color": span.get("color", 0)
                        }
                        
                        line_info["spans"].append(span_info)
                    
                    line_info["text"] = line_text
                    block_info["lines"].append(line_info)
                    block_info["text"] += line_text + "\n"
                
                block_info["text"] = block_info["text"].strip()
                processed_blocks.append(block_info)
        
        return processed_blocks
    
    def _extract_fonts(self, blocks: List[Dict]) -> Dict[str, int]:
        """提取字体信息"""
        fonts = {}
        
        for block in blocks:
            if block.get("type") == 0:
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        font = span.get("font", "")
                        size = span.get("size", 0)
                        
                        if font:
                            font_key = f"{font}_{size}"
                            fonts[font_key] = fonts.get(font_key, 0) + 1
        
        return fonts
    
    def _extract_images(self, doc: fitz.Document) -> List[Dict]:
        """提取图片信息"""
        images = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            image_list = page.get_images()
            
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    
                    if pix.n - pix.alpha < 4:  # 确保是RGB或灰度图
                        image_info = {
                            "page_number": page_num + 1,
                            "index": img_index,
                            "xref": xref,
                            "width": pix.width,
                            "height": pix.height,
                            "colorspace": pix.colorspace.name if pix.colorspace else "unknown",
                            "has_alpha": bool(pix.alpha),
                            "size_bytes": len(pix.tobytes()),
                            "bbox": img[1:5] if len(img) > 4 else None
                        }
                        
                        images.append(image_info)
                    
                    pix = None
                    
                except Exception as e:
                    logger.warning(f"提取图片失败 (页面 {page_num + 1}, 图片 {img_index}): {e}")
        
        return images
    
    def _analyze_structure(self, doc: fitz.Document) -> Dict[str, Any]:
        """分析文档结构"""
        structure = {
            "headings": [],
            "paragraphs": [],
            "lists": [],
            "outline": []
        }
        
        # 提取大纲
        try:
            outline = doc.get_toc()
            structure["outline"] = [
                {
                    "level": item[0],
                    "title": item[1],
                    "page": item[2]
                }
                for item in outline
            ]
        except Exception:
            pass
        
        # 分析标题和段落
        for page_num in range(doc.page_count):
            page = doc[page_num]
            text_dict = page.get_text("dict")
            
            page_headings, page_paragraphs = self._identify_headings_and_paragraphs(
                text_dict.get("blocks", []), page_num + 1
            )
            
            structure["headings"].extend(page_headings)
            structure["paragraphs"].extend(page_paragraphs)
        
        return structure
    
    def _identify_headings_and_paragraphs(self, blocks: List[Dict], page_num: int) -> tuple[List[Dict], List[Dict]]:
        """识别标题和段落"""
        headings = []
        paragraphs = []
        
        # 计算平均字体大小
        font_sizes = []
        for block in blocks:
            if block.get("type") == 0:
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        size = span.get("size", 0)
                        if size > 0:
                            font_sizes.append(size)
        
        avg_font_size = sum(font_sizes) / len(font_sizes) if font_sizes else 12
        
        for block in blocks:
            if block.get("type") == 0:
                block_text = ""
                max_font_size = 0
                is_bold = False
                
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        text = span.get("text", "")
                        size = span.get("size", 0)
                        flags = span.get("flags", 0)
                        
                        block_text += text
                        max_font_size = max(max_font_size, size)
                        
                        # 检查是否为粗体 (flags & 16)
                        if flags & 16:
                            is_bold = True
                
                block_text = block_text.strip()
                
                if block_text:
                    # 判断是否为标题
                    if (max_font_size > avg_font_size * 1.2 or 
                        is_bold or 
                        self._looks_like_heading(block_text)):
                        
                        heading = {
                            "text": block_text,
                            "page_number": page_num,
                            "font_size": max_font_size,
                            "is_bold": is_bold,
                            "level": self._determine_heading_level(max_font_size, avg_font_size),
                            "bbox": block.get("bbox", [])
                        }
                        headings.append(heading)
                    else:
                        paragraph = {
                            "text": block_text,
                            "page_number": page_num,
                            "word_count": len(block_text.split()),
                            "char_count": len(block_text),
                            "bbox": block.get("bbox", [])
                        }
                        paragraphs.append(paragraph)
        
        return headings, paragraphs
    
    def _looks_like_heading(self, text: str) -> bool:
        """判断文本是否像标题"""
        # 短文本更可能是标题
        if len(text) < 100 and len(text.split()) < 10:
            # 以数字开头的可能是章节标题
            if re.match(r'^\d+\.', text.strip()):
                return True
            
            # 全大写的短文本
            if text.isupper() and len(text) > 3:
                return True
            
            # 以常见标题词开头
            heading_words = ['第', '章', 'Chapter', 'Section', '摘要', 'Abstract', '引言', 'Introduction']
            for word in heading_words:
                if text.strip().startswith(word):
                    return True
        
        return False
    
    def _determine_heading_level(self, font_size: float, avg_font_size: float) -> int:
        """确定标题层级"""
        ratio = font_size / avg_font_size
        
        if ratio >= 1.8:
            return 1
        elif ratio >= 1.5:
            return 2
        elif ratio >= 1.3:
            return 3
        elif ratio >= 1.1:
            return 4
        else:
            return 5
    
    def _detect_tables(self, doc: fitz.Document) -> List[Dict]:
        """检测表格（简化版本）"""
        tables = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            
            # 获取页面上的所有矩形（可能是表格边框）
            drawings = page.get_drawings()
            
            # 简单的表格检测逻辑
            # 这里可以实现更复杂的表格检测算法
            if len(drawings) > 10:  # 如果有很多绘图元素，可能包含表格
                table_info = {
                    "page_number": page_num + 1,
                    "estimated_tables": 1,  # 简化处理
                    "drawing_count": len(drawings)
                }
                tables.append(table_info)
        
        return tables
    
    def _has_images(self, doc: fitz.Document) -> bool:
        """检查是否包含图片"""
        for page_num in range(min(3, doc.page_count)):  # 只检查前3页
            page = doc[page_num]
            if page.get_images():
                return True
        return False
    
    def _has_tables(self, doc: fitz.Document) -> bool:
        """检查是否包含表格"""
        for page_num in range(min(3, doc.page_count)):  # 只检查前3页
            page = doc[page_num]
            drawings = page.get_drawings()
            if len(drawings) > 5:  # 简单判断
                return True
        return False
    
    def _extract_keywords(self, keywords_str: str) -> List[str]:
        """提取关键词"""
        if not keywords_str:
            return []
        
        # 分割关键词（常见分隔符）
        keywords = re.split(r'[,;，；\s]+', keywords_str)
        return [kw.strip() for kw in keywords if kw.strip()]
```

### 1.3 测试PDF解析器

**创建 `test_pdf_parser.py`：**
```python
"""测试PDF解析器"""

from pathlib import Path
from src.document.pdf_parser import PDFParser
import json

def test_pdf_parser():
    """测试PDF解析功能"""
    parser = PDFParser()
    
    # 测试文件路径（请替换为实际的PDF文件）
    test_files = [
        "test_documents/sample1.pdf",
        "test_documents/sample2.pdf",
        "test_documents/sample3.pdf"
    ]
    
    for file_path in test_files:
        pdf_path = Path(file_path)
        
        if not pdf_path.exists():
            print(f"⚠️ 测试文件不存在: {file_path}")
            continue
        
        print(f"\n📄 解析文件: {pdf_path.name}")
        print("=" * 50)
        
        try:
            # 解析文档
            content = parser.parse(pdf_path)
            
            # 显示元数据
            print("📋 文档元数据:")
            print(f"  标题: {content.metadata.title}")
            print(f"  作者: {content.metadata.author}")
            print(f"  页数: {content.metadata.page_count}")
            print(f"  文件大小: {content.metadata.file_size:,} 字节")
            print(f"  语言: {content.metadata.language}")
            print(f"  包含图片: {content.metadata.has_images}")
            print(f"  包含表格: {content.metadata.has_tables}")
            
            # 显示文本统计
            print(f"\n📝 文本统计:")
            print(f"  总字符数: {len(content.text):,}")
            print(f"  总单词数: {len(content.text.split()):,}")
            print(f"  总页数: {len(content.pages)}")
            
            # 显示结构信息
            if content.structure["headings"]:
                print(f"\n📑 标题结构:")
                for heading in content.structure["headings"][:10]:  # 只显示前10个
                    print(f"  H{heading['level']}: {heading['text'][:50]}...")
            
            # 显示大纲
            if content.structure["outline"]:
                print(f"\n📖 文档大纲:")
                for item in content.structure["outline"][:10]:  # 只显示前10个
                    indent = "  " * (item["level"] - 1)
                    print(f"{indent}- {item['title']} (页 {item['page']})")
            
            # 显示图片信息
            if content.images:
                print(f"\n🖼️ 图片信息:")
                print(f"  图片总数: {len(content.images)}")
                for img in content.images[:5]:  # 只显示前5个
                    print(f"  页 {img['page_number']}: {img['width']}x{img['height']} ({img['colorspace']})")
            
            # 显示前200字符的文本内容
            print(f"\n📄 文本预览:")
            preview_text = content.text[:200].replace('\n', ' ').strip()
            print(f"  {preview_text}...")
            
            print("\n✅ 解析成功!")
            
        except Exception as e:
            print(f"❌ 解析失败: {e}")

def test_specific_features():
    """测试特定功能"""
    parser = PDFParser()
    
    # 测试文件验证
    print("\n🔍 测试文件验证:")
    print(f"  存在的文件: {parser.validate_file(Path('test_documents/sample1.pdf'))}")
    print(f"  不存在的文件: {parser.validate_file(Path('nonexistent.pdf'))}")
    
    # 测试支持的格式
    print(f"\n📋 支持的格式: {parser.supported_extensions}")
    print(f"  PDF文件支持: {parser.can_parse(Path('test.pdf'))}")
    print(f"  Word文件支持: {parser.can_parse(Path('test.docx'))}")

if __name__ == "__main__":
    print("开始PDF解析器测试...")
    
    try:
        test_specific_features()
        test_pdf_parser()
        print("\n🎉 所有测试完成!")
    except Exception as e:
        print(f"\n💥 测试过程中出现错误: {e}")
```

**运行测试：**
```bash
python test_pdf_parser.py
```

---

## 🔬 实验二：多格式文档解析器

### 2.1 实现Word文档解析器

**创建 `src/document/docx_parser.py`：**
```python
"""Word文档解析器"""

from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import os

from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT

from .parser import DocumentParser, DocumentContent, DocumentMetadata

class DocxParser(DocumentParser):
    """Word文档解析器"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = {".docx", ".doc"}
    
    def parse(self, file_path: Path) -> DocumentContent:
        """解析Word文档"""
        if not self.validate_file(file_path):
            raise ValueError(f"无效的Word文件: {file_path}")
        
        try:
            doc = Document(str(file_path))
            
            # 提取元数据
            metadata = self._extract_metadata(doc, file_path)
            
            # 提取内容
            text, structure = self._extract_content_and_structure(doc)
            
            # 提取图片信息
            images = self._extract_images(doc)
            
            # 提取表格
            tables = self._extract_tables(doc)
            
            return DocumentContent(
                text=text,
                metadata=metadata,
                images=images,
                tables=tables,
                structure=structure
            )
            
        except Exception as e:
            logger.error(f"Word文档解析失败: {e}")
            raise
    
    def _extract_metadata(self, doc: Document, file_path: Path) -> DocumentMetadata:
        """提取Word文档元数据"""
        core_props = doc.core_properties
        
        return DocumentMetadata(
            title=core_props.title or "",
            author=core_props.author or "",
            subject=core_props.subject or "",
            creator=core_props.author or "",
            creation_date=core_props.created,
            modification_date=core_props.modified,
            page_count=self._estimate_page_count(doc),
            file_size=os.path.getsize(file_path),
            file_name=file_path.name,
            file_extension=file_path.suffix,
            language=core_props.language or "unknown",
            has_images=self._has_images(doc),
            has_tables=self._has_tables(doc),
            keywords=self._extract_keywords(core_props.keywords or "")
        )
    
    def _extract_content_and_structure(self, doc: Document) -> tuple[str, Dict[str, Any]]:
        """提取内容和结构"""
        full_text = ""
        paragraphs = []
        headings = []
        lists = []
        
        for i, paragraph in enumerate(doc.paragraphs):
            text = paragraph.text.strip()
            if not text:
                continue
            
            full_text += text + "\n\n"
            
            # 分析段落样式
            style_name = paragraph.style.name if paragraph.style else "Normal"
            
            para_info = {
                "index": i,
                "text": text,
                "style": style_name,
                "alignment": self._get_alignment(paragraph),
                "word_count": len(text.split()),
                "char_count": len(text)
            }
            
            # 判断是否为标题
            if self._is_heading(paragraph, style_name):
                heading = {
                    "text": text,
                    "level": self._get_heading_level(style_name),
                    "style": style_name,
                    "index": i
                }
                headings.append(heading)
            
            # 判断是否为列表项
            elif self._is_list_item(text):
                list_item = {
                    "text": text,
                    "type": "bullet" if text.startswith("•") else "numbered",
                    "index": i
                }
                lists.append(list_item)
            
            paragraphs.append(para_info)
        
        structure = {
            "paragraphs": paragraphs,
            "headings": headings,
            "lists": lists,
            "outline": self._build_outline(headings)
        }
        
        return full_text.strip(), structure
    
    def _get_alignment(self, paragraph) -> str:
        """获取段落对齐方式"""
        alignment_map = {
            WD_PARAGRAPH_ALIGNMENT.LEFT: "left",
            WD_PARAGRAPH_ALIGNMENT.CENTER: "center",
            WD_PARAGRAPH_ALIGNMENT.RIGHT: "right",
            WD_PARAGRAPH_ALIGNMENT.JUSTIFY: "justify"
        }
        return alignment_map.get(paragraph.alignment, "left")
    
    def _is_heading(self, paragraph, style_name: str) -> bool:
        """判断是否为标题"""
        # 基于样式名判断
        heading_styles = ["Heading 1", "Heading 2", "Heading 3", "Heading 4", "Heading 5", "Heading 6"]
        if style_name in heading_styles:
            return True
        
        # 基于文本特征判断
        text = paragraph.text.strip()
        if len(text) < 100 and len(text.split()) < 10:
            # 检查是否有粗体格式
            for run in paragraph.runs:
                if run.bold:
                    return True
        
        return False
    
    def _get_heading_level(self, style_name: str) -> int:
        """获取标题层级"""
        if "Heading" in style_name:
            try:
                return int(style_name.split()[-1])
            except (ValueError, IndexError):
                pass
        return 1
    
    def _is_list_item(self, text: str) -> bool:
        """判断是否为列表项"""
        return (
            text.startswith("•") or
            text.startswith("-") or
            re.match(r'^\d+\.', text) or
            re.match(r'^[a-zA-Z]\)', text)
        )
    
    def _build_outline(self, headings: List[Dict]) -> List[Dict]:
        """构建文档大纲"""
        outline = []
        for heading in headings:
            outline.append({
                "level": heading["level"],
                "title": heading["text"],
                "index": heading["index"]
            })
        return outline
    
    def _extract_images(self, doc: Document) -> List[Dict]:
        """提取图片信息"""
        images = []
        
        # 遍历所有段落中的图片
        for i, paragraph in enumerate(doc.paragraphs):
            for run in paragraph.runs:
                if run.element.xpath('.//pic:pic'):
                    # 找到图片
                    image_info = {
                        "paragraph_index": i,
                        "type": "inline",
                        "description": "Word embedded image"
                    }
                    images.append(image_info)
        
        return images
    
    def _extract_tables(self, doc: Document) -> List[Dict]:
        """提取表格"""
        tables = []
        
        for i, table in enumerate(doc.tables):
            rows_data = []
            
            for row in table.rows:
                row_data = []
                for cell in row.cells:
                    row_data.append(cell.text.strip())
                rows_data.append(row_data)
            
            table_info = {
                "index": i,
                "rows": len(table.rows),
                "columns": len(table.columns) if table.rows else 0,
                "data": rows_data,
                "text": self._table_to_text(rows_data)
            }
            
            tables.append(table_info)
        
        return tables
    
    def _table_to_text(self, rows_data: List[List[str]]) -> str:
        """将表格转换为文本"""
        text_lines = []
        for row in rows_data:
            text_lines.append(" | ".join(row))
        return "\n".join(text_lines)
    
    def _estimate_page_count(self, doc: Document) -> int:
        """估算页数（基于字符数）"""
        total_chars = sum(len(p.text) for p in doc.paragraphs)
        # 假设每页约2000字符
        return max(1, total_chars // 2000)
    
    def _has_images(self, doc: Document) -> bool:
        """检查是否包含图片"""
        for paragraph in doc.paragraphs:
            for run in paragraph.runs:
                if run.element.xpath('.//pic:pic'):
                    return True
        return False
    
    def _has_tables(self, doc: Document) -> bool:
        """检查是否包含表格"""
        return len(doc.tables) > 0
    
    def _extract_keywords(self, keywords_str: str) -> List[str]:
        """提取关键词"""
        if not keywords_str:
            return []
        
        keywords = re.split(r'[,;，；\s]+', keywords_str)
        return [kw.strip() for kw in keywords if kw.strip()]
```

### 2.2 实现文本文档解析器

**创建 `src/document/txt_parser.py`：**
```python
"""文本文档解析器"""

from pathlib import Path
from typing import Dict, List, Any
import os
import chardet
import re

from .parser import DocumentParser, DocumentContent, DocumentMetadata

class TxtParser(DocumentParser):
    """文本文档解析器"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = {".txt", ".md", ".rst"}
    
    def parse(self, file_path: Path) -> DocumentContent:
        """解析文本文档"""
        if not self.validate_file(file_path):
            raise ValueError(f"无效的文本文件: {file_path}")
        
        try:
            # 检测编码
            encoding = self._detect_encoding(file_path)
            
            # 读取文件内容
            with open(file_path, 'r', encoding=encoding) as f:
                text = f.read()
            
            # 提取元数据
            metadata = self._extract_metadata(text, file_path, encoding)
            
            # 分析结构
            structure = self._analyze_structure(text, file_path.suffix)
            
            return DocumentContent(
                text=text,
                metadata=metadata,
                structure=structure
            )
            
        except Exception as e:
            logger.error(f"文本文档解析失败: {e}")
            raise
    
    def _detect_encoding(self, file_path: Path) -> str:
        """检测文件编码"""
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # 读取前10KB
            
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            
            # 常见编码的映射
            encoding_map = {
                'GB2312': 'gbk',
                'ISO-8859-1': 'utf-8'  # 通常是误判
            }
            
            return encoding_map.get(encoding, encoding or 'utf-8')
            
        except Exception:
            return 'utf-8'
    
    def _extract_metadata(self, text: str, file_path: Path, encoding: str) -> DocumentMetadata:
        """提取文本文档元数据"""
        lines = text.split('\n')
        
        # 尝试从文件开头提取标题
        title = ""
        if lines:
            first_line = lines[0].strip()
            if first_line and len(first_line) < 100:
                title = first_line
        
        # 检测语言
        language = self._detect_language(text)
        
        return DocumentMetadata(
            title=title,
            page_count=1,  # 文本文件视为1页
            file_size=os.path.getsize(file_path),
            file_name=file_path.name,
            file_extension=file_path.suffix,
            language=language,
            has_images=False,
            has_tables=self._has_tables(text)
        )
    
    def _detect_language(self, text: str) -> str:
        """检测文本语言"""
        try:
            from langdetect import detect
            return detect(text[:1000])  # 只检测前1000字符
        except Exception:
            # 简单的中英文检测
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text[:1000]))
            english_chars = len(re.findall(r'[a-zA-Z]', text[:1000]))
            
            if chinese_chars > english_chars:
                return "zh"
            elif english_chars > 0:
                return "en"
            else:
                return "unknown"
    
    def _analyze_structure(self, text: str, file_extension: str) -> Dict[str, Any]:
        """分析文本结构"""
        lines = text.split('\n')
        
        if file_extension == '.md':
            return self._analyze_markdown_structure(lines)
        elif file_extension == '.rst':
            return self._analyze_rst_structure(lines)
        else:
            return self._analyze_plain_text_structure(lines)
    
    def _analyze_markdown_structure(self, lines: List[str]) -> Dict[str, Any]:
        """分析Markdown结构"""
        headings = []
        paragraphs = []
        lists = []
        code_blocks = []
        
        in_code_block = False
        current_paragraph = []
        
        for i, line in enumerate(lines):
            line = line.rstrip()
            
            # 代码块
            if line.startswith('```'):
                in_code_block = not in_code_block
                if not in_code_block and current_paragraph:
                    code_blocks.append({
                        "start_line": i - len(current_paragraph),
                        "end_line": i,
                        "content": '\n'.join(current_paragraph)
                    })
                    current_paragraph = []
                continue
            
            if in_code_block:
                current_paragraph.append(line)
                continue
            
            # 标题
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                if title:
                    headings.append({
                        "text": title,
                        "level": level,
                        "line_number": i + 1
                    })
            
            # 列表项
            elif re.match(r'^\s*[-*+]\s', line) or re.match(r'^\s*\d+\.\s', line):
                lists.append({
                    "text": line.strip(),
                    "type": "numbered" if re.match(r'^\s*\d+\.', line) else "bullet",
                    "line_number": i + 1
                })
            
            # 段落
            elif line.strip():
                if current_paragraph:
                    current_paragraph.append(line)
                else:
                    current_paragraph = [line]
            
            # 空行，结束当前段落
            elif current_paragraph:
                paragraphs.append({
                    "text": '\n'.join(current_paragraph),
                    "start_line": i - len(current_paragraph) + 1,
                    "end_line": i,
                    "word_count": len(' '.join(current_paragraph).split())
                })
                current_paragraph = []
        
        # 处理最后一个段落
        if current_paragraph:
            paragraphs.append({
                "text": '\n'.join(current_paragraph),
                "start_line": len(lines) - len(current_paragraph),
                "end_line": len(lines),
                "word_count": len(' '.join(current_paragraph).split())
            })
        
        return {
            "headings": headings,
            "paragraphs": paragraphs,
            "lists": lists,
            "code_blocks": code_blocks,
            "outline": self._build_outline(headings)
        }
    
    def _analyze_rst_structure(self, lines: List[str]) -> Dict[str, Any]:
        """分析reStructuredText结构"""
        # 简化的RST解析
        headings = []
        paragraphs = []
        
        for i, line in enumerate(lines):
            # RST标题检测（简化版本）
            if i < len(lines) - 1:
                next_line = lines[i + 1]
                if (len(next_line.strip()) > 0 and 
                    len(set(next_line.strip())) == 1 and 
                    next_line.strip()[0] in '=-~^"'):
                    
                    headings.append({
                        "text": line.strip(),
                        "level": self._get_rst_heading_level(next_line.strip()[0]),
                        "line_number": i + 1
                    })
        
        return {
            "headings": headings,
            "paragraphs": paragraphs,
            "lists": [],
            "outline": self._build_outline(headings)
        }
    
    def _get_rst_heading_level(self, char: str) -> int:
        """获取RST标题层级"""
        level_map = {
            '=': 1,
            '-': 2,
            '~': 3,
            '^': 4,
            '"': 5
        }
        return level_map.get(char, 1)
    
    def _analyze_plain_text_structure(self, lines: List[str]) -> Dict[str, Any]:
        """分析纯文本结构"""
        paragraphs = []
        current_paragraph = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            if line:
                current_paragraph.append(line)
            elif current_paragraph:
                paragraphs.append({
                    "text": ' '.join(current_paragraph),
                    "start_line": i - len(current_paragraph) + 1,
                    "end_line": i,
                    "word_count": len(' '.join(current_paragraph).split())
                })
                current_paragraph = []
        
        # 处理最后一个段落
        if current_paragraph:
            paragraphs.append({
                "text": ' '.join(current_paragraph),
                "start_line": len(lines) - len(current_paragraph),
                "end_line": len(lines),
                "word_count": len(' '.join(current_paragraph).split())
            })
        
        return {
            "headings": [],
            "paragraphs": paragraphs,
            "lists": [],
            "outline": []
        }
    
    def _build_outline(self, headings: List[Dict]) -> List[Dict]:
        """构建文档大纲"""
        return [{
            "level": h["level"],
            "title": h["text"],
            "line": h["line_number"]
        } for h in headings]
    
    def _has_tables(self, text: str) -> bool:
        """检查是否包含表格（简单检测）"""
        lines = text.split('\n')
        
        # 检测Markdown表格
        for line in lines:
            if '|' in line and line.count('|') >= 2:
                return True
        
        return False
```

### 2.3 创建统一的文档解析管理器

**创建 `src/document/document_manager.py`：**
```python
"""文档解析管理器"""

from pathlib import Path
from typing import Dict, List, Optional, Type
import logging

from .parser import DocumentParser, DocumentContent
from .pdf_parser import PDFParser
from .docx_parser import DocxParser
from .txt_parser import TxtParser

logger = logging.getLogger(__name__)

class DocumentManager:
    """文档解析管理器"""
    
    def __init__(self):
        self.parsers: Dict[str, DocumentParser] = {}
        self._register_default_parsers()
    
    def _register_default_parsers(self):
        """注册默认解析器"""
        parsers = [
            PDFParser(),
            DocxParser(),
            TxtParser()
        ]
        
        for parser in parsers:
            self.register_parser(parser)
    
    def register_parser(self, parser: DocumentParser):
        """注册解析器"""
        for ext in parser.supported_extensions:
            self.parsers[ext.lower()] = parser
            logger.info(f"注册解析器: {ext} -> {parser.__class__.__name__}")
    
    def get_parser(self, file_path: Path) -> Optional[DocumentParser]:
        """获取适合的解析器"""
        extension = file_path.suffix.lower()
        return self.parsers.get(extension)
    
    def can_parse(self, file_path: Path) -> bool:
        """检查是否可以解析该文件"""
        return self.get_parser(file_path) is not None
    
    def parse_document(self, file_path: Path) -> DocumentContent:
        """解析文档"""
        file_path = Path(file_path)
        
        parser = self.get_parser(file_path)
        if not parser:
            raise ValueError(f"不支持的文件格式: {file_path.suffix}")
        
        logger.info(f"使用 {parser.__class__.__name__} 解析: {file_path.name}")
        return parser.parse(file_path)
    
    def batch_parse(self, file_paths: List[Path], 
                   skip_errors: bool = True) -> List[DocumentContent]:
        """批量解析文档"""
        results = []
        
        for file_path in file_paths:
            try:
                content = self.parse_document(file_path)
                results.append(content)
                logger.info(f"✅ 解析成功: {file_path.name}")
            except Exception as e:
                logger.error(f"❌ 解析失败: {file_path.name} - {e}")
                if not skip_errors:
                    raise
        
        return results
    
    def get_supported_extensions(self) -> List[str]:
        """获取支持的文件扩展名"""
        return list(self.parsers.keys())
    
    def get_parser_info(self) -> Dict[str, str]:
        """获取解析器信息"""
        info = {}
        for ext, parser in self.parsers.items():
            info[ext] = parser.__class__.__name__
        return info
```

### 2.4 测试多格式解析器

**创建 `test_document_manager.py`：**
```python
"""测试文档管理器"""

from pathlib import Path
from src.document.document_manager import DocumentManager

def test_document_manager():
    """测试文档管理器"""
    manager = DocumentManager()
    
    print("📋 文档管理器信息:")
    print(f"支持的格式: {manager.get_supported_extensions()}")
    print(f"解析器映射: {manager.get_parser_info()}")
    
    # 测试文件列表
    test_files = [
        "test_documents/sample.pdf",
        "test_documents/sample.docx",
        "test_documents/sample.txt",
        "test_documents/sample.md",
        "test_documents/README.md"
    ]
    
    print("\n🔍 检查文件支持情况:")
    for file_path in test_files:
        path = Path(file_path)
        supported = manager.can_parse(path)
        parser = manager.get_parser(path)
        parser_name = parser.__class__.__name__ if parser else "None"
        
        print(f"  {path.name}: {supported} ({parser_name})")
    
    # 批量解析测试
    print("\n📄 批量解析测试:")
    existing_files = [Path(f) for f in test_files if Path(f).exists()]
    
    if existing_files:
        try:
            results = manager.batch_parse(existing_files)
            
            print(f"\n✅ 成功解析 {len(results)} 个文件:")
            for i, content in enumerate(results):
                print(f"\n文件 {i+1}: {content.metadata.file_name}")
                print(f"  标题: {content.metadata.title}")
                print(f"  大小: {content.metadata.file_size:,} 字节")
                print(f"  语言: {content.metadata.language}")
                print(f"  字符数: {len(content.text):,}")
                print(f"  单词数: {len(content.text.split()):,}")
                
                if content.structure.get("headings"):
                    print(f"  标题数: {len(content.structure['headings'])}")
                
                if content.images:
                    print(f"  图片数: {len(content.images)}")
                
                if content.tables:
                    print(f"  表格数: {len(content.tables)}")
                
                # 显示文本预览
                preview = content.text[:100].replace('\n', ' ').strip()
                print(f"  预览: {preview}...")
        
        except Exception as e:
            print(f"❌ 批量解析失败: {e}")
    else:
        print("⚠️ 没有找到测试文件")

def test_specific_formats():
    """测试特定格式"""
    manager = DocumentManager()
    
    # 创建测试文本文件
    test_md_content = """
# 测试Markdown文档

## 第一章 介绍

这是一个测试文档，用于验证Markdown解析功能。

### 1.1 功能特性

- 支持标题解析
- 支持列表识别
- 支持代码块

```python
def hello_world():
    print("Hello, World!")
```

## 第二章 总结

解析功能正常工作。
"""
    
    # 写入测试文件
    test_file = Path("test_markdown.md")
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(test_md_content)
    
    try:
        print("\n📝 测试Markdown解析:")
        content = manager.parse_document(test_file)
        
        print(f"标题: {content.metadata.title}")
        print(f"语言: {content.metadata.language}")
        print(f"字符数: {len(content.text):,}")
        
        if content.structure.get("headings"):
            print(f"\n📑 标题结构:")
            for heading in content.structure["headings"]:
                indent = "  " * (heading["level"] - 1)
                print(f"{indent}H{heading['level']}: {heading['text']}")
        
        if content.structure.get("code_blocks"):
            print(f"\n💻 代码块: {len(content.structure['code_blocks'])} 个")
        
        if content.structure.get("lists"):
            print(f"\n📋 列表项: {len(content.structure['lists'])} 个")
    
    finally:
        # 清理测试文件
        if test_file.exists():
            test_file.unlink()

if __name__ == "__main__":
    print("开始文档管理器测试...")
    
    try:
        test_document_manager()
        test_specific_formats()
        print("\n🎉 所有测试完成!")
    except Exception as e:
        print(f"\n💥 测试过程中出现错误: {e}")
```

**运行测试：**
```bash
python test_document_manager.py
```

---

## 🔬 实验三：智能Chunk拆分策略

### 3.1 创建Chunk拆分器基类

**创建 `src/chunking/chunker.py`：**
```python
"""文档分块器模块"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ChunkMetadata:
    """分块元数据"""
    chunk_id: str
    source_file: str
    page_number: Optional[int] = None
    start_char: int = 0
    end_char: int = 0
    chunk_type: str = "text"  # text, heading, list, table, code
    heading_level: Optional[int] = None
    parent_heading: Optional[str] = None
    word_count: int = 0
    char_count: int = 0
    language: str = "unknown"
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

@dataclass
class DocumentChunk:
    """文档分块"""
    content: str
    metadata: ChunkMetadata
    
    def __post_init__(self):
        # 自动计算统计信息
        self.metadata.word_count = len(self.content.split())
        self.metadata.char_count = len(self.content)

class ChunkingStrategy(ABC):
    """分块策略基类"""
    
    def __init__(self, 
                 max_chunk_size: int = 1000,
                 overlap_size: int = 100,
                 min_chunk_size: int = 50):
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size
        self.min_chunk_size = min_chunk_size
    
    @abstractmethod
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """将文本分块"""
        pass
    
    def _create_chunk_id(self, source_file: str, index: int) -> str:
        """创建分块ID"""
        return f"{source_file}_{index:04d}"
    
    def _should_merge_chunks(self, chunk1: DocumentChunk, chunk2: DocumentChunk) -> bool:
        """判断是否应该合并分块"""
        total_size = chunk1.metadata.char_count + chunk2.metadata.char_count
        return total_size <= self.max_chunk_size
    
    def _merge_chunks(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """合并过小的分块"""
        if not chunks:
            return chunks
        
        merged = []
        current_chunk = chunks[0]
        
        for next_chunk in chunks[1:]:
            if (current_chunk.metadata.char_count < self.min_chunk_size and
                self._should_merge_chunks(current_chunk, next_chunk)):
                
                # 合并分块
                merged_content = current_chunk.content + "\n\n" + next_chunk.content
                merged_metadata = ChunkMetadata(
                    chunk_id=current_chunk.metadata.chunk_id,
                    source_file=current_chunk.metadata.source_file,
                    page_number=current_chunk.metadata.page_number,
                    start_char=current_chunk.metadata.start_char,
                    end_char=next_chunk.metadata.end_char,
                    chunk_type="merged",
                    parent_heading=current_chunk.metadata.parent_heading
                )
                
                current_chunk = DocumentChunk(merged_content, merged_metadata)
            else:
                merged.append(current_chunk)
                current_chunk = next_chunk
        
        merged.append(current_chunk)
        return merged
```

### 3.2 实现基于句子的分块策略

**创建 `src/chunking/sentence_chunker.py`：**
```python
"""基于句子的分块器"""

import re
from typing import List, Dict, Any
import nltk
from nltk.tokenize import sent_tokenize

from .chunker import ChunkingStrategy, DocumentChunk, ChunkMetadata

# 确保NLTK数据已下载
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class SentenceChunker(ChunkingStrategy):
    """基于句子的分块器"""
    
    def __init__(self, 
                 max_chunk_size: int = 1000,
                 overlap_size: int = 100,
                 min_chunk_size: int = 50,
                 language: str = 'english'):
        super().__init__(max_chunk_size, overlap_size, min_chunk_size)
        self.language = language
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """基于句子分块"""
        if not text.strip():
            return []
        
        metadata = metadata or {}
        source_file = metadata.get('source_file', 'unknown')
        
        # 分割句子
        sentences = self._split_sentences(text)
        
        # 创建分块
        chunks = []
        current_chunk_sentences = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            # 如果当前句子加入后超过最大大小，先保存当前分块
            if (current_size + sentence_size > self.max_chunk_size and 
                current_chunk_sentences):
                
                chunk_content = ' '.join(current_chunk_sentences)
                chunk = self._create_chunk(
                    chunk_content, 
                    source_file, 
                    len(chunks),
                    metadata
                )
                chunks.append(chunk)
                
                # 处理重叠
                if self.overlap_size > 0:
                    overlap_sentences = self._get_overlap_sentences(
                        current_chunk_sentences, self.overlap_size
                    )
                    current_chunk_sentences = overlap_sentences + [sentence]
                    current_size = sum(len(s) for s in current_chunk_sentences)
                else:
                    current_chunk_sentences = [sentence]
                    current_size = sentence_size
            else:
                current_chunk_sentences.append(sentence)
                current_size += sentence_size
        
        # 处理最后一个分块
        if current_chunk_sentences:
            chunk_content = ' '.join(current_chunk_sentences)
            chunk = self._create_chunk(
                chunk_content, 
                source_file, 
                len(chunks),
                metadata
            )
            chunks.append(chunk)
        
        # 合并过小的分块
        return self._merge_chunks(chunks)
    
    def _split_sentences(self, text: str) -> List[str]:
        """分割句子"""
        # 使用NLTK进行句子分割
        try:
            sentences = sent_tokenize(text, language=self.language)
        except Exception:
            # 如果NLTK失败，使用简单的正则表达式
            sentences = self._simple_sentence_split(text)
        
        # 清理和过滤句子
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:  # 过滤过短的句子
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _simple_sentence_split(self, text: str) -> List[str]:
        """简单的句子分割"""
        # 基于标点符号分割
        sentence_endings = r'[.!?。！？]\s+'
        sentences = re.split(sentence_endings, text)
        
        # 重新添加标点符号
        result = []
        endings = re.findall(sentence_endings, text)
        
        for i, sentence in enumerate(sentences[:-1]):
            if i < len(endings):
                result.append(sentence + endings[i].strip())
            else:
                result.append(sentence)
        
        # 添加最后一个句子
        if sentences[-1].strip():
            result.append(sentences[-1])
        
        return result
    
    def _get_overlap_sentences(self, sentences: List[str], overlap_size: int) -> List[str]:
        """获取重叠的句子"""
        if not sentences:
            return []
        
        # 从后往前选择句子，直到达到重叠大小
        overlap_sentences = []
        current_size = 0
        
        for sentence in reversed(sentences):
            if current_size + len(sentence) <= overlap_size:
                overlap_sentences.insert(0, sentence)
                current_size += len(sentence)
            else:
                break
        
        return overlap_sentences
    
    def _create_chunk(self, content: str, source_file: str, index: int, 
                     metadata: Dict[str, Any]) -> DocumentChunk:
        """创建文档分块"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=metadata.get('page_number'),
            chunk_type="sentence",
            language=metadata.get('language', 'unknown'),
            parent_heading=metadata.get('parent_heading')
        )
        
        return DocumentChunk(content, chunk_metadata)
```

### 3.5 创建分块管理器

**创建 `src/chunking/chunk_manager.py`：**
```python
"""分块管理器"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import logging

from ..document.document_manager import DocumentManager
from .chunker import DocumentChunk
from .sentence_chunker import SentenceChunker
from .semantic_chunker import SemanticChunker
from .structure_chunker import StructureChunker

logger = logging.getLogger(__name__)

class ChunkManager:
    """分块管理器"""
    
    def __init__(self):
        self.document_manager = DocumentManager()
        self.chunkers = {
            'sentence': SentenceChunker(),
            'semantic': SemanticChunker(),
            'structure': StructureChunker()
        }
    
    def chunk_file(self, file_path: Path, strategy: str = 'sentence', 
                   **kwargs) -> List[DocumentChunk]:
        """对单个文件进行分块"""
        if strategy not in self.chunkers:
            raise ValueError(f"不支持的分块策略: {strategy}")
        
        # 解析文档
        document = self.document_manager.parse_document(file_path)
        if not document:
            return []
        
        # 选择分块器
        chunker = self.chunkers[strategy]
        
        # 更新分块器参数
        if kwargs:
            for key, value in kwargs.items():
                if hasattr(chunker, key):
                    setattr(chunker, key, value)
        
        # 执行分块
        if strategy == 'structure' and hasattr(chunker, 'chunk_document'):
            chunks = chunker.chunk_document(document)
        else:
            metadata = {
                'source_file': file_path.name,
                'language': document.metadata.language,
                'page_number': 1
            }
            chunks = chunker.chunk_text(document.text, metadata)
        
        logger.info(f"文件 {file_path.name} 分块完成，共 {len(chunks)} 个分块")
        return chunks
    
    def chunk_directory(self, directory: Path, strategy: str = 'sentence',
                       **kwargs) -> Dict[str, List[DocumentChunk]]:
        """对目录中的所有文件进行分块"""
        results = {}
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and self.document_manager.is_supported(file_path):
                try:
                    chunks = self.chunk_file(file_path, strategy, **kwargs)
                    if chunks:
                        results[str(file_path)] = chunks
                except Exception as e:
                    logger.error(f"分块文件 {file_path} 失败: {e}")
        
        return results
    
    def save_chunks(self, chunks: List[DocumentChunk], output_file: Path):
        """保存分块结果到文件"""
        chunk_data = []
        
        for chunk in chunks:
            chunk_dict = {
                'content': chunk.content,
                'metadata': {
                    'chunk_id': chunk.metadata.chunk_id,
                    'source_file': chunk.metadata.source_file,
                    'page_number': chunk.metadata.page_number,
                    'start_char': chunk.metadata.start_char,
                    'end_char': chunk.metadata.end_char,
                    'chunk_type': chunk.metadata.chunk_type,
                    'heading_level': chunk.metadata.heading_level,
                    'parent_heading': chunk.metadata.parent_heading,
                    'word_count': chunk.metadata.word_count,
                    'char_count': chunk.metadata.char_count,
                    'language': chunk.metadata.language,
                    'keywords': chunk.metadata.keywords
                }
            }
            chunk_data.append(chunk_dict)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"分块结果已保存到 {output_file}")
    
    def load_chunks(self, input_file: Path) -> List[DocumentChunk]:
        """从文件加载分块结果"""
        with open(input_file, 'r', encoding='utf-8') as f:
            chunk_data = json.load(f)
        
        chunks = []
        for item in chunk_data:
            from .chunker import ChunkMetadata
            
            metadata = ChunkMetadata(
                chunk_id=item['metadata']['chunk_id'],
                source_file=item['metadata']['source_file'],
                page_number=item['metadata'].get('page_number'),
                start_char=item['metadata'].get('start_char', 0),
                end_char=item['metadata'].get('end_char', 0),
                chunk_type=item['metadata'].get('chunk_type', 'text'),
                heading_level=item['metadata'].get('heading_level'),
                parent_heading=item['metadata'].get('parent_heading'),
                word_count=item['metadata'].get('word_count', 0),
                char_count=item['metadata'].get('char_count', 0),
                language=item['metadata'].get('language', 'unknown'),
                keywords=item['metadata'].get('keywords', [])
            )
            
            chunk = DocumentChunk(item['content'], metadata)
            chunks.append(chunk)
        
        logger.info(f"从 {input_file} 加载了 {len(chunks)} 个分块")
        return chunks
    
    def get_chunk_statistics(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """获取分块统计信息"""
        if not chunks:
            return {}
        
        total_chunks = len(chunks)
        total_chars = sum(chunk.metadata.char_count for chunk in chunks)
        total_words = sum(chunk.metadata.word_count for chunk in chunks)
        
        chunk_sizes = [chunk.metadata.char_count for chunk in chunks]
        avg_chunk_size = total_chars / total_chunks
        min_chunk_size = min(chunk_sizes)
        max_chunk_size = max(chunk_sizes)
        
        chunk_types = {}
        for chunk in chunks:
            chunk_type = chunk.metadata.chunk_type
            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        return {
            'total_chunks': total_chunks,
            'total_characters': total_chars,
            'total_words': total_words,
            'average_chunk_size': round(avg_chunk_size, 2),
            'min_chunk_size': min_chunk_size,
            'max_chunk_size': max_chunk_size,
            'chunk_types': chunk_types
        }
```

### 3.6 创建分块测试脚本

**创建 `test_chunking.py`：**
```python
"""分块功能测试"""

from pathlib import Path
from src.chunking.chunk_manager import ChunkManager

def test_chunking_strategies():
    """测试不同的分块策略"""
    print("🔬 测试分块策略...")
    
    # 创建测试文档
    test_content = """
# 人工智能简介

人工智能（Artificial Intelligence，AI）是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。

## 发展历史

人工智能的发展可以追溯到20世纪50年代。1950年，英国数学家艾伦·图灵发表了《计算机器与智能》一文，提出了著名的"图灵测试"。

### 早期发展

1956年夏季，在美国达特茅斯学院举行的一次会议上，"人工智能"这个术语被正式提出。这次会议被认为是人工智能学科的诞生标志。

### 现代发展

进入21世纪以来，随着计算能力的提升和大数据的出现，人工智能迎来了新的发展机遇。深度学习、机器学习等技术取得了突破性进展。

## 主要应用领域

人工智能在多个领域都有广泛应用：

1. **自然语言处理**：包括机器翻译、语音识别、文本分析等。
2. **计算机视觉**：图像识别、人脸识别、自动驾驶等。
3. **机器学习**：数据挖掘、预测分析、推荐系统等。
4. **机器人技术**：工业机器人、服务机器人、医疗机器人等。

## 未来展望

人工智能技术将继续快速发展，预计在未来几年内将在更多领域得到应用，为人类社会带来更大的便利和价值。
    """
    
    # 创建测试文件
    test_file = Path("test_ai_document.md")
    test_file.write_text(test_content, encoding='utf-8')
    
    try:
        chunk_manager = ChunkManager()
        
        # 测试不同策略
        strategies = ['sentence', 'semantic', 'structure']
        
        for strategy in strategies:
            print(f"\n📝 测试 {strategy} 分块策略:")
            
            # 设置不同的参数
            kwargs = {
                'max_chunk_size': 500,
                'overlap_size': 50,
                'min_chunk_size': 100
            }
            
            if strategy == 'semantic':
                kwargs['similarity_threshold'] = 0.3
            
            chunks = chunk_manager.chunk_file(test_file, strategy, **kwargs)
            
            # 显示结果
            print(f"  分块数量: {len(chunks)}")
            
            for i, chunk in enumerate(chunks[:3]):  # 只显示前3个
                print(f"\n  分块 {i+1}:")
                print(f"    类型: {chunk.metadata.chunk_type}")
                print(f"    字符数: {chunk.metadata.char_count}")
                print(f"    内容预览: {chunk.content[:100]}...")
                if chunk.metadata.parent_heading:
                    print(f"    父标题: {chunk.metadata.parent_heading}")
            
            # 显示统计信息
            stats = chunk_manager.get_chunk_statistics(chunks)
            print(f"\n  📊 统计信息:")
            print(f"    平均分块大小: {stats['average_chunk_size']} 字符")
            print(f"    最小分块: {stats['min_chunk_size']} 字符")
            print(f"    最大分块: {stats['max_chunk_size']} 字符")
            print(f"    分块类型分布: {stats['chunk_types']}")
            
            # 保存结果
            output_file = Path(f"chunks_{strategy}.json")
            chunk_manager.save_chunks(chunks, output_file)
            print(f"    结果已保存到: {output_file}")
    
    finally:
        # 清理测试文件
        if test_file.exists():
            test_file.unlink()
        
        # 清理输出文件
        for strategy in strategies:
            output_file = Path(f"chunks_{strategy}.json")
            if output_file.exists():
                output_file.unlink()

def test_batch_chunking():
    """测试批量分块"""
    print("\n🔄 测试批量分块...")
    
    # 创建测试目录和文件
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # 创建多个测试文件
    test_files = {
        "doc1.txt": "这是第一个测试文档。它包含一些简单的文本内容，用于测试分块功能。",
        "doc2.md": "# 标题\n\n这是一个Markdown文档。\n\n## 子标题\n\n包含一些结构化内容。",
        "doc3.txt": "另一个测试文档，内容稍微长一些。它用于验证批量处理功能是否正常工作。我们需要确保所有文档都能被正确处理。"
    }
    
    try:
        # 创建测试文件
        for filename, content in test_files.items():
            (test_dir / filename).write_text(content, encoding='utf-8')
        
        chunk_manager = ChunkManager()
        
        # 批量分块
        results = chunk_manager.chunk_directory(
            test_dir, 
            strategy='sentence',
            max_chunk_size=200,
            overlap_size=20
        )
        
        print(f"处理了 {len(results)} 个文件:")
        
        for file_path, chunks in results.items():
            filename = Path(file_path).name
            print(f"  📄 {filename}: {len(chunks)} 个分块")
            
            for i, chunk in enumerate(chunks):
                print(f"    分块 {i+1}: {chunk.metadata.char_count} 字符")
    
    finally:
        # 清理测试目录
        if test_dir.exists():
            for file in test_dir.iterdir():
                file.unlink()
            test_dir.rmdir()

if __name__ == "__main__":
    print("开始分块功能测试...")
    
    try:
        test_chunking_strategies()
        test_batch_chunking()
        print("\n🎉 所有测试完成!")
    except Exception as e:
        print(f"\n💥 测试过程中出现错误: {e}")
```

**运行测试：**
```bash
python test_chunking.py
```

---

## 🤔 思考题

1. **分块策略比较**：
   - 基于句子、语义和结构的分块策略各有什么优缺点？
   - 在什么场景下应该选择哪种分块策略？

2. **参数调优**：
   - `max_chunk_size`、`overlap_size` 和 `min_chunk_size` 这些参数如何影响分块效果？
   - 如何根据不同类型的文档调整这些参数？

3. **性能优化**：
   - 如何优化大文档的分块处理性能？
   - 语义分块中的TF-IDF计算是否可以用其他方法替代？

4. **质量评估**：
   - 如何评估分块质量的好坏？
   - 设计一个分块质量评估指标。

5. **扩展功能**：
   - 如何处理多语言文档的分块？
   - 如何保持表格和代码块的完整性？

---

## ✅ 实验检查清单

- [ ] **环境准备**
  - [ ] 安装所有必需的依赖包
  - [ ] 准备测试文档（PDF、Word、文本等）
  - [ ] 验证PyMuPDF和其他解析库正常工作

- [ ] **文档解析**
  - [ ] 实现PDF解析器，能够提取文本、元数据和结构信息
  - [ ] 实现Word文档解析器
  - [ ] 实现文本文件解析器
  - [ ] 创建统一的文档管理器

- [ ] **分块功能**
  - [ ] 实现基于句子的分块策略
  - [ ] 实现基于语义的分块策略
  - [ ] 实现基于结构的分块策略
  - [ ] 创建分块管理器

- [ ] **测试验证**
  - [ ] 测试不同格式文档的解析
  - [ ] 测试不同分块策略的效果
  - [ ] 验证分块结果的保存和加载
  - [ ] 检查分块统计信息的准确性

- [ ] **代码质量**
  - [ ] 代码符合PEP 8规范
  - [ ] 添加适当的错误处理
  - [ ] 包含详细的文档字符串
  - [ ] 通过类型检查

---

## 🚨 常见问题解决

### 1. PyMuPDF安装问题
```bash
# 如果uv安装失败，尝试：
conda install -c conda-forge pymupdf
# 或者
uv pip install --upgrade pip
uv add PyMuPDF
```

### 2. NLTK数据下载问题
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

### 3. 编码问题
```python
# 确保文件读取时指定正确编码
with open(file_path, 'r', encoding='utf-8') as f:
    content = f.read()
```

### 4. 内存使用过多
```python
# 处理大文件时使用流式处理
def process_large_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # 逐行处理
            yield line.strip()
```

### 5. 分块质量不佳
- 调整 `max_chunk_size` 参数
- 尝试不同的分块策略
- 检查文档预处理是否充分
- 考虑文档特定的分块规则

---

## 📚 参考资料

1. **PyMuPDF官方文档**：https://pymupdf.readthedocs.io/
2. **python-docx文档**：https://python-docx.readthedocs.io/
3. **NLTK文档**：https://www.nltk.org/
4. **scikit-learn TF-IDF**：https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction
5. **文档分块最佳实践**：相关学术论文和技术博客

---

## 📝 实验完成后的Git操作

### 为什么要进行Git提交？

完成PDF解析与文档分块实验后，进行Git提交非常重要：

1. **文档处理代码保存**：保存PDF解析器、文档结构识别和分块策略的实现代码
2. **算法版本管理**：记录不同分块算法的演进过程，便于后续优化和调试
3. **配置文件管理**：保存分块参数配置、文档类型映射等重要配置
4. **测试数据管理**：保存测试用的PDF文件和预期输出结果
5. **团队协作基础**：为团队成员提供完整的文档处理功能实现
6. **学习进度记录**：标记PDF解析和智能分块技术的学习里程碑

### Git操作步骤

#### 1. 检查当前修改状态
```bash
git status
```

预期会看到以下类型的文件变更：
- `src/document_processor/` - PDF解析器相关代码
- `src/chunking/` - 文档分块策略实现
- `src/models/document.py` - 文档数据模型
- `tests/test_pdf_parser.py` - PDF解析测试
- `tests/test_chunking.py` - 分块功能测试
- `config/chunking_config.yaml` - 分块配置文件
- `requirements.txt` - 新增的依赖包

#### 2. 添加所有修改的文件
```bash
git add .
```

#### 3. 提交更改
```bash
git commit -m "完成lesson04实验：PDF解析与智能分块功能实现

- 实现PDF文档解析器，支持文本、图片、表格提取
- 开发多种分块策略：固定长度、语义分块、结构化分块
- 添加文档结构识别功能：标题、段落、列表检测
- 实现分块质量评估和优化机制
- 完善分块元数据管理和索引功能
- 添加comprehensive测试用例和性能基准测试"
```

#### 4. 查看提交历史
```bash
git log --oneline -5
```

#### 5. （可选）推送到远程仓库
如果你使用远程Git仓库，可以推送你的更改：
```bash
git push origin lesson04
```

### 文档处理项目的特殊注意事项

#### 提交前验证清单
1. **解析功能验证**：确保PDF解析器能正确处理各种格式的文档
2. **分块质量检查**：验证分块结果的语义完整性和大小合理性
3. **性能测试**：确认大文件处理的内存使用和处理速度
4. **配置文件检查**：确保分块参数配置合理且文档完整
5. **测试覆盖率**：验证关键功能都有对应的测试用例

#### 敏感信息处理
- 确保测试用的PDF文件不包含敏感或版权保护内容
- 检查日志文件中是否包含文档内容片段
- 验证临时文件和缓存目录已被正确忽略

### 常见问题解决

#### 问题1：大文件无法提交
```bash
# 检查文件大小
find . -name "*.pdf" -size +50M

# 将大文件添加到.gitignore
echo "*.pdf" >> .gitignore
echo "test_data/large_files/" >> .gitignore
```

#### 问题2：想要修改提交信息
```bash
# 修改最近一次提交的信息
git commit --amend -m "新的提交信息"
```

#### 问题3：误添加了不需要的文件
```bash
# 从暂存区移除文件但保留本地修改
git reset HEAD 文件名

# 或者移除所有暂存的文件
git reset HEAD .
```

#### 问题4：分块算法性能问题
```bash
# 创建性能优化分支
git checkout -b optimize-chunking

# 提交优化后的代码
git commit -m "优化分块算法性能：减少内存使用，提升处理速度"
```

### 文档处理项目Git最佳实践

1. **分层提交**：将解析器、分块器、测试分别提交
2. **算法标记**：为重要的算法版本打标签
3. **配置管理**：将配置文件变更单独提交并详细说明
4. **测试数据**：使用Git LFS管理大型测试文档
5. **性能基准**：记录性能测试结果和优化历程

### 下一步学习指导

完成Git提交后，你可以：
1. 回顾本次实验的分块策略实现
2. 准备进入lesson05：Embedding与向量入库
3. 思考如何将文档分块与向量化技术结合
4. 预习向量数据库的基本概念和操作

## 🎯 实验完成标志

当你完成以下任务时，说明实验成功：

1. ✅ 能够解析多种格式的文档（PDF、Word、文本）
2. ✅ 实现了三种不同的分块策略
3. ✅ 分块结果包含完整的元数据信息
4. ✅ 能够保存和加载分块结果
5. ✅ 测试脚本运行正常，输出合理的统计信息
6. ✅ 代码结构清晰，错误处理完善
7. ✅ Git提交完成：已将所有代码变更提交到版本控制系统

**恭喜！你已经掌握了文档解析和智能分块的核心技术！** 🎉

### 3.3 实现基于语义的分块策略

**创建 `src/chunking/semantic_chunker.py`：**
```python
"""基于语义的分块器"""

import re
from typing import List, Dict, Any, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from .sentence_chunker import SentenceChunker
from .chunker import DocumentChunk, ChunkMetadata

class SemanticChunker(SentenceChunker):
    """基于语义的分块器"""
    
    def __init__(self, 
                 max_chunk_size: int = 1000,
                 overlap_size: int = 100,
                 min_chunk_size: int = 50,
                 similarity_threshold: float = 0.5,
                 language: str = 'english'):
        super().__init__(max_chunk_size, overlap_size, min_chunk_size, language)
        self.similarity_threshold = similarity_threshold
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english' if language == 'english' else None,
            ngram_range=(1, 2)
        )
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """基于语义相似性分块"""
        if not text.strip():
            return []
        
        metadata = metadata or {}
        source_file = metadata.get('source_file', 'unknown')
        
        # 分割句子
        sentences = self._split_sentences(text)
        
        if len(sentences) < 2:
            # 如果句子太少，直接返回
            return [self._create_chunk(' '.join(sentences), source_file, 0, metadata)]
        
        # 计算句子相似性
        similarity_matrix = self._calculate_similarity_matrix(sentences)
        
        # 基于相似性分组
        sentence_groups = self._group_sentences_by_similarity(
            sentences, similarity_matrix
        )
        
        # 创建分块
        chunks = []
        for i, group in enumerate(sentence_groups):
            chunk_content = ' '.join(group)
            
            # 如果分块太大，进一步分割
            if len(chunk_content) > self.max_chunk_size:
                sub_chunks = self._split_large_chunk(group, source_file, len(chunks), metadata)
                chunks.extend(sub_chunks)
            else:
                chunk = self._create_chunk(chunk_content, source_file, len(chunks), metadata)
                chunks.append(chunk)
        
        # 合并过小的分块
        return self._merge_chunks(chunks)
    
    def _calculate_similarity_matrix(self, sentences: List[str]) -> np.ndarray:
        """计算句子相似性矩阵"""
        try:
            # 使用TF-IDF向量化
            tfidf_matrix = self.vectorizer.fit_transform(sentences)
            
            # 计算余弦相似性
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            return similarity_matrix
        
        except Exception as e:
            logger.warning(f"计算相似性矩阵失败: {e}，使用默认分组")
            # 如果失败，返回单位矩阵（每个句子自成一组）
            return np.eye(len(sentences))
    
    def _group_sentences_by_similarity(self, sentences: List[str], 
                                     similarity_matrix: np.ndarray) -> List[List[str]]:
        """基于相似性分组句子"""
        n_sentences = len(sentences)
        visited = [False] * n_sentences
        groups = []
        
        for i in range(n_sentences):
            if visited[i]:
                continue
            
            # 开始新的组
            current_group = [sentences[i]]
            visited[i] = True
            current_size = len(sentences[i])
            
            # 寻找相似的句子
            for j in range(i + 1, n_sentences):
                if visited[j]:
                    continue
                
                # 检查相似性和大小限制
                if (similarity_matrix[i][j] >= self.similarity_threshold and
                    current_size + len(sentences[j]) <= self.max_chunk_size):
                    
                    current_group.append(sentences[j])
                    visited[j] = True
                    current_size += len(sentences[j])
            
            groups.append(current_group)
        
        return groups
    
    def _split_large_chunk(self, sentences: List[str], source_file: str, 
                          start_index: int, metadata: Dict[str, Any]) -> List[DocumentChunk]:
        """分割过大的分块"""
        chunks = []
        current_sentences = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > self.max_chunk_size and current_sentences:
                # 创建当前分块
                chunk_content = ' '.join(current_sentences)
                chunk = self._create_chunk(
                    chunk_content, 
                    source_file, 
                    start_index + len(chunks),
                    metadata
                )
                chunks.append(chunk)
                
                # 处理重叠
                if self.overlap_size > 0:
                    overlap_sentences = self._get_overlap_sentences(
                        current_sentences, self.overlap_size
                    )
                    current_sentences = overlap_sentences + [sentence]
                    current_size = sum(len(s) for s in current_sentences)
                else:
                    current_sentences = [sentence]
                    current_size = sentence_size
            else:
                current_sentences.append(sentence)
                current_size += sentence_size
        
        # 处理最后一个分块
        if current_sentences:
            chunk_content = ' '.join(current_sentences)
            chunk = self._create_chunk(
                chunk_content, 
                source_file, 
                start_index + len(chunks),
                metadata
            )
            chunks.append(chunk)
        
        return chunks
    
    def _create_chunk(self, content: str, source_file: str, index: int, 
                     metadata: Dict[str, Any]) -> DocumentChunk:
        """创建文档分块"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=metadata.get('page_number'),
            chunk_type="semantic",
            language=metadata.get('language', 'unknown'),
            parent_heading=metadata.get('parent_heading')
        )
        
        return DocumentChunk(content, chunk_metadata)
```

### 3.4 实现基于结构的分块策略

**创建 `src/chunking/structure_chunker.py`：**
```python
"""基于文档结构的分块器"""

from typing import List, Dict, Any
from ..document.parser import DocumentContent
from .chunker import ChunkingStrategy, DocumentChunk, ChunkMetadata

class StructureChunker(ChunkingStrategy):
    """基于文档结构的分块器"""
    
    def __init__(self, 
                 max_chunk_size: int = 1500,
                 overlap_size: int = 150,
                 min_chunk_size: int = 100,
                 preserve_structure: bool = True):
        super().__init__(max_chunk_size, overlap_size, min_chunk_size)
        self.preserve_structure = preserve_structure
    
    def chunk_document(self, document: DocumentContent) -> List[DocumentChunk]:
        """基于文档结构分块"""
        chunks = []
        
        # 处理标题和段落
        if document.structure.get('headings') and document.structure.get('paragraphs'):
            chunks.extend(self._chunk_by_headings(document))
        else:
            # 如果没有结构信息，按段落分块
            chunks.extend(self._chunk_by_paragraphs(document))
        
        # 处理表格
        if document.tables:
            chunks.extend(self._chunk_tables(document))
        
        # 处理代码块（如果有）
        if document.structure.get('code_blocks'):
            chunks.extend(self._chunk_code_blocks(document))
        
        return chunks
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[DocumentChunk]:
        """简单文本分块（兼容基类接口）"""
        metadata = metadata or {}
        source_file = metadata.get('source_file', 'unknown')
        
        # 按段落分割
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            if len(current_chunk) + len(paragraph) <= self.max_chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunk = self._create_chunk(
                        current_chunk.strip(), 
                        source_file, 
                        len(chunks), 
                        metadata
                    )
                    chunks.append(chunk)
                
                current_chunk = paragraph + "\n\n"
        
        # 处理最后一个分块
        if current_chunk:
            chunk = self._create_chunk(
                current_chunk.strip(), 
                source_file, 
                len(chunks), 
                metadata
            )
            chunks.append(chunk)
        
        return self._merge_chunks(chunks)
    
    def _chunk_by_headings(self, document: DocumentContent) -> List[DocumentChunk]:
        """基于标题结构分块"""
        chunks = []
        headings = document.structure['headings']
        paragraphs = document.structure['paragraphs']
        
        # 为每个标题创建分块
        for i, heading in enumerate(headings):
            chunk_content = heading['text'] + "\n\n"
            current_size = len(chunk_content)
            
            # 查找属于这个标题的段落
            next_heading_page = headings[i + 1]['page_number'] if i + 1 < len(headings) else float('inf')
            
            for paragraph in paragraphs:
                # 判断段落是否属于当前标题
                if (paragraph['page_number'] >= heading['page_number'] and
                    paragraph['page_number'] < next_heading_page):
                    
                    para_text = paragraph['text']
                    
                    if current_size + len(para_text) <= self.max_chunk_size:
                        chunk_content += para_text + "\n\n"
                        current_size += len(para_text)
                    else:
                        # 当前分块已满，保存并开始新分块
                        if chunk_content.strip():
                            chunk = self._create_structure_chunk(
                                chunk_content.strip(),
                                document.metadata.file_name,
                                len(chunks),
                                heading,
                                "heading_section"
                            )
                            chunks.append(chunk)
                        
                        # 开始新分块
                        chunk_content = para_text + "\n\n"
                        current_size = len(para_text)
            
            # 保存最后一个分块
            if chunk_content.strip():
                chunk = self._create_structure_chunk(
                    chunk_content.strip(),
                    document.metadata.file_name,
                    len(chunks),
                    heading,
                    "heading_section"
                )
                chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_paragraphs(self, document: DocumentContent) -> List[DocumentChunk]:
        """基于段落分块"""
        chunks = []
        paragraphs = document.structure.get('paragraphs', [])
        
        current_chunk = ""
        current_size = 0
        
        for paragraph in paragraphs:
            para_text = paragraph['text']
            para_size = len(para_text)
            
            if current_size + para_size <= self.max_chunk_size:
                current_chunk += para_text + "\n\n"
                current_size += para_size
            else:
                # 保存当前分块
                if current_chunk.strip():
                    chunk = self._create_chunk(
                        current_chunk.strip(),
                        document.metadata.file_name,
                        len(chunks),
                        {'page_number': paragraph.get('page_number')}
                    )
                    chunks.append(chunk)
                
                # 开始新分块
                current_chunk = para_text + "\n\n"
                current_size = para_size
        
        # 处理最后一个分块
        if current_chunk.strip():
            chunk = self._create_chunk(
                current_chunk.strip(),
                document.metadata.file_name,
                len(chunks),
                {}
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_tables(self, document: DocumentContent) -> List[DocumentChunk]:
        """处理表格分块"""
        chunks = []
        
        for i, table in enumerate(document.tables):
            table_text = table.get('text', '')
            if not table_text:
                continue
            
            chunk_metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(document.metadata.file_name, f"table_{i}"),
                source_file=document.metadata.file_name,
                page_number=table.get('page_number'),
                chunk_type="table",
                language=document.metadata.language
            )
            
            chunk = DocumentChunk(table_text, chunk_metadata)
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_code_blocks(self, document: DocumentContent) -> List[DocumentChunk]:
        """处理代码块分块"""
        chunks = []
        code_blocks = document.structure.get('code_blocks', [])
        
        for i, code_block in enumerate(code_blocks):
            code_content = code_block.get('content', '')
            if not code_content:
                continue
            
            chunk_metadata = ChunkMetadata(
                chunk_id=self._create_chunk_id(document.metadata.file_name, f"code_{i}"),
                source_file=document.metadata.file_name,
                chunk_type="code",
                language="code"
            )
            
            chunk = DocumentChunk(code_content, chunk_metadata)
            chunks.append(chunk)
        
        return chunks
    
    def _create_structure_chunk(self, content: str, source_file: str, index: int,
                              heading: Dict[str, Any], chunk_type: str) -> DocumentChunk:
        """创建结构化分块"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=heading.get('page_number'),
            chunk_type=chunk_type,
            heading_level=heading.get('level'),
            parent_heading=heading.get('text')
        )
        
        return DocumentChunk(content, chunk_metadata)
    
    def _create_chunk(self, content: str, source_file: str, index: int, 
                     metadata: Dict[str, Any]) -> DocumentChunk:
        """创建普通分块"""
        chunk_metadata = ChunkMetadata(
            chunk_id=self._create_chunk_id(source_file, index),
            source_file=source_file,
            page_number=metadata.get('page_number'),
            chunk_type="paragraph"
        )
        
        return DocumentChunk(content, chunk_metadata)
```