# 第四课：PDF解析与Chunk拆分 - 教师讲义

## 📋 课程信息
- **课程名称**：PDF解析与Chunk拆分
- **课程时长**：90分钟
- **授课方式**：理论讲解 + 实践操作
- **先修要求**：完成前三课内容，熟悉Python基础和数据模型

---

## 🎯 教学目标

### 知识目标
- 理解文档解析的核心概念和技术原理
- 掌握PyMuPDF库的使用方法和最佳实践
- 了解不同文档格式的特点和处理策略
- 理解Chunk拆分的策略和算法

### 能力目标
- 能够使用PyMuPDF解析PDF文档
- 能够提取文档的文本、图片和元数据
- 能够实现智能的文档结构识别
- 能够设计和实现高效的Chunk拆分算法

### 素养目标
- 培养对文档处理技术的深入理解
- 提升解决复杂文档解析问题的能力
- 增强代码质量和性能优化意识

---

## 📚 教学内容

### 1. 文档解析技术概述（15分钟）

#### 1.1 文档解析的重要性
- **RAG系统的基础**：文档解析是RAG系统的第一步
- **数据质量影响**：解析质量直接影响检索和生成效果
- **多格式支持**：需要处理PDF、Word、PPT等多种格式
- **结构化提取**：不仅提取文本，还要保留文档结构

#### 1.2 常见文档格式特点
```python
# 不同格式的特点对比
format_characteristics = {
    "PDF": {
        "优点": ["格式固定", "跨平台兼容", "支持复杂布局"],
        "挑战": ["文本提取复杂", "表格识别困难", "图文混排"]
    },
    "DOCX": {
        "优点": ["结构化存储", "样式信息丰富", "易于解析"],
        "挑战": ["版本兼容性", "嵌入对象处理"]
    },
    "TXT": {
        "优点": ["简单直接", "处理速度快"],
        "挑战": ["缺少结构信息", "编码问题"]
    }
}
```

#### 1.3 PyMuPDF技术优势
- **高性能**：基于C++实现，处理速度快
- **功能全面**：支持文本、图片、表格提取
- **跨平台**：支持Windows、Linux、macOS
- **活跃维护**：社区活跃，更新频繁

### 2. PyMuPDF核心功能（20分钟）

#### 2.1 基础文档操作
```python
import fitz  # PyMuPDF

# 打开文档
doc = fitz.open("document.pdf")

# 获取文档信息
print(f"页数: {doc.page_count}")
print(f"元数据: {doc.metadata}")

# 遍历页面
for page_num in range(doc.page_count):
    page = doc[page_num]
    # 处理页面内容
    pass

# 关闭文档
doc.close()
```

#### 2.2 文本提取策略
```python
# 不同的文本提取方法
def extract_text_methods(page):
    # 方法1：简单文本提取
    simple_text = page.get_text()
    
    # 方法2：保留格式的文本提取
    formatted_text = page.get_text("text")
    
    # 方法3：获取文本块信息
    text_blocks = page.get_text("blocks")
    
    # 方法4：获取详细的文本信息
    text_dict = page.get_text("dict")
    
    return {
        "simple": simple_text,
        "formatted": formatted_text,
        "blocks": text_blocks,
        "detailed": text_dict
    }
```

#### 2.3 图片和表格处理
```python
# 图片提取
def extract_images(page):
    image_list = page.get_images()
    images = []
    
    for img_index, img in enumerate(image_list):
        xref = img[0]
        pix = fitz.Pixmap(doc, xref)
        
        if pix.n - pix.alpha < 4:  # 确保是RGB或灰度图
            img_data = pix.tobytes("png")
            images.append({
                "index": img_index,
                "data": img_data,
                "width": pix.width,
                "height": pix.height
            })
        pix = None
    
    return images

# 表格识别（基于文本块位置）
def detect_tables(page):
    blocks = page.get_text("dict")["blocks"]
    # 基于位置信息识别表格结构
    # 这里需要实现表格检测算法
    pass
```

### 3. 文档结构识别（20分钟）

#### 3.1 标题层级识别
```python
def identify_headings(text_dict):
    """识别文档中的标题层级"""
    headings = []
    
    for block in text_dict["blocks"]:
        if "lines" in block:
            for line in block["lines"]:
                for span in line["spans"]:
                    # 基于字体大小和样式判断标题
                    font_size = span["size"]
                    font_flags = span["flags"]
                    
                    if font_size > 14 or font_flags & 2**4:  # 大字体或粗体
                        headings.append({
                            "text": span["text"],
                            "level": determine_heading_level(font_size),
                            "bbox": span["bbox"]
                        })
    
    return headings

def determine_heading_level(font_size):
    """根据字体大小确定标题层级"""
    if font_size >= 18:
        return 1
    elif font_size >= 16:
        return 2
    elif font_size >= 14:
        return 3
    else:
        return 4
```

#### 3.2 段落和列表识别
```python
def identify_paragraphs(blocks):
    """识别段落结构"""
    paragraphs = []
    current_paragraph = []
    
    for block in blocks:
        if block["type"] == 0:  # 文本块
            text = block["text"].strip()
            
            if text:
                # 判断是否为新段落的开始
                if is_paragraph_start(text, block):
                    if current_paragraph:
                        paragraphs.append(" ".join(current_paragraph))
                        current_paragraph = []
                
                current_paragraph.append(text)
    
    if current_paragraph:
        paragraphs.append(" ".join(current_paragraph))
    
    return paragraphs

def is_paragraph_start(text, block):
    """判断是否为段落开始"""
    # 基于缩进、行间距等判断
    return (
        text[0].isupper() or  # 首字母大写
        block["bbox"][0] > 50 or  # 有缩进
        text.startswith(("•", "-", "1.", "a)"))  # 列表项
    )
```

### 4. Chunk拆分策略（25分钟）

#### 4.1 拆分策略概述
```python
class ChunkStrategy:
    """Chunk拆分策略基类"""
    
    def __init__(self, max_chunk_size=1000, overlap_size=200):
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size
    
    def split(self, text: str) -> List[str]:
        """拆分文本为chunks"""
        raise NotImplementedError
    
    def calculate_overlap(self, prev_chunk: str, current_pos: int) -> str:
        """计算重叠部分"""
        if len(prev_chunk) < self.overlap_size:
            return prev_chunk
        return prev_chunk[-self.overlap_size:]
```

#### 4.2 基于句子的拆分
```python
import re
from typing import List

class SentenceBasedChunker(ChunkStrategy):
    """基于句子的拆分器"""
    
    def __init__(self, max_chunk_size=1000, overlap_size=200):
        super().__init__(max_chunk_size, overlap_size)
        # 中英文句子结束符
        self.sentence_endings = re.compile(r'[.!?。！？]\s+')
    
    def split(self, text: str) -> List[str]:
        sentences = self.sentence_endings.split(text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # 检查添加句子后是否超过限制
            potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(potential_chunk) <= self.max_chunk_size:
                current_chunk = potential_chunk
            else:
                # 当前chunk已满，开始新chunk
                if current_chunk:
                    chunks.append(current_chunk)
                
                # 处理重叠
                overlap = self.calculate_overlap(current_chunk, len(chunks))
                current_chunk = overlap + " " + sentence if overlap else sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
```

#### 4.3 基于语义的拆分
```python
class SemanticChunker(ChunkStrategy):
    """基于语义的拆分器"""
    
    def __init__(self, max_chunk_size=1000, overlap_size=200, similarity_threshold=0.7):
        super().__init__(max_chunk_size, overlap_size)
        self.similarity_threshold = similarity_threshold
    
    def split(self, text: str) -> List[str]:
        sentences = self.split_into_sentences(text)
        chunks = []
        current_chunk_sentences = []
        
        for i, sentence in enumerate(sentences):
            # 计算与当前chunk的语义相似度
            if current_chunk_sentences:
                similarity = self.calculate_similarity(
                    " ".join(current_chunk_sentences), 
                    sentence
                )
                
                # 如果相似度低或chunk太大，开始新chunk
                if (similarity < self.similarity_threshold or 
                    len(" ".join(current_chunk_sentences + [sentence])) > self.max_chunk_size):
                    
                    chunks.append(" ".join(current_chunk_sentences))
                    
                    # 处理重叠
                    overlap_sentences = self.get_overlap_sentences(
                        current_chunk_sentences, self.overlap_size
                    )
                    current_chunk_sentences = overlap_sentences + [sentence]
                else:
                    current_chunk_sentences.append(sentence)
            else:
                current_chunk_sentences.append(sentence)
        
        if current_chunk_sentences:
            chunks.append(" ".join(current_chunk_sentences))
        
        return chunks
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """计算文本相似度（简化版本）"""
        # 这里可以使用更复杂的语义相似度计算
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0
```

#### 4.4 基于结构的拆分
```python
class StructureBasedChunker(ChunkStrategy):
    """基于文档结构的拆分器"""
    
    def __init__(self, max_chunk_size=1000, overlap_size=200):
        super().__init__(max_chunk_size, overlap_size)
    
    def split_by_structure(self, document_structure: dict) -> List[dict]:
        """基于文档结构拆分"""
        chunks = []
        
        for section in document_structure["sections"]:
            section_chunks = self.split_section(section)
            chunks.extend(section_chunks)
        
        return chunks
    
    def split_section(self, section: dict) -> List[dict]:
        """拆分单个章节"""
        chunks = []
        current_chunk = {
            "title": section["title"],
            "content": "",
            "metadata": {
                "section_id": section["id"],
                "level": section["level"]
            }
        }
        
        for paragraph in section["paragraphs"]:
            # 检查添加段落后是否超过限制
            potential_content = current_chunk["content"] + "\n" + paragraph
            
            if len(potential_content) <= self.max_chunk_size:
                current_chunk["content"] = potential_content.strip()
            else:
                # 保存当前chunk
                if current_chunk["content"]:
                    chunks.append(current_chunk.copy())
                
                # 开始新chunk
                current_chunk["content"] = paragraph
        
        if current_chunk["content"]:
            chunks.append(current_chunk)
        
        return chunks
```

### 5. 元数据提取与管理（10分钟）

#### 5.1 文档元数据提取
```python
def extract_document_metadata(doc, file_path: str) -> dict:
    """提取文档元数据"""
    metadata = doc.metadata
    
    return {
        "title": metadata.get("title", ""),
        "author": metadata.get("author", ""),
        "subject": metadata.get("subject", ""),
        "creator": metadata.get("creator", ""),
        "producer": metadata.get("producer", ""),
        "creation_date": metadata.get("creationDate", ""),
        "modification_date": metadata.get("modDate", ""),
        "page_count": doc.page_count,
        "file_size": os.path.getsize(file_path),
        "file_name": os.path.basename(file_path),
        "file_extension": os.path.splitext(file_path)[1],
        "language": detect_language(doc),
        "has_images": has_images(doc),
        "has_tables": has_tables(doc)
    }

def detect_language(doc) -> str:
    """检测文档语言"""
    # 提取前几页文本进行语言检测
    sample_text = ""
    for page_num in range(min(3, doc.page_count)):
        sample_text += doc[page_num].get_text()
    
    # 使用langdetect或其他语言检测库
    try:
        from langdetect import detect
        return detect(sample_text)
    except:
        return "unknown"
```

---

## 🎯 教学重点与难点

### 教学重点
1. **PyMuPDF基础操作**：文档打开、页面遍历、文本提取
2. **结构识别技术**：标题、段落、列表的自动识别
3. **Chunk拆分策略**：不同场景下的拆分方法选择
4. **性能优化**：大文档处理的内存和速度优化

### 教学难点
1. **复杂文档结构处理**：表格、图文混排的处理
2. **语义拆分算法**：如何保持语义连贯性
3. **多语言支持**：中英文混合文档的处理
4. **错误处理**：损坏文档、特殊格式的处理

### 突破策略
- 通过实际案例演示复杂文档的处理过程
- 提供多种拆分策略的对比分析
- 强调错误处理和边界情况的重要性
- 引导学生思考不同场景下的最优策略

---

## 🎭 课堂互动设计

### 1. 开场互动（5分钟）
**问题**："大家平时处理PDF文档时遇到过什么困难？"
- 让学生分享经验
- 引出文档解析的重要性
- 激发学习兴趣

### 2. 中期讨论（10分钟）
**场景分析**：给出不同类型的PDF文档
- 学术论文（双栏布局）
- 技术手册（多级标题）
- 财务报表（大量表格）
- 让学生讨论各自的处理策略

### 3. 实践演示（15分钟）
**现场编程**：
- 选择一个复杂PDF文档
- 现场演示解析过程
- 让学生参与调试和优化
- 展示不同参数的效果对比

### 4. 小组讨论（10分钟）
**主题**："如何设计一个通用的文档解析器？"
- 分组讨论设计思路
- 各组分享设计方案
- 教师点评和总结

---

## 📝 课后作业

### 基础作业
1. **文档解析器实现**：
   - 实现支持PDF、TXT、DOCX的通用解析器
   - 要求提取文本、标题、元数据
   - 处理至少3种不同类型的文档

2. **Chunk拆分算法**：
   - 实现3种不同的拆分策略
   - 对比不同策略的效果
   - 分析各自的适用场景

### 进阶作业
1. **性能优化**：
   - 优化大文档的处理速度
   - 实现内存友好的流式处理
   - 添加进度显示和错误恢复

2. **智能结构识别**：
   - 实现表格自动识别和提取
   - 支持图文混排的处理
   - 添加文档质量评估功能

### 挑战作业
1. **多语言支持**：
   - 支持中英文混合文档
   - 实现语言自动检测
   - 优化不同语言的拆分策略

2. **OCR集成**：
   - 集成OCR功能处理扫描文档
   - 实现图片中文字的提取
   - 处理低质量扫描文档

---

## 📊 评估标准

### 知识理解（30%）
- 文档解析原理的理解程度
- PyMuPDF API的掌握情况
- Chunk拆分策略的理论基础

### 实践能力（40%）
- 代码实现的正确性和完整性
- 错误处理的完善程度
- 性能优化的效果

### 创新思维（20%）
- 解决方案的创新性
- 对特殊情况的处理能力
- 算法优化的思路

### 代码质量（10%）
- 代码结构和可读性
- 注释和文档的完整性
- 测试用例的覆盖度

---

## 📚 参考资料

### 官方文档
- [PyMuPDF官方文档](https://pymupdf.readthedocs.io/)
- [PDF格式规范](https://www.adobe.com/content/dam/acom/en/devnet/pdf/pdfs/PDF32000_2008.pdf)
- [Python-docx文档](https://python-docx.readthedocs.io/)

### 技术博客
- [PDF文本提取最佳实践](https://example.com/pdf-extraction)
- [文档结构识别算法](https://example.com/document-structure)
- [Chunk拆分策略对比](https://example.com/chunking-strategies)

### 开源项目
- [pdfplumber](https://github.com/jsvine/pdfplumber)
- [camelot](https://github.com/camelot-dev/camelot)
- [tabula-py](https://github.com/chezou/tabula-py)

### 学术论文
- "Document Layout Analysis: A Comprehensive Survey"
- "Semantic Text Segmentation for Information Retrieval"
- "Table Detection in PDF Documents"

---

## 🔄 下节课预告

### 第五课：Embedding与向量入库
**主要内容**：
- sentence-transformers框架使用
- bge-m3模型的特点和优势
- Qdrant向量数据库操作
- 批量处理和性能优化
- 向量索引和检索策略

**预习建议**：
- 了解向量嵌入的基本概念
- 安装sentence-transformers库
- 阅读Qdrant官方文档
- 思考如何优化向量存储和检索

**准备工作**：
- 确保Qdrant服务正常运行
- 准备一些测试文档
- 检查GPU环境（如果有的话）

---

*本讲义为第四课教学的指导文档，请结合实际教学情况灵活调整内容和时间安排。*