# 第16节课：增量更新

## 课程信息
- **课程时长**：45分钟
- **课程目标**：实现文档增量更新，提高索引与检索效率
- **前置知识**：Python基础、文件操作、哈希算法、数据库基础

## 1. 课程概述（5分钟）

### 1.1 为什么需要增量更新？
- **全量更新的问题**：每次都重新处理所有文档，耗时耗资源
- **实际场景需求**：文档库中只有少部分文档发生变化
- **效率提升**：只处理变更的文档，大幅提升系统性能
- **用户体验**：更快的更新速度，更及时的内容同步

### 1.2 增量更新的核心概念
- **变更检测**：识别哪些文档发生了变化
- **版本控制**：跟踪文档的版本信息
- **增量索引**：只更新变化部分的索引
- **冲突解决**：处理并发更新时的冲突

## 2. 技术原理讲解（15分钟）

### 2.1 变更检测算法
```python
import hashlib
import os
from datetime import datetime
from typing import Dict, List, Optional

class ChangeDetector:
    """文档变更检测器"""
    
    def __init__(self, metadata_file: str = "doc_metadata.json"):
        self.metadata_file = metadata_file
        self.metadata = self.load_metadata()
    
    def calculate_file_hash(self, file_path: str) -> str:
        """计算文件的MD5哈希值"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except FileNotFoundError:
            return ""
    
    def get_file_info(self, file_path: str) -> Dict:
        """获取文件基本信息"""
        if not os.path.exists(file_path):
            return {}
        
        stat = os.stat(file_path)
        return {
            'path': file_path,
            'size': stat.st_size,
            'mtime': stat.st_mtime,
            'hash': self.calculate_file_hash(file_path)
        }
    
    def detect_changes(self, file_paths: List[str]) -> Dict[str, List[str]]:
        """检测文件变更"""
        changes = {
            'added': [],      # 新增文件
            'modified': [],   # 修改文件
            'deleted': [],    # 删除文件
            'unchanged': []   # 未变更文件
        }
        
        current_files = set(file_paths)
        previous_files = set(self.metadata.keys())
        
        # 检测新增文件
        for file_path in current_files - previous_files:
            changes['added'].append(file_path)
        
        # 检测删除文件
        for file_path in previous_files - current_files:
            changes['deleted'].append(file_path)
        
        # 检测修改文件
        for file_path in current_files & previous_files:
            current_info = self.get_file_info(file_path)
            previous_info = self.metadata.get(file_path, {})
            
            if current_info.get('hash') != previous_info.get('hash'):
                changes['modified'].append(file_path)
            else:
                changes['unchanged'].append(file_path)
        
        return changes
```

### 2.2 文档版本控制
```python
from dataclasses import dataclass
from typing import Optional
import json

@dataclass
class DocumentVersion:
    """文档版本信息"""
    doc_id: str
    version: int
    hash: str
    timestamp: str
    file_path: str
    size: int
    
class VersionManager:
    """文档版本管理器"""
    
    def __init__(self, version_file: str = "versions.json"):
        self.version_file = version_file
        self.versions = self.load_versions()
    
    def load_versions(self) -> Dict[str, DocumentVersion]:
        """加载版本信息"""
        try:
            with open(self.version_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return {k: DocumentVersion(**v) for k, v in data.items()}
        except FileNotFoundError:
            return {}
    
    def save_versions(self):
        """保存版本信息"""
        data = {k: v.__dict__ for k, v in self.versions.items()}
        with open(self.version_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def get_version(self, doc_id: str) -> Optional[DocumentVersion]:
        """获取文档版本"""
        return self.versions.get(doc_id)
    
    def update_version(self, doc_id: str, file_path: str, file_hash: str, file_size: int):
        """更新文档版本"""
        current_version = self.versions.get(doc_id)
        new_version = 1 if current_version is None else current_version.version + 1
        
        self.versions[doc_id] = DocumentVersion(
            doc_id=doc_id,
            version=new_version,
            hash=file_hash,
            timestamp=datetime.now().isoformat(),
            file_path=file_path,
            size=file_size
        )
        self.save_versions()
        return new_version
```

### 2.3 增量索引更新
```python
class IncrementalIndexer:
    """增量索引更新器"""
    
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.change_detector = ChangeDetector()
        self.version_manager = VersionManager()
    
    def update_index(self, file_paths: List[str]) -> Dict[str, int]:
        """增量更新索引"""
        # 检测变更
        changes = self.change_detector.detect_changes(file_paths)
        
        results = {
            'added': 0,
            'modified': 0,
            'deleted': 0,
            'unchanged': 0
        }
        
        # 处理新增文档
        for file_path in changes['added']:
            try:
                self.add_document(file_path)
                results['added'] += 1
            except Exception as e:
                print(f"添加文档失败 {file_path}: {e}")
        
        # 处理修改文档
        for file_path in changes['modified']:
            try:
                self.update_document(file_path)
                results['modified'] += 1
            except Exception as e:
                print(f"更新文档失败 {file_path}: {e}")
        
        # 处理删除文档
        for file_path in changes['deleted']:
            try:
                self.delete_document(file_path)
                results['deleted'] += 1
            except Exception as e:
                print(f"删除文档失败 {file_path}: {e}")
        
        results['unchanged'] = len(changes['unchanged'])
        
        # 更新元数据
        self.update_metadata(file_paths)
        
        return results
    
    def add_document(self, file_path: str):
        """添加新文档到索引"""
        doc_id = os.path.basename(file_path)
        
        # 读取文档内容
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 添加到RAG系统
        self.rag_system.add_document(doc_id, content)
        
        # 更新版本信息
        file_info = self.change_detector.get_file_info(file_path)
        self.version_manager.update_version(
            doc_id, file_path, file_info['hash'], file_info['size']
        )
    
    def update_document(self, file_path: str):
        """更新文档索引"""
        doc_id = os.path.basename(file_path)
        
        # 先删除旧版本
        self.rag_system.delete_document(doc_id)
        
        # 添加新版本
        self.add_document(file_path)
    
    def delete_document(self, file_path: str):
        """从索引中删除文档"""
        doc_id = os.path.basename(file_path)
        self.rag_system.delete_document(doc_id)
        
        # 删除版本信息
        if doc_id in self.version_manager.versions:
            del self.version_manager.versions[doc_id]
            self.version_manager.save_versions()
```

### 2.4 冲突解决策略
```python
class ConflictResolver:
    """冲突解决器"""
    
    def __init__(self, strategy: str = "timestamp"):
        self.strategy = strategy
    
    def resolve_conflict(self, local_version: DocumentVersion, 
                        remote_version: DocumentVersion) -> str:
        """解决版本冲突"""
        if self.strategy == "timestamp":
            # 基于时间戳的策略：选择最新的版本
            if local_version.timestamp > remote_version.timestamp:
                return "local"
            else:
                return "remote"
        
        elif self.strategy == "version":
            # 基于版本号的策略：选择版本号更高的
            if local_version.version > remote_version.version:
                return "local"
            else:
                return "remote"
        
        elif self.strategy == "size":
            # 基于文件大小的策略：选择更大的文件
            if local_version.size > remote_version.size:
                return "local"
            else:
                return "remote"
        
        else:
            # 默认选择本地版本
            return "local"
```

## 3. 实际应用场景（10分钟）

### 3.1 文档库同步场景
- **企业知识库**：定期同步更新的文档
- **新闻资讯系统**：实时更新的新闻内容
- **技术文档**：版本控制的API文档

### 3.2 性能优化考虑
- **批量处理**：将多个小更新合并为批量操作
- **异步处理**：使用异步IO提高处理效率
- **缓存策略**：缓存文件哈希值避免重复计算

### 3.3 监控与日志
```python
import logging

# 配置增量更新日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('incremental_update.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('IncrementalUpdate')

def log_update_results(results: Dict[str, int]):
    """记录更新结果"""
    logger.info(f"增量更新完成: 新增{results['added']}个, "
                f"修改{results['modified']}个, "
                f"删除{results['deleted']}个, "
                f"未变更{results['unchanged']}个")
```

## 4. 最佳实践与注意事项（10分钟）

### 4.1 设计原则
1. **原子性**：确保更新操作的原子性，避免部分更新
2. **一致性**：保持索引与源文档的一致性
3. **可恢复性**：支持更新失败后的回滚操作
4. **可扩展性**：支持不同类型的变更检测算法

### 4.2 常见陷阱
- **哈希冲突**：虽然概率很低，但需要考虑MD5冲突的可能性
- **文件锁定**：处理文件被其他进程占用的情况
- **大文件处理**：大文件的哈希计算可能很耗时
- **并发更新**：多个进程同时更新可能导致冲突

### 4.3 性能优化技巧
- **增量哈希**：对于大文件，可以只计算部分内容的哈希
- **并行处理**：使用多线程或多进程并行检测变更
- **智能缓存**：缓存最近计算的哈希值
- **批量操作**：将多个更新操作合并为批量执行

## 5. 课堂总结（5分钟）

### 5.1 核心要点回顾
- 增量更新通过只处理变更文档大幅提升效率
- 变更检测是增量更新的核心技术
- 版本控制确保文档更新的可追溯性
- 冲突解决策略处理并发更新问题

### 5.2 下节课预告
下节课将学习"结构化数据接入"，探讨如何将数据库、API等结构化数据源集成到RAG系统中。

### 5.3 课后思考
1. 在你的实际项目中，哪些场景适合使用增量更新？
2. 如何设计一个支持多种变更检测算法的可扩展系统？
3. 在分布式环境下，如何实现一致的增量更新？

## 6. 参考资料
- [文件系统监控最佳实践](https://docs.python.org/3/library/watchdog.html)
- [哈希算法选择指南](https://en.wikipedia.org/wiki/Cryptographic_hash_function)
- [版本控制系统设计](https://git-scm.com/book/en/v2/Git-Internals-Git-Objects)
- [数据库增量同步策略](https://dev.mysql.com/doc/refman/8.0/en/replication.html)