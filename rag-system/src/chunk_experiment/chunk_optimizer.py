#!/usr/bin/env python3
"""Chunkåˆ†å—å‚æ•°ä¼˜åŒ–å™¨"""

import time
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
import numpy as np
from concurrent.futures import ThreadPoolExecutor

@dataclass
class ExperimentResult:
    """å®éªŒç»“æœæ•°æ®ç±»"""
    chunk_size: int
    overlap_ratio: float
    avg_chunk_length: float
    total_chunks: int
    retrieval_accuracy: float
    retrieval_recall: float
    response_time: float
    storage_overhead: float
    
class ChunkOptimizer:
    """Chunkå‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, rag_system, test_documents: List[str], evaluation_queries: List[Dict]):
        self.rag_system = rag_system
        self.test_documents = test_documents
        self.evaluation_queries = evaluation_queries
        self.results: List[ExperimentResult] = []
        
    def run_grid_search(self, 
                       chunk_sizes: List[int] = [300, 500, 800, 1000, 1200],
                       overlap_ratios: List[float] = [0.1, 0.15, 0.2, 0.25, 0.3]) -> List[ExperimentResult]:
        """è¿è¡Œç½‘æ ¼æœç´¢å®éªŒ"""
        print(f"ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢å®éªŒ...")
        print(f"ğŸ“Š å‚æ•°ç»„åˆæ•°é‡: {len(chunk_sizes)} Ã— {len(overlap_ratios)} = {len(chunk_sizes) * len(overlap_ratios)}")
        
        total_experiments = len(chunk_sizes) * len(overlap_ratios)
        current_experiment = 0
        
        for chunk_size in chunk_sizes:
            for overlap_ratio in overlap_ratios:
                current_experiment += 1
                print(f"\nğŸ§ª å®éªŒ {current_experiment}/{total_experiments}: chunk_size={chunk_size}, overlap_ratio={overlap_ratio}")
                
                result = self._run_single_experiment(chunk_size, overlap_ratio)
                self.results.append(result)
                
                # æ˜¾ç¤ºå®æ—¶ç»“æœ
                print(f"   âœ… å‡†ç¡®ç‡: {result.retrieval_accuracy:.3f}")
                print(f"   âœ… å¬å›ç‡: {result.retrieval_recall:.3f}")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {result.response_time:.2f}ms")
                
        return self.results
    
    def _run_single_experiment(self, chunk_size: int, overlap_ratio: float) -> ExperimentResult:
        """è¿è¡Œå•ä¸ªå‚æ•°ç»„åˆå®éªŒ"""
        # 1. é‡æ–°é…ç½®åˆ†å—å‚æ•°
        self._reconfigure_chunking(chunk_size, overlap_ratio)
        
        # 2. é‡æ–°å¤„ç†æµ‹è¯•æ–‡æ¡£
        chunk_stats = self._reprocess_documents()
        
        # 3. è¿è¡Œæ£€ç´¢è¯„ä¼°
        retrieval_metrics = self._evaluate_retrieval()
        
        # 4. è®¡ç®—å­˜å‚¨å¼€é”€
        storage_overhead = self._calculate_storage_overhead(overlap_ratio)
        
        return ExperimentResult(
            chunk_size=chunk_size,
            overlap_ratio=overlap_ratio,
            avg_chunk_length=chunk_stats['avg_length'],
            total_chunks=chunk_stats['total_count'],
            retrieval_accuracy=retrieval_metrics['accuracy'],
            retrieval_recall=retrieval_metrics['recall'],
            response_time=retrieval_metrics['avg_response_time'],
            storage_overhead=storage_overhead
        )
    
    def _reconfigure_chunking(self, chunk_size: int, overlap_ratio: float):
        """é‡æ–°é…ç½®åˆ†å—å‚æ•°"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„RAGç³»ç»Ÿæ¥å£è°ƒæ•´
        if hasattr(self.rag_system, 'chunk_manager'):
            self.rag_system.chunk_manager.chunk_size = chunk_size
            self.rag_system.chunk_manager.overlap_ratio = overlap_ratio
        elif hasattr(self.rag_system, 'set_chunk_params'):
            self.rag_system.set_chunk_params(chunk_size, overlap_ratio)
    
    def _reprocess_documents(self) -> Dict[str, float]:
        """é‡æ–°å¤„ç†æ–‡æ¡£å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        total_chunks = 0
        total_length = 0
        
        for doc_path in self.test_documents:
            # æ¨¡æ‹Ÿæ–‡æ¡£å¤„ç†
            if hasattr(self.rag_system, 'process_document'):
                chunks = self.rag_system.process_document(doc_path)
                total_chunks += len(chunks)
                total_length += sum(len(chunk.content) for chunk in chunks)
            else:
                # æ¨¡æ‹Ÿå¤„ç†ç»“æœ
                chunk_count = np.random.randint(10, 50)
                avg_length = np.random.randint(200, 1000)
                total_chunks += chunk_count
                total_length += chunk_count * avg_length
        
        return {
            'total_count': total_chunks,
            'avg_length': total_length / total_chunks if total_chunks > 0 else 0
        }
    
    def _evaluate_retrieval(self) -> Dict[str, float]:
        """è¯„ä¼°æ£€ç´¢æ€§èƒ½"""
        correct_retrievals = 0
        total_relevant = 0
        total_retrieved = 0
        total_time = 0
        
        for query_data in self.evaluation_queries:
            query = query_data['query']
            expected_chunks = query_data.get('expected_chunks', [])
            
            # æ‰§è¡Œæ£€ç´¢
            start_time = time.time()
            if hasattr(self.rag_system, 'search'):
                results = self.rag_system.search(query, top_k=5)
            else:
                # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
                results = [{'chunk_id': f'chunk_{i}'} for i in range(5)]
            end_time = time.time()
            
            total_time += (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è®¡ç®—å‡†ç¡®ç‡å’Œå¬å›ç‡
            retrieved_ids = [r.get('chunk_id') for r in results]
            
            # è®¡ç®—äº¤é›†
            relevant_retrieved = len(set(retrieved_ids) & set(expected_chunks))
            correct_retrievals += relevant_retrieved
            total_retrieved += len(retrieved_ids)
            total_relevant += len(expected_chunks)
        
        accuracy = correct_retrievals / total_retrieved if total_retrieved > 0 else 0
        recall = correct_retrievals / total_relevant if total_relevant > 0 else 0
        avg_response_time = total_time / len(self.evaluation_queries) if self.evaluation_queries else 0
        
        return {
            'accuracy': accuracy,
            'recall': recall,
            'avg_response_time': avg_response_time
        }
    
    def _calculate_storage_overhead(self, overlap_ratio: float) -> float:
        """è®¡ç®—å­˜å‚¨å¼€é”€"""
        # é‡å æ¯”ä¾‹ç›´æ¥å½±å“å­˜å‚¨å¼€é”€
        base_storage = 1.0
        overhead = overlap_ratio * 0.8  # ç®€åŒ–è®¡ç®—
        return base_storage + overhead
    
    def get_best_parameters(self) -> ExperimentResult:
        """è·å–æœ€ä½³å‚æ•°ç»„åˆ"""
        if not self.results:
            raise ValueError("æ²¡æœ‰å®éªŒç»“æœï¼Œè¯·å…ˆè¿è¡Œå®éªŒ")
        
        # ç»¼åˆè¯„åˆ†ï¼šå‡†ç¡®ç‡æƒé‡0.4ï¼Œå¬å›ç‡æƒé‡0.3ï¼Œå“åº”æ—¶é—´æƒé‡0.2ï¼Œå­˜å‚¨å¼€é”€æƒé‡0.1
        best_result = None
        best_score = -1
        
        for result in self.results:
            # å½’ä¸€åŒ–æŒ‡æ ‡ï¼ˆå“åº”æ—¶é—´å’Œå­˜å‚¨å¼€é”€éœ€è¦å–å€’æ•°ï¼‰
            normalized_accuracy = result.retrieval_accuracy
            normalized_recall = result.retrieval_recall
            normalized_time = 1 / (1 + result.response_time / 100)  # å½’ä¸€åŒ–å“åº”æ—¶é—´
            normalized_storage = 1 / result.storage_overhead  # å½’ä¸€åŒ–å­˜å‚¨å¼€é”€
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            score = (0.4 * normalized_accuracy + 
                    0.3 * normalized_recall + 
                    0.2 * normalized_time + 
                    0.1 * normalized_storage)
            
            if score > best_score:
                best_score = score
                best_result = result
        
        return best_result
    
    def save_results(self, filepath: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_data = []
        for result in self.results:
            results_data.append({
                'chunk_size': result.chunk_size,
                'overlap_ratio': result.overlap_ratio,
                'avg_chunk_length': result.avg_chunk_length,
                'total_chunks': result.total_chunks,
                'retrieval_accuracy': result.retrieval_accuracy,
                'retrieval_recall': result.retrieval_recall,
                'response_time': result.response_time,
                'storage_overhead': result.storage_overhead
            })
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    def load_results(self, filepath: str):
        """åŠ è½½å®éªŒç»“æœ"""
        with open(filepath, 'r', encoding='utf-8') as f:
            results_data = json.load(f)
        
        self.results = []
        for data in results_data:
            result = ExperimentResult(**data)
            self.results.append(result)
        
        print(f"ğŸ“ å·²åŠ è½½ {len(self.results)} ä¸ªå®éªŒç»“æœ")
    
    def run_parallel_experiments(self, 
                               chunk_sizes: List[int] = [300, 500, 800, 1000, 1200],
                               overlap_ratios: List[float] = [0.1, 0.15, 0.2, 0.25, 0.3],
                               max_workers: int = 4) -> List[ExperimentResult]:
        """å¹¶è¡Œè¿è¡Œå®éªŒä»¥æé«˜æ•ˆç‡"""
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œç½‘æ ¼æœç´¢å®éªŒ (æœ€å¤§å·¥ä½œçº¿ç¨‹: {max_workers})...")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_combinations = [(size, ratio) for size in chunk_sizes for ratio in overlap_ratios]
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [executor.submit(self._run_single_experiment, size, ratio) 
                      for size, ratio in param_combinations]
            
            # æ”¶é›†ç»“æœ
            for i, future in enumerate(futures):
                result = future.result()
                self.results.append(result)
                print(f"âœ… å®Œæˆå®éªŒ {i+1}/{len(param_combinations)}: "
                      f"size={result.chunk_size}, ratio={result.overlap_ratio:.2f}, "
                      f"accuracy={result.retrieval_accuracy:.3f}")
        
        return self.results