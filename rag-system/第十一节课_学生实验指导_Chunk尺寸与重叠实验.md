# ç¬¬åä¸€èŠ‚è¯¾ï¼šå­¦ç”Ÿå®éªŒæŒ‡å¯¼ - Chunkå°ºå¯¸ä¸é‡å å®éªŒ

## å®éªŒç›®æ ‡

é€šè¿‡æœ¬å®éªŒï¼Œå­¦ç”Ÿå°†ï¼š
1. æŒæ¡Chunkåˆ†å—å‚æ•°å¯¹RAGç³»ç»Ÿæ€§èƒ½çš„å½±å“
2. å­¦ä¼šè®¾è®¡å’Œæ‰§è¡Œåˆ†å—å‚æ•°ä¼˜åŒ–å®éªŒ
3. ç†è§£å¦‚ä½•è¯„ä¼°åˆ†å—è´¨é‡å’Œæ£€ç´¢æ•ˆæœ
4. å®ç°è‡ªåŠ¨åŒ–çš„å‚æ•°è°ƒä¼˜å·¥å…·

## å®éªŒç¯å¢ƒå‡†å¤‡

### ä¾èµ–å®‰è£…
```bash
pip install streamlit plotly pandas numpy scikit-learn
```

### é¡¹ç›®ç»“æ„
```
src/
â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ chunk_optimizer.py      # å‚æ•°ä¼˜åŒ–å™¨
â”‚   â”œâ”€â”€ experiment_runner.py    # å®éªŒæ‰§è¡Œå™¨
â”‚   â””â”€â”€ quality_evaluator.py    # è´¨é‡è¯„ä¼°å™¨
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ chunk_optimization/
â”‚   â”‚   â”œâ”€â”€ interactive_tuner.py    # äº¤äº’å¼è°ƒä¼˜å·¥å…·
â”‚   â”‚   â”œâ”€â”€ visualizer.py          # ç»“æœå¯è§†åŒ–
â”‚   â”‚   â””â”€â”€ test_data/             # æµ‹è¯•æ•°æ®
â”‚   â””â”€â”€ results/                   # å®éªŒç»“æœ
```

## æ ¸å¿ƒå®éªŒç»„ä»¶

### 1. ChunkOptimizer ç±»å®ç°

åˆ›å»º `src/chunking/chunk_optimizer.py`ï¼š

```python
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from sklearn.metrics.pairwise import cosine_similarity
from ..chunking import ChunkManager, ChunkingConfig
from ..embedding import EmbeddingService

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœæ•°æ®ç±»"""
    best_params: Dict[str, any]
    best_score: float
    all_results: List[Dict]
    optimization_history: List[Dict]

class ChunkOptimizer:
    """Chunkå‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, embedding_service: EmbeddingService):
        self.embedding_service = embedding_service
        self.chunk_manager = ChunkManager()
        self.evaluation_cache = {}
    
    def grid_search(self, 
                   documents: List[str],
                   test_queries: List[str],
                   param_grid: Dict[str, List],
                   scoring_weights: Optional[Dict[str, float]] = None) -> OptimizationResult:
        """
        ç½‘æ ¼æœç´¢æœ€ä¼˜åˆ†å—å‚æ•°
        
        Args:
            documents: æµ‹è¯•æ–‡æ¡£åˆ—è¡¨
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            param_grid: å‚æ•°ç½‘æ ¼ï¼Œå¦‚ {'chunk_size': [300, 500, 800], 'overlap_ratio': [0.1, 0.2, 0.3]}
            scoring_weights: è¯„åˆ†æƒé‡ï¼Œå¦‚ {'precision': 0.4, 'recall': 0.3, 'coherence': 0.3}
        
        Returns:
            OptimizationResult: ä¼˜åŒ–ç»“æœ
        """
        if scoring_weights is None:
            scoring_weights = {
                'precision': 0.3,
                'recall': 0.3, 
                'coherence': 0.2,
                'efficiency': 0.2
            }
        
        results = []
        best_score = -1
        best_params = None
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_combinations = self._generate_param_combinations(param_grid)
        
        print(f"å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œå…± {len(param_combinations)} ç§å‚æ•°ç»„åˆ...")
        
        for i, params in enumerate(param_combinations):
            print(f"\næµ‹è¯•å‚æ•°ç»„åˆ {i+1}/{len(param_combinations)}: {params}")
            
            # è¯„ä¼°å½“å‰å‚æ•°ç»„åˆ
            metrics = self._evaluate_params(
                documents, test_queries, params
            )
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            composite_score = sum(
                metrics.get(metric, 0) * weight 
                for metric, weight in scoring_weights.items()
            )
            
            result = {
                'params': params.copy(),
                'metrics': metrics,
                'composite_score': composite_score
            }
            results.append(result)
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if composite_score > best_score:
                best_score = composite_score
                best_params = params.copy()
            
            print(f"ç»¼åˆå¾—åˆ†: {composite_score:.4f}")
        
        return OptimizationResult(
            best_params=best_params,
            best_score=best_score,
            all_results=results,
            optimization_history=results
        )
    
    def _generate_param_combinations(self, param_grid: Dict[str, List]) -> List[Dict]:
        """ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ"""
        import itertools
        
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        
        combinations = []
        for combination in itertools.product(*values):
            param_dict = dict(zip(keys, combination))
            combinations.append(param_dict)
        
        return combinations
    
    def _evaluate_params(self, 
                        documents: List[str], 
                        test_queries: List[str], 
                        params: Dict) -> Dict[str, float]:
        """è¯„ä¼°ç‰¹å®šå‚æ•°ç»„åˆçš„æ€§èƒ½"""
        
        # åˆ›å»ºåˆ†å—é…ç½®
        config = ChunkingConfig(
            chunk_size=params.get('chunk_size', 500),
            chunk_overlap=int(params.get('chunk_size', 500) * params.get('overlap_ratio', 0.2)),
            min_chunk_size=params.get('min_chunk_size', 100)
        )
        
        # å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†å—
        all_chunks = []
        chunk_doc_mapping = []  # è®°å½•æ¯ä¸ªchunkå±äºå“ªä¸ªæ–‡æ¡£
        
        for doc_idx, document in enumerate(documents):
            chunks = self.chunk_manager.chunk_text(
                document, 
                chunker_type=params.get('chunker_type', 'sentence'),
                config=config
            )
            
            chunk_texts = [chunk.content for chunk in chunks]
            all_chunks.extend(chunk_texts)
            chunk_doc_mapping.extend([doc_idx] * len(chunk_texts))
        
        if not all_chunks:
            return {'precision': 0, 'recall': 0, 'coherence': 0, 'efficiency': 0}
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        metrics = {}
        
        # 1. æ£€ç´¢ç²¾åº¦å’Œå¬å›ç‡
        precision, recall = self._calculate_retrieval_metrics(
            all_chunks, chunk_doc_mapping, test_queries, documents
        )
        metrics['precision'] = precision
        metrics['recall'] = recall
        
        # 2. è¯­ä¹‰è¿è´¯æ€§
        coherence = self._calculate_semantic_coherence(all_chunks)
        metrics['coherence'] = coherence
        
        # 3. è®¡ç®—æ•ˆç‡
        efficiency = self._calculate_efficiency(all_chunks, params)
        metrics['efficiency'] = efficiency
        
        # 4. F1åˆ†æ•°
        if precision + recall > 0:
            metrics['f1_score'] = 2 * (precision * recall) / (precision + recall)
        else:
            metrics['f1_score'] = 0
        
        return metrics
    
    def _calculate_retrieval_metrics(self, 
                                   chunks: List[str], 
                                   chunk_doc_mapping: List[int],
                                   queries: List[str], 
                                   documents: List[str]) -> Tuple[float, float]:
        """è®¡ç®—æ£€ç´¢ç²¾åº¦å’Œå¬å›ç‡"""
        if not chunks or not queries:
            return 0.0, 0.0
        
        try:
            # è·å–chunkå’Œqueryçš„åµŒå…¥
            chunk_embeddings = self.embedding_service.encode_batch(chunks)
            query_embeddings = self.embedding_service.encode_batch(queries)
            
            total_precision = 0
            total_recall = 0
            valid_queries = 0
            
            for query_idx, query_emb in enumerate(query_embeddings):
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = cosine_similarity([query_emb], chunk_embeddings)[0]
                
                # è·å–top-kæœ€ç›¸ä¼¼çš„chunks
                k = min(5, len(chunks))
                top_k_indices = np.argsort(similarities)[-k:][::-1]
                
                # è®¡ç®—ç²¾åº¦ï¼štop-kä¸­æœ‰å¤šå°‘æ¥è‡ªç›¸å…³æ–‡æ¡£
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£éƒ½ç›¸å…³
                relevant_chunks = len(set(chunk_doc_mapping[i] for i in top_k_indices))
                precision = relevant_chunks / len(set(chunk_doc_mapping))
                
                # è®¡ç®—å¬å›ç‡ï¼šç›¸å…³æ–‡æ¡£çš„chunksæœ‰å¤šå°‘è¢«æ£€ç´¢åˆ°
                total_relevant = len(set(chunk_doc_mapping))
                recall = relevant_chunks / total_relevant if total_relevant > 0 else 0
                
                total_precision += precision
                total_recall += recall
                valid_queries += 1
            
            avg_precision = total_precision / valid_queries if valid_queries > 0 else 0
            avg_recall = total_recall / valid_queries if valid_queries > 0 else 0
            
            return avg_precision, avg_recall
            
        except Exception as e:
            print(f"è®¡ç®—æ£€ç´¢æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            return 0.0, 0.0
    
    def _calculate_semantic_coherence(self, chunks: List[str]) -> float:
        """è®¡ç®—è¯­ä¹‰è¿è´¯æ€§"""
        if len(chunks) < 2:
            return 1.0
        
        try:
            # è®¡ç®—ç›¸é‚»chunksçš„è¯­ä¹‰ç›¸ä¼¼åº¦
            embeddings = self.embedding_service.encode_batch(chunks)
            
            coherence_scores = []
            for i in range(len(embeddings) - 1):
                similarity = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]
                coherence_scores.append(similarity)
            
            return np.mean(coherence_scores) if coherence_scores else 0.0
            
        except Exception as e:
            print(f"è®¡ç®—è¯­ä¹‰è¿è´¯æ€§æ—¶å‡ºé”™: {e}")
            return 0.0
    
    def _calculate_efficiency(self, chunks: List[str], params: Dict) -> float:
        """è®¡ç®—æ•ˆç‡æŒ‡æ ‡ï¼ˆåŸºäºchunkæ•°é‡å’Œå¤§å°ï¼‰"""
        if not chunks:
            return 0.0
        
        # æ•ˆç‡æŒ‡æ ‡ï¼šè€ƒè™‘chunkæ•°é‡å’Œå¹³å‡é•¿åº¦
        avg_chunk_length = np.mean([len(chunk) for chunk in chunks])
        chunk_count = len(chunks)
        
        # ç†æƒ³çš„chunkæ•°é‡å’Œé•¿åº¦ï¼ˆå¯è°ƒæ•´ï¼‰
        ideal_chunk_length = params.get('chunk_size', 500)
        ideal_chunk_count_ratio = 1.0  # å‡è®¾ç†æƒ³æ¯”ä¾‹
        
        # é•¿åº¦æ•ˆç‡ï¼šè¶Šæ¥è¿‘ç†æƒ³é•¿åº¦è¶Šå¥½
        length_efficiency = 1 - abs(avg_chunk_length - ideal_chunk_length) / ideal_chunk_length
        length_efficiency = max(0, length_efficiency)
        
        # æ•°é‡æ•ˆç‡ï¼šé€‚ä¸­çš„chunkæ•°é‡
        count_efficiency = 1 / (1 + chunk_count * 0.001)  # æ•°é‡è¶Šå¤šæ•ˆç‡è¶Šä½
        
        return (length_efficiency + count_efficiency) / 2
    
    def bayesian_optimization(self, 
                            documents: List[str],
                            test_queries: List[str],
                            param_bounds: Dict[str, Tuple[float, float]],
                            n_iterations: int = 20) -> OptimizationResult:
        """
        è´å¶æ–¯ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            documents: æµ‹è¯•æ–‡æ¡£
            test_queries: æµ‹è¯•æŸ¥è¯¢
            param_bounds: å‚æ•°è¾¹ç•Œï¼Œå¦‚ {'chunk_size': (200, 1500), 'overlap_ratio': (0.1, 0.5)}
            n_iterations: è¿­ä»£æ¬¡æ•°
        
        Returns:
            OptimizationResult: ä¼˜åŒ–ç»“æœ
        """
        # è¿™é‡Œå®ç°ç®€åŒ–çš„éšæœºæœç´¢ä½œä¸ºè´å¶æ–¯ä¼˜åŒ–çš„æ›¿ä»£
        results = []
        best_score = -1
        best_params = None
        
        print(f"å¼€å§‹è´å¶æ–¯ä¼˜åŒ–ï¼Œè¿­ä»£ {n_iterations} æ¬¡...")
        
        for i in range(n_iterations):
            # éšæœºé‡‡æ ·å‚æ•°
            params = {}
            for param_name, (min_val, max_val) in param_bounds.items():
                if param_name == 'chunk_size':
                    params[param_name] = int(np.random.uniform(min_val, max_val))
                else:
                    params[param_name] = np.random.uniform(min_val, max_val)
            
            print(f"\nè¿­ä»£ {i+1}/{n_iterations}: {params}")
            
            # è¯„ä¼°å‚æ•°
            metrics = self._evaluate_params(documents, test_queries, params)
            composite_score = np.mean(list(metrics.values()))
            
            result = {
                'params': params.copy(),
                'metrics': metrics,
                'composite_score': composite_score
            }
            results.append(result)
            
            if composite_score > best_score:
                best_score = composite_score
                best_params = params.copy()
            
            print(f"ç»¼åˆå¾—åˆ†: {composite_score:.4f}")
        
        return OptimizationResult(
            best_params=best_params,
            best_score=best_score,
            all_results=results,
            optimization_history=results
        )
```

### 2. MockRAGSystem ç±»å®ç°

åˆ›å»º `src/experiments/chunk_optimization/mock_rag_system.py`ï¼š

```python
import numpy as np
from typing import List, Dict, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from ...chunking import ChunkManager, ChunkingConfig
from ...embedding import EmbeddingService

class MockRAGSystem:
    """æ¨¡æ‹ŸRAGç³»ç»Ÿï¼Œç”¨äºæµ‹è¯•ä¸åŒåˆ†å—ç­–ç•¥"""
    
    def __init__(self, embedding_service: EmbeddingService):
        self.embedding_service = embedding_service
        self.chunk_manager = ChunkManager()
        self.knowledge_base = []
        self.chunk_embeddings = []
    
    def build_knowledge_base(self, 
                           documents: List[str], 
                           chunking_config: ChunkingConfig,
                           chunker_type: str = 'sentence') -> Dict[str, any]:
        """
        æ„å»ºçŸ¥è¯†åº“
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            chunking_config: åˆ†å—é…ç½®
            chunker_type: åˆ†å—å™¨ç±»å‹
        
        Returns:
            Dict: æ„å»ºç»Ÿè®¡ä¿¡æ¯
        """
        print(f"æ„å»ºçŸ¥è¯†åº“ï¼Œä½¿ç”¨ {chunker_type} åˆ†å—å™¨...")
        
        self.knowledge_base = []
        chunk_texts = []
        
        for doc_idx, document in enumerate(documents):
            # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—
            chunks = self.chunk_manager.chunk_text(
                document, 
                chunker_type=chunker_type,
                config=chunking_config
            )
            
            for chunk in chunks:
                chunk_info = {
                    'content': chunk.content,
                    'doc_id': doc_idx,
                    'chunk_id': chunk.metadata.chunk_id,
                    'chunk_index': chunk.metadata.chunk_index,
                    'metadata': chunk.metadata
                }
                self.knowledge_base.append(chunk_info)
                chunk_texts.append(chunk.content)
        
        # è®¡ç®—embeddings
        if chunk_texts:
            print(f"è®¡ç®— {len(chunk_texts)} ä¸ªchunksçš„embeddings...")
            self.chunk_embeddings = self.embedding_service.encode_batch(chunk_texts)
        else:
            self.chunk_embeddings = []
        
        stats = {
            'total_documents': len(documents),
            'total_chunks': len(self.knowledge_base),
            'avg_chunks_per_doc': len(self.knowledge_base) / len(documents) if documents else 0,
            'avg_chunk_length': np.mean([len(chunk['content']) for chunk in self.knowledge_base]) if self.knowledge_base else 0
        }
        
        print(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {stats}")
        return stats
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³chunks
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›top-kä¸ªç»“æœ
        
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        if not self.knowledge_base or not self.chunk_embeddings:
            return []
        
        # è®¡ç®—æŸ¥è¯¢embedding
        query_embedding = self.embedding_service.encode([query])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity([query_embedding], self.chunk_embeddings)[0]
        
        # è·å–top-kç»“æœ
        top_k = min(top_k, len(similarities))
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            result = {
                'chunk': self.knowledge_base[idx],
                'similarity': float(similarities[idx]),
                'rank': len(results) + 1
            }
            results.append(result)
        
        return results
    
    def evaluate_retrieval(self, 
                         test_queries: List[str], 
                         ground_truth: List[List[int]] = None) -> Dict[str, float]:
        """
        è¯„ä¼°æ£€ç´¢æ€§èƒ½
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            ground_truth: æ¯ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Dict: è¯„ä¼°æŒ‡æ ‡
        """
        if not test_queries:
            return {}
        
        metrics = {
            'avg_similarity': 0,
            'coverage': 0,
            'diversity': 0
        }
        
        total_similarity = 0
        covered_docs = set()
        all_similarities = []
        
        for query in test_queries:
            results = self.retrieve(query, top_k=5)
            
            if results:
                # å¹³å‡ç›¸ä¼¼åº¦
                query_similarities = [r['similarity'] for r in results]
                total_similarity += np.mean(query_similarities)
                all_similarities.extend(query_similarities)
                
                # æ–‡æ¡£è¦†ç›–åº¦
                for result in results:
                    covered_docs.add(result['chunk']['doc_id'])
        
        # è®¡ç®—æŒ‡æ ‡
        if test_queries:
            metrics['avg_similarity'] = total_similarity / len(test_queries)
        
        total_docs = len(set(chunk['doc_id'] for chunk in self.knowledge_base))
        if total_docs > 0:
            metrics['coverage'] = len(covered_docs) / total_docs
        
        if all_similarities:
            metrics['diversity'] = np.std(all_similarities)  # ç›¸ä¼¼åº¦çš„æ ‡å‡†å·®ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
        
        return metrics
    
    def analyze_chunk_distribution(self) -> Dict[str, any]:
        """
        åˆ†æchunkåˆ†å¸ƒæƒ…å†µ
        
        Returns:
            Dict: åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯
        """
        if not self.knowledge_base:
            return {}
        
        chunk_lengths = [len(chunk['content']) for chunk in self.knowledge_base]
        doc_chunk_counts = {}
        
        for chunk in self.knowledge_base:
            doc_id = chunk['doc_id']
            doc_chunk_counts[doc_id] = doc_chunk_counts.get(doc_id, 0) + 1
        
        analysis = {
            'chunk_count': len(self.knowledge_base),
            'length_stats': {
                'mean': np.mean(chunk_lengths),
                'std': np.std(chunk_lengths),
                'min': np.min(chunk_lengths),
                'max': np.max(chunk_lengths),
                'median': np.median(chunk_lengths)
            },
            'chunks_per_doc': {
                'mean': np.mean(list(doc_chunk_counts.values())),
                'std': np.std(list(doc_chunk_counts.values())),
                'min': np.min(list(doc_chunk_counts.values())),
                'max': np.max(list(doc_chunk_counts.values()))
            }
        }
        
        return analysis
```

### 3. äº¤äº’å¼è°ƒä¼˜å·¥å…·

åˆ›å»º `src/experiments/chunk_optimization/interactive_tuner.py`ï¼š

```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from typing import List, Dict

from ...chunking.chunk_optimizer import ChunkOptimizer
from .mock_rag_system import MockRAGSystem
from ...embedding import EmbeddingService
from ...chunking import ChunkingConfig

class InteractiveChunkTuner:
    """äº¤äº’å¼Chunkå‚æ•°è°ƒä¼˜å·¥å…·"""
    
    def __init__(self):
        self.embedding_service = None
        self.optimizer = None
        self.mock_rag = None
        self.test_documents = []
        self.test_queries = []
    
    def initialize_services(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        if 'embedding_service' not in st.session_state:
            st.session_state.embedding_service = EmbeddingService()
        
        self.embedding_service = st.session_state.embedding_service
        self.optimizer = ChunkOptimizer(self.embedding_service)
        self.mock_rag = MockRAGSystem(self.embedding_service)
    
    def run_app(self):
        """è¿è¡ŒStreamlitåº”ç”¨"""
        st.set_page_config(
            page_title="Chunkå‚æ•°è°ƒä¼˜å·¥å…·",
            page_icon="ğŸ”§",
            layout="wide"
        )
        
        st.title("ğŸ”§ Chunkå‚æ•°è°ƒä¼˜å·¥å…·")
        st.markdown("---")
        
        # åˆå§‹åŒ–æœåŠ¡
        self.initialize_services()
        
        # ä¾§è¾¹æ é…ç½®
        self._render_sidebar()
        
        # ä¸»ç•Œé¢
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š å‚æ•°å®éªŒ", "ğŸ” å®æ—¶æµ‹è¯•", "ğŸ“ˆ ç»“æœåˆ†æ", "ğŸ’¾ å†å²è®°å½•"])
        
        with tab1:
            self._render_parameter_experiment()
        
        with tab2:
            self._render_real_time_test()
        
        with tab3:
            self._render_result_analysis()
        
        with tab4:
            self._render_history()
    
    def _render_sidebar(self):
        """æ¸²æŸ“ä¾§è¾¹æ """
        st.sidebar.header("ğŸ› ï¸ é…ç½®")
        
        # æµ‹è¯•æ•°æ®é…ç½®
        st.sidebar.subheader("æµ‹è¯•æ•°æ®")
        
        # æ–‡æ¡£è¾“å…¥
        doc_input_method = st.sidebar.radio(
            "æ–‡æ¡£è¾“å…¥æ–¹å¼",
            ["æ‰‹åŠ¨è¾“å…¥", "ç¤ºä¾‹æ•°æ®", "æ–‡ä»¶ä¸Šä¼ "]
        )
        
        if doc_input_method == "æ‰‹åŠ¨è¾“å…¥":
            doc_text = st.sidebar.text_area(
                "è¾“å…¥æµ‹è¯•æ–‡æ¡£ï¼ˆæ¯è¡Œä¸€ä¸ªæ–‡æ¡£ï¼‰",
                height=150,
                placeholder="è¾“å…¥æ‚¨çš„æµ‹è¯•æ–‡æ¡£..."
            )
            if doc_text:
                self.test_documents = [doc.strip() for doc in doc_text.split('\n') if doc.strip()]
        
        elif doc_input_method == "ç¤ºä¾‹æ•°æ®":
            self.test_documents = self._get_sample_documents()
            st.sidebar.success(f"å·²åŠ è½½ {len(self.test_documents)} ä¸ªç¤ºä¾‹æ–‡æ¡£")
        
        # æŸ¥è¯¢è¾“å…¥
        query_text = st.sidebar.text_area(
            "æµ‹è¯•æŸ¥è¯¢ï¼ˆæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼‰",
            height=100,
            placeholder="è¾“å…¥æ‚¨çš„æµ‹è¯•æŸ¥è¯¢..."
        )
        if query_text:
            self.test_queries = [q.strip() for q in query_text.split('\n') if q.strip()]
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        if self.test_documents or self.test_queries:
            st.sidebar.markdown("**æ•°æ®ç»Ÿè®¡:**")
            st.sidebar.write(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {len(self.test_documents)}")
            st.sidebar.write(f"â“ æŸ¥è¯¢æ•°é‡: {len(self.test_queries)}")
    
    def _render_parameter_experiment(self):
        """æ¸²æŸ“å‚æ•°å®éªŒç•Œé¢"""
        st.header("ğŸ“Š å‚æ•°ç½‘æ ¼æœç´¢å®éªŒ")
        
        if not self.test_documents or not self.test_queries:
            st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®æµ‹è¯•æ•°æ®å’ŒæŸ¥è¯¢")
            return
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("å‚æ•°é…ç½®")
            
            # Chunkå¤§å°èŒƒå›´
            chunk_sizes = st.multiselect(
                "Chunkå¤§å°",
                [200, 300, 500, 800, 1000, 1200, 1500],
                default=[300, 500, 800]
            )
            
            # é‡å æ¯”ä¾‹
            overlap_ratios = st.multiselect(
                "é‡å æ¯”ä¾‹",
                [0.1, 0.15, 0.2, 0.25, 0.3, 0.4],
                default=[0.1, 0.2, 0.3]
            )
            
            # åˆ†å—å™¨ç±»å‹
            chunker_type = st.selectbox(
                "åˆ†å—å™¨ç±»å‹",
                ["sentence", "semantic", "structure"],
                index=0
            )
        
        with col2:
            st.subheader("è¯„ä¼°æƒé‡")
            
            precision_weight = st.slider("ç²¾ç¡®åº¦æƒé‡", 0.0, 1.0, 0.3, 0.1)
            recall_weight = st.slider("å¬å›ç‡æƒé‡", 0.0, 1.0, 0.3, 0.1)
            coherence_weight = st.slider("è¿è´¯æ€§æƒé‡", 0.0, 1.0, 0.2, 0.1)
            efficiency_weight = st.slider("æ•ˆç‡æƒé‡", 0.0, 1.0, 0.2, 0.1)
            
            # å½’ä¸€åŒ–æƒé‡
            total_weight = precision_weight + recall_weight + coherence_weight + efficiency_weight
            if total_weight > 0:
                weights = {
                    'precision': precision_weight / total_weight,
                    'recall': recall_weight / total_weight,
                    'coherence': coherence_weight / total_weight,
                    'efficiency': efficiency_weight / total_weight
                }
            else:
                weights = {'precision': 0.25, 'recall': 0.25, 'coherence': 0.25, 'efficiency': 0.25}
        
        # å¼€å§‹å®éªŒæŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹å‚æ•°ä¼˜åŒ–å®éªŒ", type="primary"):
            if chunk_sizes and overlap_ratios:
                with st.spinner("æ­£åœ¨è¿›è¡Œå‚æ•°ä¼˜åŒ–å®éªŒ..."):
                    param_grid = {
                        'chunk_size': chunk_sizes,
                        'overlap_ratio': overlap_ratios,
                        'chunker_type': [chunker_type]
                    }
                    
                    result = self.optimizer.grid_search(
                        self.test_documents,
                        self.test_queries,
                        param_grid,
                        weights
                    )
                    
                    # ä¿å­˜ç»“æœåˆ°session state
                    st.session_state.experiment_result = result
                    
                    st.success("å®éªŒå®Œæˆï¼")
                    
                    # æ˜¾ç¤ºæœ€ä½³å‚æ•°
                    st.subheader("ğŸ† æœ€ä½³å‚æ•°")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("æœ€ä½³Chunkå¤§å°", result.best_params['chunk_size'])
                    with col2:
                        st.metric("æœ€ä½³é‡å æ¯”ä¾‹", f"{result.best_params['overlap_ratio']:.2f}")
                    with col3:
                        st.metric("æœ€ä½³ç»¼åˆå¾—åˆ†", f"{result.best_score:.4f}")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                    self._display_experiment_results(result)
            else:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªchunkå¤§å°å’Œé‡å æ¯”ä¾‹")
    
    def _render_real_time_test(self):
        """æ¸²æŸ“å®æ—¶æµ‹è¯•ç•Œé¢"""
        st.header("ğŸ” å®æ—¶å‚æ•°æµ‹è¯•")
        
        if not self.test_documents:
            st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®æµ‹è¯•æ–‡æ¡£")
            return
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.subheader("å‚æ•°è®¾ç½®")
            
            chunk_size = st.slider("Chunkå¤§å°", 100, 2000, 500, 50)
            overlap_ratio = st.slider("é‡å æ¯”ä¾‹", 0.0, 0.5, 0.2, 0.05)
            chunker_type = st.selectbox(
                "åˆ†å—å™¨ç±»å‹",
                ["sentence", "semantic", "structure"]
            )
            
            test_query = st.text_input(
                "æµ‹è¯•æŸ¥è¯¢",
                placeholder="è¾“å…¥æŸ¥è¯¢æ¥æµ‹è¯•æ£€ç´¢æ•ˆæœ..."
            )
        
        with col2:
            st.subheader("å®æ—¶ç»“æœ")
            
            if st.button("ğŸ”„ æ›´æ–°åˆ†å—ç»“æœ"):
                config = ChunkingConfig(
                    chunk_size=chunk_size,
                    chunk_overlap=int(chunk_size * overlap_ratio)
                )
                
                # æ„å»ºçŸ¥è¯†åº“
                stats = self.mock_rag.build_knowledge_base(
                    self.test_documents, config, chunker_type
                )
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col_a, col_b, col_c = st.columns(3)
                with col_a:
                    st.metric("æ€»Chunkæ•°", stats['total_chunks'])
                with col_b:
                    st.metric("å¹³å‡æ¯æ–‡æ¡£Chunkæ•°", f"{stats['avg_chunks_per_doc']:.1f}")
                with col_c:
                    st.metric("å¹³å‡Chunké•¿åº¦", f"{stats['avg_chunk_length']:.0f}")
                
                # æ˜¾ç¤ºchunkåˆ†å¸ƒ
                analysis = self.mock_rag.analyze_chunk_distribution()
                if analysis:
                    st.subheader("Chunké•¿åº¦åˆ†å¸ƒ")
                    
                    chunk_lengths = [len(chunk['content']) for chunk in self.mock_rag.knowledge_base]
                    fig = px.histogram(
                        x=chunk_lengths,
                        nbins=20,
                        title="Chunké•¿åº¦åˆ†å¸ƒ",
                        labels={'x': 'Chunké•¿åº¦', 'y': 'é¢‘æ¬¡'}
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            # æµ‹è¯•æŸ¥è¯¢
            if test_query and hasattr(self.mock_rag, 'knowledge_base') and self.mock_rag.knowledge_base:
                st.subheader(f"æŸ¥è¯¢ç»“æœ: \"{test_query}\"")
                
                results = self.mock_rag.retrieve(test_query, top_k=3)
                
                for i, result in enumerate(results):
                    with st.expander(f"ç»“æœ {i+1} (ç›¸ä¼¼åº¦: {result['similarity']:.4f})"):
                        st.write(result['chunk']['content'])
                        st.caption(f"æ–‡æ¡£ID: {