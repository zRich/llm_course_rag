{
  "lesson02": {
    "lesson": "lesson02",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "analyze_branches.py",
        "main.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "src/__init__.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/smart_paragraph_chunker.py"
      ],
      "file_count": 24,
      "total_lines": 8530,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        }
      }
    },
    "feature_analysis": {
      "docker": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "container": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "postgresql": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "postgresql",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.3
      },
      "redis": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      }
    },
    "code_quality": {
      "total_files": 24,
      "total_lines": 8530,
      "total_code_lines": 6490,
      "avg_file_size": 355.4166666666667,
      "code_ratio": 0.7608440797186401,
      "quality_score": 76.08440797186401
    },
    "missing_implementations": []
  },
  "lesson03": {
    "lesson": "lesson03",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "analyze_branches.py",
        "main.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "src/__init__.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/smart_paragraph_chunker.py"
      ],
      "file_count": 24,
      "total_lines": 8530,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        }
      }
    },
    "feature_analysis": {
      "database": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "database",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "model": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "migration": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "schema": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "schema",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/api_connector.py",
            "keyword": "schema",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/base.py",
            "keyword": "schema",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.8999999999999999
      }
    },
    "code_quality": {
      "total_files": 24,
      "total_lines": 8530,
      "total_code_lines": 6490,
      "avg_file_size": 355.4166666666667,
      "code_ratio": 0.7608440797186401,
      "quality_score": 76.08440797186401
    },
    "missing_implementations": []
  },
  "lesson04": {
    "lesson": "lesson04",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "analyze_branches.py",
        "main.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "src/__init__.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/smart_paragraph_chunker.py"
      ],
      "file_count": 24,
      "total_lines": 8530,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        }
      }
    },
    "feature_analysis": {
      "pdf": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/incremental/config.py",
            "keyword": "pdf",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.3
      },
      "parse": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/data_connectors/api_connector.py",
            "keyword": "parse",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.3
      },
      "chunk": {
        "implemented": true,
        "evidence": [
          {
            "file": "lesson19/smart_paragraph_chunker_template.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/test_smart_paragraph.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/plugin_registry.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/strategy_interface.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/smart_paragraph_chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "split": {
        "implemented": true,
        "evidence": [
          {
            "file": "lesson19/smart_paragraph_chunker_template.py",
            "keyword": "split",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/test_smart_paragraph.py",
            "keyword": "split",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/smart_paragraph_chunker.py",
            "keyword": "split",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.8999999999999999
      }
    },
    "code_quality": {
      "total_files": 24,
      "total_lines": 8530,
      "total_code_lines": 6490,
      "avg_file_size": 355.4166666666667,
      "code_ratio": 0.7608440797186401,
      "quality_score": 76.08440797186401
    },
    "missing_implementations": []
  },
  "lesson06": {
    "lesson": "lesson06",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "test_chunking.py",
        "test_repositories.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 77,
      "total_lines": 22686,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "rag": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_connections.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "main.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/smart_paragraph_chunker_template.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/test_smart_paragraph.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "scripts/start_dev.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/main.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/database/config.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/conflict_resolver.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/change_detector.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/document/docx_parser.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/document/txt_parser.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/rag_service.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/__init__.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/structure_chunker.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/smart_paragraph_chunker.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/api/health.py",
            "keyword": "rag",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "rag",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "retrieval": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/rag/retriever.py",
            "keyword": "retrieval",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.3
      },
      "generation": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_database.py",
            "keyword": "generation",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.3
      },
      "query": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_repositories.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "test_models.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "scripts/migrate_data.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/query.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/__init__.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/rag_service.py",
            "keyword": "query",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "query",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      }
    },
    "code_quality": {
      "total_files": 77,
      "total_lines": 22686,
      "total_code_lines": 17161,
      "avg_file_size": 294.6233766233766,
      "code_ratio": 0.7564577272326545,
      "quality_score": 75.64577272326545
    },
    "missing_implementations": []
  },
  "lesson08": {
    "lesson": "lesson08",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 80,
      "total_lines": 23038,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "hybrid_search": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_repositories.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_embedding.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_batch_vectorization.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_qdrant.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/embedding/embedder.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/document.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/rag_service.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/retriever.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/__init__.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/document_vectorizer.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/qdrant_client.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/semantic_chunker.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/api/embedding.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "vector",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "bm25": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "vector": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_repositories.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_embedding.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_batch_vectorization.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_qdrant.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/embedding/embedder.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/document.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/rag_service.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/retriever.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/__init__.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/document_vectorizer.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/qdrant_client.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/semantic_chunker.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/api/embedding.py",
            "keyword": "vector",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "vector",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "fusion": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      }
    },
    "code_quality": {
      "total_files": 80,
      "total_lines": 23038,
      "total_code_lines": 17424,
      "avg_file_size": 287.975,
      "code_ratio": 0.756315652400382,
      "quality_score": 75.6315652400382
    },
    "missing_implementations": []
  },
  "lesson09": {
    "lesson": "lesson09",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 80,
      "total_lines": 23038,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "metadata_filter": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_document_manager.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "keyword_search.py",
            "keyword": "filter",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/smart_paragraph_chunker_template.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_qdrant.py",
            "keyword": "filter",
            "context": "Found in code content"
          },
          {
            "file": "alembic/env.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/config.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/change_detector.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "filter",
            "context": "Found in code content"
          },
          {
            "file": "src/document/pdf_parser.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/document/docx_parser.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/document/__init__.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/document/parser.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/document/txt_parser.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/document/document_manager.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/retriever.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/qa_generator.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/qdrant_client.py",
            "keyword": "filter",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/qdrant_client.py",
            "keyword": "condition",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunker.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/__init__.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/strategy_interface.py",
            "keyword": "metadata",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/smart_paragraph_chunker.py",
            "keyword": "metadata",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "condition": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/vector_store/qdrant_client.py",
            "keyword": "condition",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.3
      },
      "where": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      }
    },
    "code_quality": {
      "total_files": 80,
      "total_lines": 23038,
      "total_code_lines": 17424,
      "avg_file_size": 287.975,
      "code_ratio": 0.756315652400382,
      "quality_score": 75.6315652400382
    },
    "missing_implementations": []
  },
  "lesson11": {
    "lesson": "lesson11",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "chunk": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_chunking.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "test_repositories.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "start_interactive_tuner.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "test_models.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "test_chunk_system.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/smart_paragraph_chunker_template.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "lesson19/test_smart_paragraph.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "scripts/migrate_data.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/interactive_tuner.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/run_chunk_experiment.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiment_visualizer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/mock_rag_system.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/chunk_optimizer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/__init__.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/document.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/document/chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/document/__init__.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/retriever.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/document_vectorizer.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/plugin_registry.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/structure_chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunk_manager.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/__init__.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/sentence_chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/strategy_interface.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/semantic_chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/smart_paragraph_chunker.py",
            "keyword": "chunk",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "overlap": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/chunk_experiment/chunk_optimizer.py",
            "keyword": "overlap",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
            "keyword": "overlap",
            "context": "Found in code content"
          },
          {
            "file": "src/document/chunker.py",
            "keyword": "overlap",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/sentence_chunker.py",
            "keyword": "overlap",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/strategy_interface.py",
            "keyword": "overlap",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "experiment": {
        "implemented": true,
        "evidence": [
          {
            "file": "start_interactive_tuner.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "test_chunk_system.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/interactive_tuner.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/run_chunk_experiment.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiment_visualizer.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/chunk_optimizer.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
            "keyword": "experiment",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
            "keyword": "experiment",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "size": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_models.py",
            "keyword": "size",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/chunk_optimizer.py",
            "keyword": "size",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
            "keyword": "size",
            "context": "Found in code content"
          },
          {
            "file": "src/document/chunker.py",
            "keyword": "size",
            "context": "Found in code content"
          },
          {
            "file": "src/document/parser.py",
            "keyword": "size",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/strategy_interface.py",
            "keyword": "size",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  },
  "lesson12": {
    "lesson": "lesson12",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "multi_document": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "word": {
        "implemented": true,
        "evidence": [
          {
            "file": "keyword_search.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "test_jieba.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "test_repositories.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "test_lesson07.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/database/config.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/database/init_db.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/user.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/document/docx_parser.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/document/__init__.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/document/txt_parser.py",
            "keyword": "word",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunker.py",
            "keyword": "word",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "excel": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "ppt": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  },
  "lesson13": {
    "lesson": "lesson13",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "citation": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/chunk_experiment/mock_rag_system.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/retriever.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/qa_generator.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunker.py",
            "keyword": "source",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "reference": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "source": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/chunk_experiment/mock_rag_system.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/retriever.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/qa_generator.py",
            "keyword": "source",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunker.py",
            "keyword": "source",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "provenance": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  },
  "lesson14": {
    "lesson": "lesson14",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "cache": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_connections.py",
            "keyword": "redis",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "redis",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/plugin_registry.py",
            "keyword": "cache",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.8999999999999999
      },
      "redis": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_connections.py",
            "keyword": "redis",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "redis",
            "context": "Found in code content"
          }
        ],
        "confidence": 0.6
      },
      "performance": {
        "implemented": true,
        "evidence": [
          {
            "file": "tests/test_qdrant.py",
            "keyword": "performance",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "performance",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiment_visualizer.py",
            "keyword": "performance",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
            "keyword": "performance",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  },
  "lesson15": {
    "lesson": "lesson15",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "batch_processing": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_document_manager.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "test_database.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "test_repositories.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_embedding.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_batch_vectorization.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "scripts/optimize_database.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "scripts/migrate_data.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "alembic/env.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/database/config.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/database/__init__.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/database/connection.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/database/init_db.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/embedding/embedder.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/user.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/query.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/document.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/base.py",
            "keyword": "async",
            "context": "Found in code content"
          },
          {
            "file": "src/document/document_manager.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/rag_service.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/document_vectorizer.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunk_manager.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/api/embedding.py",
            "keyword": "batch",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "batch",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "bulk": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      },
      "resume": {
        "implemented": false,
        "evidence": [],
        "confidence": 0.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  },
  "lesson16": {
    "lesson": "lesson16",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "incremental_update": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_repositories.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "test_models.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "scripts/verify_environment.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "scripts/migrate_data.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/main.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/conflict_resolver.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/conflict_resolver.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/config.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/config.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/__init__.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/__init__.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/change_detector.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/api_connector.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/base.py",
            "keyword": "incremental",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/base.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/user.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/query.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/query.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/document.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/base.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunker.py",
            "keyword": "update",
            "context": "Found in code content"
          },
          {
            "file": "src/api/health.py",
            "keyword": "version",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "delta": {
        "implemented": true,
        "evidence": [
          {
            "file": "src/incremental/conflict_resolver.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/change_detector.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "delta",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/query.py",
            "keyword": "delta",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "version": {
        "implemented": true,
        "evidence": [
          {
            "file": "scripts/verify_environment.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/main.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/__init__.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "version",
            "context": "Found in code content"
          },
          {
            "file": "src/api/health.py",
            "keyword": "version",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  },
  "lesson17": {
    "lesson": "lesson17",
    "branch_info": {
      "python_files": [
        "lesson_requirements_analysis.py",
        "test_connections.py",
        "test_document_manager.py",
        "test_database.py",
        "keyword_search.py",
        "test_jieba.py",
        "test_chunking.py",
        "test_repositories.py",
        "start_interactive_tuner.py",
        "compare_actual_vs_expected.py",
        "deep_code_investigation.py",
        "test_lesson07.py",
        "analyze_branches.py",
        "test_models.py",
        "test_pdf_parser.py",
        "main.py",
        "test_chunk_system.py",
        "lesson19/smart_paragraph_chunker_template.py",
        "lesson19/test_smart_paragraph.py",
        "tests/test_embedding.py",
        "tests/test_batch_vectorization.py",
        "tests/test_qdrant.py",
        "scripts/verify_environment.py",
        "scripts/test_services.py",
        "scripts/optimize_database.py",
        "scripts/migrate_data.py",
        "scripts/start_dev.py",
        "alembic/env.py",
        "src/config.py",
        "src/__init__.py",
        "src/main.py",
        "src/database/config.py",
        "src/database/__init__.py",
        "src/database/connection.py",
        "src/database/init_db.py",
        "src/incremental/conflict_resolver.py",
        "src/incremental/config.py",
        "src/incremental/version_manager.py",
        "src/incremental/monitoring.py",
        "src/incremental/__init__.py",
        "src/incremental/integration.py",
        "src/incremental/indexer.py",
        "src/incremental/change_detector.py",
        "src/data_connectors/database_connector.py",
        "src/data_connectors/__init__.py",
        "src/data_connectors/sync_manager.py",
        "src/data_connectors/api_connector.py",
        "src/data_connectors/base.py",
        "src/chunk_experiment/interactive_tuner.py",
        "src/chunk_experiment/run_chunk_experiment.py",
        "src/chunk_experiment/experiment_visualizer.py",
        "src/chunk_experiment/mock_rag_system.py",
        "src/chunk_experiment/chunk_optimizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py",
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
        "src/embedding/__init__.py",
        "src/embedding/embedder.py",
        "src/repositories/user.py",
        "src/repositories/query.py",
        "src/repositories/__init__.py",
        "src/repositories/document.py",
        "src/repositories/base.py",
        "src/document/pdf_parser.py",
        "src/document/chunker.py",
        "src/document/docx_parser.py",
        "src/document/__init__.py",
        "src/document/parser.py",
        "src/document/txt_parser.py",
        "src/document/document_manager.py",
        "src/rag/rag_service.py",
        "src/rag/retriever.py",
        "src/rag/__init__.py",
        "src/rag/qa_generator.py",
        "src/vector_store/__init__.py",
        "src/vector_store/document_vectorizer.py",
        "src/vector_store/qdrant_client.py",
        "src/chunking/plugin_registry.py",
        "src/chunking/structure_chunker.py",
        "src/chunking/chunker.py",
        "src/chunking/chunk_manager.py",
        "src/chunking/__init__.py",
        "src/chunking/sentence_chunker.py",
        "src/chunking/strategy_interface.py",
        "src/chunking/semantic_chunker.py",
        "src/chunking/smart_paragraph_chunker.py",
        "src/api/embedding.py",
        "src/api/health.py",
        "src/api/__init__.py",
        "src/api/rag.py"
      ],
      "file_count": 92,
      "total_lines": 27524,
      "file_details": {
        "lesson_requirements_analysis.py": {
          "total_lines": 398,
          "code_lines": 364,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n课程要求分析脚本\n根据课程讲义内容，分析每个lesson应该实现的具体功能和代码变更\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Any\n\ndef analyze_lesson_requirements() -> Dict[str, Any]:\n    \"\"\"\n    根据课程讲义分析每个lesson的具体开发要求\n    \"\"\"\n    \n    lesson_requirements = {\n        \"lesson01\": {\n            \"module\": \"A\",\n            \"title\": \"课程导入与环境准备\",\n            \"expected_changes\": [\n                \"创建基础项目结构\",\n                \"配置Python环境和依赖管理(uv)\",\n                \"创建最小FastAPI应用\",\n                \"配置开发环境\"\n     ...",
          "imports": [
            "import json",
            "from typing import Dict, List, Any"
          ],
          "functions": [
            "analyze_lesson_requirements",
            "save_requirements_analysis",
            "print_summary"
          ],
          "classes": []
        },
        "test_connections.py": {
          "total_lines": 311,
          "code_lines": 237,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统依赖服务连接测试脚本\n\n这个脚本用于测试所有依赖服务的连接状态，包括：\n- PostgreSQL 数据库\n- Qdrant 向量数据库\n- Redis 缓存\n- MinIO 对象存储\n\n使用方法：\n    python test_connections.py\n\"\"\"\n\nimport sys\nimport time\nimport os\nfrom typing import Dict, Any, Optional\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n\ndef test_postgres() -> bool:\n    \"\"\"测试PostgreSQL连接\"\"\"\n    try:\n        import psycopg2\n        from psycopg2 import sql\n        \n        # 从环境变量获取连接参数\n        conn_params = {\n            \"host\": os.getenv(...",
          "imports": [
            "import sys",
            "import time",
            "import os",
            "from typing import Dict, Any, Optional",
            "from dotenv import load_dotenv",
            "import psycopg2",
            "from psycopg2 import sql",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.http import models",
            "import redis",
            "from minio import Minio",
            "from minio.error import S3Error",
            "import subprocess",
            "import json"
          ],
          "functions": [
            "test_postgres",
            "test_qdrant",
            "test_redis",
            "test_minio",
            "check_docker_services",
            "main"
          ],
          "classes": []
        },
        "test_document_manager.py": {
          "total_lines": 350,
          "code_lines": 239,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n文档管理器测试脚本\n\n测试文档管理器的统一文档解析功能，包括：\n- 多种文档格式解析\n- 批量文档处理\n- 元数据提取\n- 解析器管理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.document_manager import document_manager\nfrom src.document.parser import DocumentParser\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.docx_parser import DocxParser\nfrom src.document.t...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.document_manager import document_manager",
            "from src.document.parser import DocumentParser",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.docx_parser import DocxParser",
            "from src.document.txt_parser import TxtParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_document_manager_basic",
            "test_single_document_parsing",
            "test_batch_document_parsing",
            "test_document_search",
            "test_parser_registration",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": [
            "CustomParser(DocumentParser)"
          ]
        },
        "test_database.py": {
          "total_lines": 340,
          "code_lines": 250,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库测试文件\n\n测试数据库连接、配置和初始化功能\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom src.database import (\n    DatabaseConfig, db_config,\n    DatabaseManager, db_manager,\n    get_sync_session, get_async_session,\n    init_database, close_database, check_database_health\n)\nfrom src.config import settings\n\n\nclass TestDatabaseConfig:\n    \"\"\"数据库配置测试\"\"\"\n    \n...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import patch, MagicMock",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import SQLAlchemyError",
            "from src.database import (",
            "from src.config import settings",
            "from src.database.init_db import create_database_if_not_exists",
            "from src.database.init_db import create_extensions",
            "from src.database.init_db import create_indexes",
            "from src.database.init_db import create_default_admin"
          ],
          "functions": [
            "test_config_initialization",
            "test_sync_url_generation",
            "test_async_url_generation",
            "test_alembic_url_generation",
            "test_connection_params",
            "test_engine_params",
            "test_manager_initialization",
            "test_init_sync_engine",
            "test_init_async_engine",
            "test_get_sync_session",
            "test_init_database",
            "test_close_database",
            "test_check_database_health_success",
            "test_check_database_health_failure",
            "test_get_sync_session_function",
            "test_create_database_if_not_exists",
            "test_create_extensions",
            "test_create_indexes",
            "test_create_default_admin",
            "test_global_config_instance",
            "test_global_manager_instance",
            "test_config_from_settings"
          ],
          "classes": [
            "TestDatabaseConfig",
            "TestDatabaseManager",
            "TestDatabaseOperations",
            "TestSessionManagement",
            "TestDatabaseInitialization",
            "TestConfigIntegration"
          ]
        },
        "keyword_search.py": {
          "total_lines": 108,
          "code_lines": 76,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n关键词搜索引擎\n基于PostgreSQL全文检索和jieba中文分词\n\"\"\"\n\nimport jieba\nimport psycopg2\nfrom typing import List, Dict\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef preprocess_query(query: str) -> str:\n    \"\"\"预处理查询文本\"\"\"\n    # 使用jieba分词\n    words = jieba.lcut_for_search(query)\n    \n    # 过滤空词和单字符\n    filtered_words = [w.strip() for w in words if len(w.strip(...",
          "imports": [
            "import jieba",
            "import psycopg2",
            "from typing import List, Dict"
          ],
          "functions": [
            "preprocess_query",
            "keyword_search",
            "test_search"
          ],
          "classes": []
        },
        "test_jieba.py": {
          "total_lines": 38,
          "code_lines": 24,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n中文分词测试模块\n演示jieba分词的基本用法\n\"\"\"\n\nimport jieba\n\ndef test_segmentation():\n    \"\"\"测试中文分词功能\"\"\"\n    # 测试文本\n    test_texts = [\n        \"Python是一种高级编程语言\",\n        \"数据库管理系统\",\n        \"机器学习和人工智能\"\n    ]\n    \n    print(\"🔤 中文分词测试\")\n    print(\"=\" * 40)\n    \n    for i, text in enumerate(test_texts, 1):\n        print(f\"\\n测试 {i}: {text}\")\n        \n        # 精确模式\n        words1 = jieba.lcut(text)\n        print(f\"精确模式: {' / '.join(words1)}\")\n        \n        # 搜索模式\n ...",
          "imports": [
            "import jieba"
          ],
          "functions": [
            "test_segmentation"
          ],
          "classes": []
        },
        "test_chunking.py": {
          "total_lines": 431,
          "code_lines": 288,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n分块器测试脚本\n\n测试各种文档分块策略，包括：\n- 基于句子的分块器\n- 基于语义的分块器\n- 基于结构的分块器\n- 分块管理器\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.chunking.sentence_chunker import SentenceChunker\nfrom src.chunking.semantic_chunker import SemanticChunker\nfrom src.chunking.structure_chunker import StructureChunker\nfrom src.chunking.chunk_manager import chunk_m...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.chunking.sentence_chunker import SentenceChunker",
            "from src.chunking.semantic_chunker import SemanticChunker",
            "from src.chunking.structure_chunker import StructureChunker",
            "from src.chunking.chunk_manager import chunk_manager",
            "from src.chunking.chunker import ChunkingConfig",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_sentence_chunker",
            "test_semantic_chunker",
            "test_structure_chunker",
            "test_chunk_manager",
            "test_file_chunking",
            "test_chunk_export",
            "test_chunking_config",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "test_repositories.py": {
          "total_lines": 504,
          "code_lines": 363,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n仓库测试文件\n\n测试所有仓库类的CRUD操作和业务逻辑\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import MagicMock, patch\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.repositories import (\n    BaseRepository,\n    UserRepository, user_repository,\n    DocumentRepository, DocumentChunkRepository,\n    document_repository, document_chunk_repository,\n    QueryHistoryRepository, SystemConfigRepository,\n    query_h...",
          "imports": [
            "import pytest",
            "import asyncio",
            "from unittest.mock import MagicMock, patch",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.repositories import (",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "setup_method",
            "test_repository_initialization",
            "test_create_sync",
            "test_get_by_id_sync",
            "test_get_all_sync",
            "test_update_sync",
            "test_delete_sync",
            "setup_method",
            "test_get_by_username",
            "test_get_by_email",
            "test_hash_password",
            "test_verify_password",
            "test_authenticate_user",
            "test_get_active_users",
            "setup_method",
            "test_get_by_title",
            "test_get_by_hash",
            "test_get_by_owner",
            "test_get_by_status",
            "setup_method",
            "test_get_by_document_id",
            "test_get_by_vector_id",
            "setup_method",
            "test_get_by_user_id",
            "test_get_by_session_id",
            "setup_method",
            "test_get_by_key",
            "test_get_by_category",
            "test_set_config",
            "test_global_instances_exist"
          ],
          "classes": [
            "TestBaseRepository",
            "TestUserRepository",
            "TestDocumentRepository",
            "TestDocumentChunkRepository",
            "TestQueryHistoryRepository",
            "TestSystemConfigRepository",
            "TestRepositoryInstances"
          ]
        },
        "start_interactive_tuner.py": {
          "total_lines": 45,
          "code_lines": 33,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"启动交互式Chunk参数调优工具\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"启动Streamlit应用\"\"\"\n    # 获取交互式调优工具的路径\n    tuner_path = Path(__file__).parent / \"experiments\" / \"chunk_optimization\" / \"interactive_tuner.py\"\n    \n    if not tuner_path.exists():\n        print(f\"❌ 找不到交互式调优工具: {tuner_path}\")\n        sys.exit(1)\n    \n    print(\"🚀 正在启动交互式Chunk参数调优工具...\")\n    print(f\"📁 工具路径: {tuner_path}\")\n    print(\"\\n🌐 浏览器将自动打开，如果没有请手动访问显示的URL\")\n    print(\"⏹️  按 Ct...",
          "imports": [
            "import subprocess",
            "import sys",
            "from pathlib import Path"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "compare_actual_vs_expected.py": {
          "total_lines": 282,
          "code_lines": 227,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n实际代码变更与课程要求对比分析脚本\n\"\"\"\n\nimport json\nimport subprocess\nfrom typing import Dict, List, Any, Tuple\nfrom pathlib import Path\n\ndef load_actual_changes(filename: str = \"branch_analysis_report.json\") -> Dict[str, Any]:\n    \"\"\"\n    加载实际分支变更数据\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"警告: 找不到文件 {filename}\")\n        return {}\n\ndef load_expected_requirements(filename: str ...",
          "imports": [
            "import json",
            "import subprocess",
            "from typing import Dict, List, Any, Tuple",
            "from pathlib import Path"
          ],
          "functions": [
            "load_actual_changes",
            "load_expected_requirements",
            "analyze_lesson_implementation",
            "generate_comparison_report",
            "print_comparison_summary",
            "save_comparison_report",
            "investigate_lesson11_refactor"
          ],
          "classes": []
        },
        "deep_code_investigation.py": {
          "total_lines": 265,
          "code_lines": 210,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n深度代码调查脚本\n详细分析每个有问题lesson分支的实际代码内容和缺失情况\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport difflib\n\nclass DeepCodeInvestigator:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self.investigation_results = {}\n        \n    def get_branch_files(self, branch: str) -> Dict[str, Any]:\n        \"\"\"获取指定分支的所有文件信息\"\"\"\n        try:\n            # 切换到指定分支\n            subprocess.run(['...",
          "imports": [
            "import os",
            "import json",
            "import subprocess",
            "from pathlib import Path",
            "from typing import Dict, List, Any",
            "import difflib"
          ],
          "functions": [
            "__init__",
            "get_branch_files",
            "extract_imports",
            "extract_functions",
            "extract_classes",
            "analyze_lesson_implementation",
            "check_feature_implementation",
            "analyze_code_quality",
            "investigate_problematic_lessons",
            "save_investigation_results",
            "main"
          ],
          "classes": [
            "DeepCodeInvestigator"
          ]
        },
        "test_lesson07.py": {
          "total_lines": 206,
          "code_lines": 163,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nLesson07 功能测试脚本\n测试关键词检索优化的所有功能\n\"\"\"\n\nimport sys\nimport psycopg2\nfrom keyword_search import keyword_search, preprocess_query\nfrom test_jieba import test_segmentation\n\n# 数据库连接配置\nDB_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'rag_user',\n    'password': 'rag_password'\n}\n\ndef test_database_connection():\n    \"\"\"测试数据库连接\"\"\"\n    print(\"📊 测试数据库连接...\")\n    try:\n        conn = psycopg2.connect(**DB_CONFIG)\n   ...",
          "imports": [
            "import sys",
            "import psycopg2",
            "from keyword_search import keyword_search, preprocess_query",
            "from test_jieba import test_segmentation"
          ],
          "functions": [
            "test_database_connection",
            "test_database_schema",
            "test_data_content",
            "test_jieba_segmentation",
            "test_keyword_search_engine",
            "run_all_tests"
          ],
          "classes": []
        },
        "analyze_branches.py": {
          "total_lines": 232,
          "code_lines": 167,
          "content_preview": "#!/usr/bin/env python3\n\nimport subprocess\nimport json\nfrom collections import defaultdict\n\ndef run_git_command(cmd):\n    \"\"\"执行git命令并返回结果\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running command: {cmd}\")\n        print(f\"Error: {e.stderr}\")\n        return None\n\ndef analyze_branch_changes():\n    \"\"\"分析所有lesson分支的增量变更\"\"\"\n    branches...",
          "imports": [
            "import subprocess",
            "import json",
            "from collections import defaultdict"
          ],
          "functions": [
            "run_git_command",
            "analyze_branch_changes",
            "generate_report"
          ],
          "classes": []
        },
        "test_models.py": {
          "total_lines": 261,
          "code_lines": 219,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据模型测试文件\n\n测试所有数据模型的创建、验证和序列化功能\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\nfrom decimal import Decimal\n\nfrom src.models import (\n    User, UserCreate, UserUpdate, UserResponse,\n    Document, DocumentCreate, DocumentUpdate, DocumentResponse,\n    DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate, DocumentChunkResponse,\n    QueryHistory, QueryHistoryCreate, QueryHistoryUpdate, QueryHistoryResponse,\n    Sy...",
          "imports": [
            "import pytest",
            "from datetime import datetime, timezone",
            "from uuid import uuid4",
            "from decimal import Decimal",
            "from src.models import (",
            "from src.models.base import UserRole, DocumentStatus, DocumentType, QueryStatus, QueryType"
          ],
          "functions": [
            "test_user_create_valid",
            "test_user_create_admin",
            "test_user_update",
            "test_user_response",
            "test_document_create",
            "test_document_update",
            "test_document_response",
            "test_chunk_create",
            "test_chunk_update",
            "test_query_create",
            "test_query_update",
            "test_config_create",
            "test_config_update",
            "test_user_email_validation",
            "test_document_file_size_validation",
            "test_chunk_index_validation"
          ],
          "classes": [
            "TestUserModel",
            "TestDocumentModel",
            "TestDocumentChunkModel",
            "TestQueryHistoryModel",
            "TestSystemConfigModel",
            "TestModelValidation"
          ]
        },
        "test_pdf_parser.py": {
          "total_lines": 176,
          "code_lines": 118,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nPDF解析器测试脚本\n\n测试PDF文档解析功能，包括：\n- 文档内容解析\n- 元数据提取\n- 页面提取\n- 错误处理\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nfrom src.document.pdf_parser import PDFParser\nfrom src.document.document_manager import document_manager\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from src.document.pdf_parser import PDFParser",
            "from src.document.document_manager import document_manager"
          ],
          "functions": [
            "test_pdf_parser_basic",
            "test_pdf_parsing",
            "test_document_manager_pdf",
            "test_error_handling",
            "create_test_environment",
            "main"
          ],
          "classes": []
        },
        "main.py": {
          "total_lines": 7,
          "code_lines": 4,
          "content_preview": "def main():\n    print(\"Hello from rag-system!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
          "imports": [],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "test_chunk_system.py": {
          "total_lines": 223,
          "code_lines": 152,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk实验系统功能测试脚本\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# 添加实验目录到Python路径\nexp_dir = Path(__file__).parent / \"experiments\" / \"chunk_optimization\"\nsys.path.append(str(exp_dir))\n\ntry:\n    from chunk_optimizer import ChunkOptimizer, ExperimentResult\n    from experiment_visualizer import ExperimentVisualizer\n    from mock_rag_system import MockRAGSystem, MockDocumentGenerator\nexcept ImportError as e:\n    print(f\"❌ 导入模块失败: {e}\")\n    print(\"请确保所有必要的文件都已创建\")\n    sy...",
          "imports": [
            "import sys",
            "import time",
            "from pathlib import Path",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "test_mock_rag_system",
            "test_chunk_optimizer",
            "test_experiment_visualizer",
            "test_integration",
            "main"
          ],
          "classes": []
        },
        "lesson19/smart_paragraph_chunker_template.py": {
          "total_lines": 405,
          "code_lines": 283,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略模板\n\n这是第19节课的核心实现文件，学生需要基于此模板完成智能段落切分策略。\n本文件提供了完整的实现框架和关键方法的示例代码。\n\n使用方法：\n1. 将此文件复制到 src/chunking/smart_paragraph_chunker.py\n2. 根据注释提示完成TODO部分的实现\n3. 在 src/chunking/__init__.py 中注册策略\n4. 运行测试验证功能\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类（需要确保路径正确）\ntry:\n    from .strategy_interface import ChunkingStrategy, StrategyMetrics\n    from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\nexcept ImportError:\n    # 如果在lesson19目...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import sys",
            "import os",
            "from src.chunking.strategy_interface import ChunkingStrategy, StrategyMetrics",
            "from src.chunking.chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "lesson19/test_smart_paragraph.py": {
          "total_lines": 248,
          "code_lines": 165,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n第19节课 - 智能段落切分策略测试脚本\n\n测试SmartParagraphStrategy的各项功能：\n1. 基本段落切分\n2. 短段落合并\n3. 长段落分割\n4. 插件系统集成\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加src目录到Python路径\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\n# 导入所需模块 - 通过chunking包导入以触发注册\nfrom chunking import SmartParagraphStrategy, ChunkingConfig\nfrom chunking.plugin_registry import registry as StrategyRegistry\n\ndef test_basic_chunking():\n    \"\"\"测试基本段落切分功能\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "from chunking import SmartParagraphStrategy, ChunkingConfig",
            "from chunking.plugin_registry import registry as StrategyRegistry",
            "import traceback"
          ],
          "functions": [
            "test_basic_chunking",
            "test_short_paragraph_merging",
            "test_long_paragraph_splitting",
            "test_plugin_system_integration",
            "test_configuration_options",
            "main"
          ],
          "classes": []
        },
        "tests/test_embedding.py": {
          "total_lines": 223,
          "code_lines": 157,
          "content_preview": "\"\"\"测试向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_basic_embedding():\n    \"\"\"测试基础向量化功能\"\"\"\n    print(\"\\n=== 测试基础向量化功能 ===\")\n    \n    try:\n        # 初始化向量化器\n        embedder = TextEmbedder(model_name=\"BAAI/bge-m3\")\n        \n        # 测试文本\n        test_texts = [\n...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "from src.embedding.embedder import TextEmbedder",
            "import logging"
          ],
          "functions": [
            "test_basic_embedding",
            "test_batch_embedding",
            "test_different_models",
            "test_vector_operations",
            "main"
          ],
          "classes": []
        },
        "tests/test_batch_vectorization.py": {
          "total_lines": 382,
          "code_lines": 267,
          "content_preview": "\"\"\"测试批量向量化功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tempfile\nimport shutil\nimport pytest\nfrom pathlib import Path\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture\ndef test_dir():\n    \"...",
          "imports": [
            "import sys",
            "import os",
            "import tempfile",
            "import shutil",
            "import pytest",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import logging",
            "import json"
          ],
          "functions": [
            "test_dir",
            "create_test_documents",
            "vectorizer",
            "test_document_vectorizer_setup",
            "test_single_document_processing",
            "test_batch_directory_processing",
            "test_document_search",
            "test_collection_stats",
            "test_processing_log"
          ],
          "classes": []
        },
        "tests/test_qdrant.py": {
          "total_lines": 258,
          "code_lines": 188,
          "content_preview": "\"\"\"测试Qdrant向量数据库功能\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport numpy as np\nimport time\nimport pytest\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.embedding.embedder import TextEmbedder\nimport logging\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.fixture(scope=\"module\")\ndef vector_store():\n    \"\"\"创建Qdrant向量存储实例\"\"\"\n    try:\n        store = QdrantVectorStore(\n  ...",
          "imports": [
            "import sys",
            "import os",
            "import numpy as np",
            "import time",
            "import pytest",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.embedding.embedder import TextEmbedder",
            "import logging",
            "import time"
          ],
          "functions": [
            "vector_store",
            "embedder",
            "test_qdrant_connection",
            "test_collection_operations",
            "test_vector_operations",
            "test_vector_search",
            "test_filtered_search",
            "test_performance"
          ],
          "classes": []
        },
        "scripts/verify_environment.py": {
          "total_lines": 93,
          "code_lines": 77,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n环境验证脚本\n验证所有必需的技术组件是否正确安装和配置\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib\nfrom typing import List, Tuple\n\ndef check_python_version() -> Tuple[bool, str]:\n    \"\"\"检查Python版本\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 12:\n        return True, f\"Python {version.major}.{version.minor}.{version.micro}\"\n    return False, f\"Python版本过低: {version.major}.{version.minor}.{version.micro}\"\n\ndef check_command(command: str) -> Tuple[bool, str...",
          "imports": [
            "import sys",
            "import subprocess",
            "import importlib",
            "from typing import List, Tuple"
          ],
          "functions": [
            "check_python_version",
            "check_command",
            "check_python_package",
            "main"
          ],
          "classes": []
        },
        "scripts/test_services.py": {
          "total_lines": 238,
          "code_lines": 175,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\nRAG系统服务连接测试脚本\n用于测试FastAPI、PostgreSQL、Redis、Qdrant、MinIO等服务的连接状态\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nimport httpx\nimport psycopg2\nimport redis\nfrom qdrant_client import QdrantClient\nfrom minio import Minio\nfrom src.config import settings\n\nclass ServiceTester:\n    \"\"\"服务测试类\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n    \n    a...",
          "imports": [
            "import asyncio",
            "import sys",
            "import os",
            "from pathlib import Path",
            "import httpx",
            "import psycopg2",
            "import redis",
            "from qdrant_client import QdrantClient",
            "from minio import Minio",
            "from src.config import settings",
            "from qdrant_client.models import Distance, VectorParams",
            "import io"
          ],
          "functions": [
            "__init__",
            "test_postgresql",
            "test_redis",
            "test_qdrant",
            "test_minio"
          ],
          "classes": [
            "ServiceTester"
          ]
        },
        "scripts/optimize_database.py": {
          "total_lines": 602,
          "code_lines": 481,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据库优化脚本\n\n用于数据库性能优化、索引管理和维护任务\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom sqlalchemy import text, inspect\nfrom sqlalchemy.engine import Engine\n\n# 配置日志\nloggin...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from sqlalchemy import text, inspect",
            "from sqlalchemy.engine import Engine",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DatabaseOptimizer"
          ]
        },
        "scripts/migrate_data.py": {
          "total_lines": 369,
          "code_lines": 274,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n数据迁移脚本\n\n用于处理数据库迁移、数据转换和版本升级\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom uuid import uuid4\n\n# 添加项目根目录到Python路径\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom src.config import get_config\nfrom src.database import DatabaseManager, get_async_session\nfrom src.models import (\n    User, Document, DocumentChunk, QueryH...",
          "imports": [
            "import os",
            "import sys",
            "import asyncio",
            "import logging",
            "from typing import List, Dict, Any, Optional",
            "from datetime import datetime, timezone",
            "from pathlib import Path",
            "from uuid import uuid4",
            "from src.config import get_config",
            "from src.database import DatabaseManager, get_async_session",
            "from src.models import (",
            "from src.repositories import (",
            "from src.models import UserCreate",
            "from src.models import DocumentUpdate",
            "from src.models import SystemConfigUpdate",
            "import argparse"
          ],
          "functions": [
            "__init__"
          ],
          "classes": [
            "DataMigrator"
          ]
        },
        "scripts/start_dev.py": {
          "total_lines": 84,
          "code_lines": 67,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n开发环境启动脚本\n用于启动RAG系统的开发服务器\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# 添加项目根目录到Python路径\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"src\"))\n\ntry:\n    import uvicorn\n    from src.config import settings, validate_config\nexcept ImportError as e:\n    print(f\"导入错误: {e}\")\n    print(\"请确保已安装所有依赖: pip install fastapi uvicorn pydantic-settings\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"主函数\"\"\"\n    prin...",
          "imports": [
            "import sys",
            "import os",
            "from pathlib import Path",
            "import uvicorn",
            "from src.config import settings, validate_config",
            "import socket"
          ],
          "functions": [
            "main"
          ],
          "classes": []
        },
        "alembic/env.py": {
          "total_lines": 155,
          "code_lines": 100,
          "content_preview": "\"\"\"Alembic环境配置\"\"\"\nimport asyncio\nfrom logging.config import fileConfig\nfrom typing import Any, Dict\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlmodel import SQLModel\n\n# 导入所有模型以确保它们被注册到SQLModel.metadata\nfrom src.models import *  # noqa: F403, F401\nfrom src.database.config import db_config\n\n# this is the Alembic Config object, which provides\n# access to the values within...",
          "imports": [
            "import asyncio",
            "from logging.config import fileConfig",
            "from typing import Any, Dict",
            "from alembic import context",
            "from sqlalchemy import engine_from_config, pool",
            "from sqlalchemy.engine import Connection",
            "from sqlalchemy.ext.asyncio import AsyncEngine",
            "from sqlmodel import SQLModel",
            "from src.models import *  # noqa: F403, F401",
            "from src.database.config import db_config"
          ],
          "functions": [
            "get_url",
            "run_migrations_offline",
            "do_run_migrations",
            "include_object",
            "render_item",
            "run_migrations_online"
          ],
          "classes": []
        },
        "src/config.py": {
          "total_lines": 177,
          "code_lines": 122,
          "content_preview": "from pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\nfrom pathlib import Path\n\n# 获取项目根目录\nPROJECT_ROOT = Path(__file__).parent.parent\n\nclass Settings(BaseSettings):\n    \"\"\"应用配置类\"\"\"\n    \n    # 应用基础配置\n    app_name: str = \"RAG System\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    \n    # 服务器配置\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n    \n    # API配置\n    api_prefix: str = \"/api/v1\"\n    \n    # 数据库配置\n    database_url: str = \"postgre...",
          "imports": [
            "from pydantic_settings import BaseSettings",
            "from typing import Optional",
            "import os",
            "from pathlib import Path"
          ],
          "functions": [
            "get_settings",
            "validate_config",
            "get_config_info",
            "get_database_config"
          ],
          "classes": [
            "Settings(BaseSettings)",
            "Config"
          ]
        },
        "src/__init__.py": {
          "total_lines": 43,
          "code_lines": 31,
          "content_preview": "\"\"\"RAG系统核心模块\n\n统一的RAG系统入口，包含所有核心功能模块\n\"\"\"\n\n# 核心模块\nfrom . import api\nfrom . import chunking\nfrom . import database\nfrom . import document\nfrom . import embedding\nfrom . import rag\nfrom . import repositories\nfrom . import rerank\nfrom . import vector_store\n\n# 实验和优化模块\nfrom . import chunk_experiment\n\n# 增量更新模块\nfrom . import incremental\n\n# 数据连接器模块\nfrom . import data_connectors\n\n# 配置\nfrom .config import Config\n\n__all__ = [\n    'api',\n    'chunking',\n    'database',\n    'document',\n    'embedding',\n    'ra...",
          "imports": [
            "from . import api",
            "from . import chunking",
            "from . import database",
            "from . import document",
            "from . import embedding",
            "from . import rag",
            "from . import repositories",
            "from . import rerank",
            "from . import vector_store",
            "from . import chunk_experiment",
            "from . import incremental",
            "from . import data_connectors",
            "from .config import Config"
          ],
          "functions": [],
          "classes": []
        },
        "src/main.py": {
          "total_lines": 76,
          "code_lines": 61,
          "content_preview": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nimport uvicorn\n\n# 创建FastAPI应用实例\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"一个基于FastAPI的RAG（检索增强生成）系统\",\n    version=\"1.0.0\"\n)\n\n# 配置CORS中间件\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 在生产环境中应该设置具体的域名\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 定义响应模型\nclass HealthResponse(BaseModel):...",
          "imports": [
            "from fastapi import FastAPI",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from pydantic import BaseModel",
            "from typing import Dict, Any",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)",
            "InfoResponse(BaseModel)"
          ]
        },
        "src/database/config.py": {
          "total_lines": 109,
          "code_lines": 84,
          "content_preview": "\"\"\"数据库配置模块\"\"\"\nimport os\nfrom typing import Optional\nfrom sqlalchemy.engine import URL\n\n\nclass DatabaseConfig:\n    \"\"\"数据库配置类\"\"\"\n    \n    def __init__(self):\n        \"\"\"初始化数据库配置\"\"\"\n        # 基础配置\n        self.host = os.getenv(\"DB_HOST\", \"localhost\")\n        self.port = int(os.getenv(\"DB_PORT\", \"5432\"))\n        self.database = os.getenv(\"DB_NAME\", \"rag_system\")\n        self.username = os.getenv(\"DB_USER\", \"postgres\")\n        self.password = os.getenv(\"DB_PASSWORD\", \"postgres\")\n        \n        # 连接...",
          "imports": [
            "import os",
            "from typing import Optional",
            "from sqlalchemy.engine import URL"
          ],
          "functions": [
            "__init__",
            "sync_url",
            "async_url",
            "alembic_url",
            "get_connect_args",
            "get_engine_kwargs",
            "validate"
          ],
          "classes": [
            "DatabaseConfig"
          ]
        },
        "src/database/__init__.py": {
          "total_lines": 44,
          "code_lines": 38,
          "content_preview": "\"\"\"数据库模块\"\"\"\nfrom .config import DatabaseConfig, db_config\nfrom .connection import (\n    DatabaseManager,\n    db_manager,\n    get_sync_session,\n    get_async_session,\n    init_database,\n    close_database,\n    check_database_health\n)\nfrom .init_db import (\n    create_database_if_not_exists,\n    create_extensions,\n    create_indexes,\n    create_default_admin,\n    create_default_configs,\n    init_database as init_db,\n    reset_database\n)\n\n__all__ = [\n    # 配置\n    \"DatabaseConfig\",\n    \"db_config\",\n...",
          "imports": [
            "from .config import DatabaseConfig, db_config",
            "from .connection import (",
            "from .init_db import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/database/connection.py": {
          "total_lines": 217,
          "code_lines": 171,
          "content_preview": "\"\"\"数据库连接管理模块\"\"\"\nimport asyncio\nfrom typing import AsyncGenerator, Optional\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import create_engine, Engine, text\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlmodel import SQLModel\nfrom .config import db_config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseManager:\n    \"\"\"数据库管理器\"\"\"\n    \n    def __init__(sel...",
          "imports": [
            "import asyncio",
            "from typing import AsyncGenerator, Optional",
            "from contextlib import asynccontextmanager",
            "from sqlalchemy import create_engine, Engine, text",
            "from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine, AsyncSession, async_sessionmaker",
            "from sqlalchemy.orm import sessionmaker, Session",
            "from sqlmodel import SQLModel",
            "from .config import db_config",
            "import logging"
          ],
          "functions": [
            "__init__",
            "initialize",
            "get_sync_session",
            "sync_engine",
            "async_engine",
            "is_initialized",
            "get_sync_session"
          ],
          "classes": [
            "DatabaseManager"
          ]
        },
        "src/database/init_db.py": {
          "total_lines": 326,
          "code_lines": 241,
          "content_preview": "\"\"\"数据库初始化脚本\"\"\"\nimport asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom .connection import db_manager, get_async_session\nfrom ..models import TABLE_MODELS, User, UserRole, SystemConfig\nfrom ..config import get_settings\nimport logging\n\n# 添加项目根目录到路径\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nlogger = logging.getLogger(__name__)\n\n\nasync def create_database_if_not_exists() -> None:\n    ...",
          "imports": [
            "import asyncio",
            "import sys",
            "from pathlib import Path",
            "from typing import Optional",
            "from sqlalchemy import text",
            "from sqlalchemy.exc import ProgrammingError",
            "from .connection import db_manager, get_async_session",
            "from ..models import TABLE_MODELS, User, UserRole, SystemConfig",
            "from ..config import get_settings",
            "import logging",
            "from .config import db_config",
            "from sqlalchemy.ext.asyncio import create_async_engine",
            "from sqlalchemy import select",
            "from werkzeug.security import generate_password_hash",
            "from sqlalchemy import select",
            "import argparse"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/conflict_resolver.py": {
          "total_lines": 715,
          "code_lines": 551,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n冲突解决器 - ConflictResolver\n\n处理增量更新过程中的各种冲突\n支持多种冲突解决策略\n提供冲突检测和自动解决机制\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple, Callable\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAIL...",
          "imports": [
            "import json",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Tuple, Callable",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from .monitoring import get_monitoring_manager",
            "import uuid",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__post_init__",
            "to_dict",
            "__init__",
            "detect_conflict",
            "resolve_conflict",
            "_perform_conflict_resolution",
            "_resolve_latest_wins",
            "_resolve_manual_review",
            "_resolve_merge_content",
            "_resolve_skip_update",
            "_resolve_force_update",
            "_resolve_rollback",
            "register_custom_handler",
            "get_conflicts",
            "get_conflict_by_id",
            "get_stats",
            "get_runtime_stats",
            "clear_resolved_conflicts",
            "_load_conflicts",
            "_save_conflicts",
            "_load_stats",
            "_update_stats",
            "custom_handler"
          ],
          "classes": [
            "ConflictType(Enum)",
            "ResolutionStrategy(Enum)",
            "ConflictRecord",
            "ConflictStats",
            "ConflictResolver"
          ]
        },
        "src/incremental/config.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "\"\"\"增量更新系统配置\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass IncrementalConfig:\n    \"\"\"增量更新配置类\"\"\"\n    \n    # 基础配置\n    data_directory: str = \"./data\"\n    metadata_directory: str = \"./metadata\"\n    log_level: str = \"INFO\"\n    \n    # 变更检测配置\n    change_detection_enabled: bool = True\n    hash_algorithm: str = \"md5\"\n    file_extensions: list = field(default_factory=lambda: [\".txt\", \".md\", \".pdf\", \".docx...",
          "imports": [
            "import os",
            "from pathlib import Path",
            "from typing import Dict, Any, Optional",
            "from dataclasses import dataclass, field",
            "import json"
          ],
          "functions": [
            "__post_init__",
            "to_dict",
            "from_dict",
            "save_to_file",
            "load_from_file",
            "update",
            "validate",
            "get_config",
            "set_config",
            "reset_config",
            "load_config_from_env",
            "create_config_with_env_override"
          ],
          "classes": [
            "IncrementalConfig"
          ]
        },
        "src/incremental/version_manager.py": {
          "total_lines": 671,
          "code_lines": 491,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n版本管理器 - VersionManager\n\n实现文档版本控制和追踪功能\n支持版本创建、查询、比较和回滚\n提供完整的版本历史管理\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom enum import Enum\n\n\nclass VersionStatus(Enum):\n    \"\"\"版本状态枚举\"\"\"\n    ACTIVE = \"active\"          # 活跃版本\n    ARCHIVED = \"archived\"      # 已归档\n    D...",
          "imports": [
            "import json",
            "import os",
            "import shutil",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Tuple, Any",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from enum import Enum",
            "from datetime import timedelta",
            "import hashlib",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "__str__",
            "to_dict",
            "from_dict",
            "__init__",
            "create_version",
            "get_version",
            "get_version_history",
            "compare_versions",
            "rollback_to_version",
            "archive_version",
            "delete_version",
            "get_document_list",
            "get_stats",
            "cleanup_old_versions",
            "_cleanup_old_versions",
            "_get_version_file_path",
            "_update_stats",
            "_load_versions",
            "_save_versions"
          ],
          "classes": [
            "VersionStatus(Enum)",
            "DocumentVersion",
            "VersionDiff",
            "VersionManager"
          ]
        },
        "src/incremental/monitoring.py": {
          "total_lines": 454,
          "code_lines": 353,
          "content_preview": "\"\"\"增量更新系统监控和日志模块\"\"\"\n\nimport os\nimport sys\nimport time\nimport psutil\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\nimport traceback\nfrom contextlib import contextmanager\n\n@dataclass\nclass MetricData:\n    \"\"\"指标数据\"\"\"\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str] = f...",
          "imports": [
            "import os",
            "import sys",
            "import time",
            "import psutil",
            "import logging",
            "import threading",
            "from pathlib import Path",
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "from dataclasses import dataclass, field",
            "from collections import defaultdict, deque",
            "import json",
            "import traceback",
            "from contextlib import contextmanager"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "record_metric",
            "increment_counter",
            "set_gauge",
            "record_timer",
            "get_metrics",
            "get_summary",
            "__init__",
            "start_monitoring",
            "stop_monitoring",
            "_monitor_loop",
            "_collect_system_metrics",
            "_check_thresholds",
            "get_current_metrics",
            "get_metrics_history",
            "__init__",
            "handle_error",
            "get_error_summary",
            "get_error_rate",
            "__init__",
            "_create_logger",
            "log_change_detection",
            "log_version_management",
            "log_incremental_indexing",
            "log_conflict_resolution",
            "log_api_request",
            "log_main",
            "__init__",
            "__del__",
            "timer",
            "log_operation",
            "handle_error",
            "get_system_health",
            "export_logs",
            "get_monitoring_manager",
            "setup_monitoring"
          ],
          "classes": [
            "MetricData",
            "PerformanceMetrics",
            "MetricsCollector",
            "PerformanceMonitor",
            "ErrorHandler",
            "IncrementalUpdateLogger",
            "MonitoringManager"
          ]
        },
        "src/incremental/__init__.py": {
          "total_lines": 24,
          "code_lines": 21,
          "content_preview": "\"\"\"增量更新模块\n\n提供增量索引更新、变更检测、冲突解决等功能\n\"\"\"\n\nfrom .indexer import IncrementalIndexer, IndexEntry, IndexStats\nfrom .change_detector import ChangeDetector\nfrom .conflict_resolver import ConflictResolver\nfrom .version_manager import VersionManager\nfrom .monitoring import get_monitoring_manager\nfrom .config import IncrementalConfig\nfrom .integration import IncrementalIntegration\n\n__all__ = [\n    'IncrementalIndexer',\n    'IndexEntry', \n    'IndexStats',\n    'ChangeDetector',\n    'ConflictResolver',\n    'Ve...",
          "imports": [
            "from .indexer import IncrementalIndexer, IndexEntry, IndexStats",
            "from .change_detector import ChangeDetector",
            "from .conflict_resolver import ConflictResolver",
            "from .version_manager import VersionManager",
            "from .monitoring import get_monitoring_manager",
            "from .config import IncrementalConfig",
            "from .integration import IncrementalIntegration"
          ],
          "functions": [],
          "classes": []
        },
        "src/incremental/integration.py": {
          "total_lines": 452,
          "code_lines": 334,
          "content_preview": "\"\"\"增量更新系统与RAG系统集成模块\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime\nfrom config import get_config, IncrementalConfig\n\n# 添加父目录到Python路径，以便导入RAG系统模块\nsys.path.append(str(Path(__file__).parent.parent))\n\ntry:\n    from src.config import get_settings\n    from src.database.connection import get_database_session\n    from src.embedding.embedder import TextEmbedder\n    from src.vector_store.qdrant_client impo...",
          "imports": [
            "import os",
            "import sys",
            "import logging",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Any, Tuple",
            "from datetime import datetime",
            "from config import get_config, IncrementalConfig",
            "from src.config import get_settings",
            "from src.database.connection import get_database_session",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.document.document_manager import DocumentManager",
            "from .change_detector import ChangeDetector",
            "from .version_manager import VersionManager",
            "from .incremental_indexer import IncrementalIndexer",
            "from .conflict_resolver import ConflictResolver",
            "from .monitoring import get_monitoring_manager",
            "import asyncio"
          ],
          "functions": [
            "__init__",
            "_setup_logging",
            "_initialize_rag_components",
            "get_system_status",
            "get_integration_stats",
            "get_integration_instance"
          ],
          "classes": [
            "RAGIncrementalIntegration"
          ]
        },
        "src/incremental/indexer.py": {
          "total_lines": 544,
          "code_lines": 416,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n增量索引器 - IncrementalIndexer\n\n实现高效的增量索引更新功能\n只处理变更文档，避免全量重建\n支持批量处理和并发更新\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport json\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AV...",
          "imports": [
            "import json",
            "import logging",
            "import asyncio",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Any, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import hashlib"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "__init__",
            "process_changes",
            "_perform_change_processing",
            "_process_batch",
            "_process_single_document",
            "_load_index",
            "_load_stats",
            "_save_index",
            "_update_stats",
            "_remove_document",
            "_chunk_document",
            "get_stats",
            "search_similar"
          ],
          "classes": [
            "IndexEntry",
            "IndexStats",
            "IncrementalIndexer"
          ]
        },
        "src/incremental/change_detector.py": {
          "total_lines": 634,
          "code_lines": 465,
          "content_preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n变更检测器 - ChangeDetector\n\n实现基于MD5哈希的文件变更检测功能\n支持文件添加、修改、删除的检测\n提供高效的批量检测能力\n\n作者: RAG系统开发团队\n日期: 2024-01-15\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\n# 尝试导入监控模块\ntry:\n    from .monitoring import get_monitoring_manager\n    MONITORING_AVAILABLE = True\nexcept ImportError:\n    MONITORING_AVAILAB...",
          "imports": [
            "import hashlib",
            "import json",
            "import os",
            "import logging",
            "from datetime import datetime",
            "from typing import Dict, List, Optional, Set, Tuple",
            "from dataclasses import dataclass, asdict",
            "from pathlib import Path",
            "from .monitoring import get_monitoring_manager",
            "import time",
            "import time",
            "from datetime import timedelta",
            "import tempfile",
            "import shutil"
          ],
          "functions": [
            "to_dict",
            "from_dict",
            "to_dict",
            "from_dict",
            "__init__",
            "calculate_file_hash",
            "get_file_info",
            "detect_changes",
            "_perform_change_detection",
            "get_file_metadata",
            "get_change_history",
            "get_stats",
            "cleanup_old_changes",
            "_load_metadata",
            "_save_metadata",
            "_load_change_history",
            "_save_change_history"
          ],
          "classes": [
            "FileMetadata",
            "ChangeRecord",
            "ChangeDetector"
          ]
        },
        "src/data_connectors/database_connector.py": {
          "total_lines": 395,
          "code_lines": 314,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData, inspect\nfrom sqlalchemy.exc import SQLAlchemyError\nimport pandas as pd\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"\n    数据库连接器\n    支持MySQL、PostgreSQL等关系型数据库\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据库连接器\n        \n     ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "from sqlalchemy import create_engine, text, MetaData, inspect",
            "from sqlalchemy.exc import SQLAlchemyError",
            "import pandas as pd",
            "from data_connector import DataConnector"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "execute_custom_query"
          ],
          "classes": [
            "DatabaseConnector(DataConnector)"
          ]
        },
        "src/data_connectors/__init__.py": {
          "total_lines": 16,
          "code_lines": 13,
          "content_preview": "\"\"\"数据连接器模块\n\n提供统一的数据源连接接口，支持API、数据库等多种数据源\n\"\"\"\n\nfrom .base import DataConnector\nfrom .api_connector import APIConnector\nfrom .database_connector import DatabaseConnector\nfrom .sync_manager import SyncManager\n\n__all__ = [\n    'DataConnector',\n    'APIConnector',\n    'DatabaseConnector',\n    'SyncManager'\n]",
          "imports": [
            "from .base import DataConnector",
            "from .api_connector import APIConnector",
            "from .database_connector import DatabaseConnector",
            "from .sync_manager import SyncManager"
          ],
          "functions": [],
          "classes": []
        },
        "src/data_connectors/sync_manager.py": {
          "total_lines": 867,
          "code_lines": 667,
          "content_preview": "from typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\n\nfrom data_connector import DataConnector\nfrom database_connector import DatabaseConnector\nfrom api_connector import APIConnector\n\nlogger = logging.getLogger(__name__)\n\nclass SyncType(Enum):\n    \"\"\"同步类型枚举\"\"\"\n    FULL = \"full\"\n    INCREMENTAL = \"incremental\"\n\nclass SyncStatus(En...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Callable",
            "from datetime import datetime, timedelta",
            "import logging",
            "import json",
            "import asyncio",
            "from enum import Enum",
            "from dataclasses import dataclass, asdict",
            "import pandas as pd",
            "from data_connector import DataConnector",
            "from database_connector import DatabaseConnector",
            "from api_connector import APIConnector"
          ],
          "functions": [
            "to_dict",
            "__init__",
            "transform_record",
            "_apply_filters",
            "_apply_field_mappings",
            "_apply_data_type_conversions",
            "_apply_custom_transformations",
            "__init__",
            "_initialize_connectors",
            "_initialize_transformers",
            "add_sync_callback",
            "start_full_sync",
            "start_incremental_sync",
            "_notify_callbacks",
            "get_sync_status",
            "get_all_sync_status",
            "cancel_sync",
            "cleanup_history",
            "get_sync_history",
            "cleanup_old_history",
            "add_connector",
            "remove_connector",
            "get_connector_info",
            "list_connectors",
            "add_transformer",
            "remove_transformer",
            "get_transformer_info",
            "list_transformers"
          ],
          "classes": [
            "SyncType(Enum)",
            "SyncStatus(Enum)",
            "SyncResult",
            "DataTransformer",
            "SyncManager"
          ]
        },
        "src/data_connectors/api_connector.py": {
          "total_lines": 584,
          "code_lines": 448,
          "content_preview": "from typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\nimport requests\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\n\nfrom data_connector import DataConnector\n\nlogger = logging.getLogger(__name__)\n\nclass APIConnector(DataConnector):\n    \"\"\"\n    REST API连接器\n    支持从REST API获取结构化数据\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化API连接器\n        \n        Args:\n            config: API配置参数\n            ...",
          "imports": [
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging",
            "import requests",
            "import time",
            "import json",
            "from urllib.parse import urljoin, urlparse",
            "from data_connector import DataConnector",
            "from urllib.parse import parse_qs"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "get_required_config_fields",
            "validate_config",
            "_apply_rate_limit",
            "_extract_records",
            "make_request",
            "make_custom_request"
          ],
          "classes": [
            "APIConnector(DataConnector)"
          ]
        },
        "src/data_connectors/base.py": {
          "total_lines": 169,
          "code_lines": 136,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Iterator\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataConnector(ABC):\n    \"\"\"\n    数据连接器基类\n    定义了所有数据连接器必须实现的抽象接口\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        初始化数据连接器\n        \n        Args:\n            config: 连接器配置参数\n        \"\"\"\n        self.config = config\n        self.connection = None\n        self.is_connected = False\n        ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, List, Any, Optional, Iterator",
            "from datetime import datetime",
            "import logging"
          ],
          "functions": [
            "__init__",
            "connect",
            "disconnect",
            "test_connection",
            "get_schema",
            "fetch_data",
            "fetch_incremental_data",
            "get_total_count",
            "validate_config",
            "get_required_config_fields",
            "get_connection_info",
            "update_last_sync_time",
            "__enter__",
            "__exit__"
          ],
          "classes": [
            "DataConnector(ABC)"
          ]
        },
        "src/chunk_experiment/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py": {
          "total_lines": 739,
          "code_lines": 553,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"基于Streamlit的交互式Chunk参数调优工具\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nimport time\nfrom pathlib import Path\nimport sys\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSy...",
          "imports": [
            "import streamlit as st",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "initialize_system",
            "run_single_experiment",
            "run_grid_search",
            "main"
          ],
          "classes": [
            "InteractiveChunkTuner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py": {
          "total_lines": 303,
          "code_lines": 205,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk参数优化实验主脚本\"\"\"\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sys\nfrom typing import Dict, List, Optional\n\n# 添加当前目录到Python路径\nsys.path.append(str(Path(__file__).parent))\n\nfrom chunk_optimizer import ChunkOptimizer, ExperimentResult\nfrom experiment_visualizer import ExperimentVisualizer\nfrom mock_rag_system import MockRAGSystem, MockDocumentGenerator\n\nclass ChunkExperimentRunner:\n    \"\"\"Chunk实验运行器\"\"\"\n    \n    def __init__(self, config: Dict):\n...",
          "imports": [
            "import argparse",
            "import json",
            "import time",
            "from pathlib import Path",
            "import sys",
            "from typing import Dict, List, Optional",
            "from chunk_optimizer import ChunkOptimizer, ExperimentResult",
            "from experiment_visualizer import ExperimentVisualizer",
            "from mock_rag_system import MockRAGSystem, MockDocumentGenerator"
          ],
          "functions": [
            "__init__",
            "setup_system",
            "run_grid_search",
            "analyze_results",
            "save_results",
            "generate_visualizations",
            "run_experiment",
            "load_config",
            "create_sample_config",
            "main"
          ],
          "classes": [
            "ChunkExperimentRunner"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py": {
          "total_lines": 412,
          "code_lines": 325,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"实验结果可视化分析器\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom chunk_optimizer import ExperimentResult\n\nclass ExperimentVisualizer:\n    \"\"\"实验结果可视化器\"\"\"\n    \n    def __init__(self, results: List[ExperimentResult]):\n        self.results = results\n        self.df ...",
          "imports": [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import pandas as pd",
            "import numpy as np",
            "from typing import List, Dict, Any",
            "from pathlib import Path",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "from plotly.subplots import make_subplots",
            "from chunk_optimizer import ExperimentResult",
            "import json"
          ],
          "functions": [
            "__init__",
            "_create_dataframe",
            "create_heatmap",
            "create_performance_curves",
            "create_3d_surface_plot",
            "create_comparison_radar_chart",
            "create_correlation_matrix",
            "create_pareto_frontier",
            "generate_summary_report",
            "_get_metric_label",
            "create_interactive_dashboard"
          ],
          "classes": [
            "ExperimentVisualizer"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/mock_rag_system.py": {
          "total_lines": 406,
          "code_lines": 293,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"模拟RAG系统用于Chunk参数测试\"\"\"\n\nimport time\nimport random\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\n\n@dataclass\nclass MockChunk:\n    \"\"\"模拟文档块\"\"\"\n    chunk_id: str\n    content: str\n    source_doc: str\n    start_pos: int\n    end_pos: int\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass MockSearchResult:\n    \"\"\"模拟搜索结果\"\"\"\n    chunk_id: str\n    content: str\n    score...",
          "imports": [
            "import time",
            "import random",
            "import hashlib",
            "import numpy as np",
            "from typing import List, Dict, Any, Optional",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import re"
          ],
          "functions": [
            "get",
            "__init__",
            "set_params",
            "chunk_text",
            "_generate_mock_embedding",
            "__init__",
            "add_chunks",
            "search",
            "_cosine_similarity",
            "__init__",
            "set_chunk_params",
            "add_document",
            "process_document",
            "process_all_documents",
            "search",
            "get_chunk_statistics",
            "evaluate_retrieval",
            "get_statistics",
            "generate_test_documents",
            "generate_test_queries"
          ],
          "classes": [
            "MockChunk",
            "MockSearchResult",
            "MockChunkManager",
            "MockVectorStore",
            "MockRAGSystem",
            "MockDocumentGenerator"
          ]
        },
        "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py": {
          "total_lines": 249,
          "code_lines": 184,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"Chunk分块参数优化器\"\"\"\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"实验结果数据类\"\"\"\n    chunk_size: int\n    overlap_ratio: float\n    avg_chunk_length: float\n    total_chunks: int\n    retrieval_accuracy: float\n    retrie...",
          "imports": [
            "import time",
            "import json",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from typing import List, Dict, Any, Tuple",
            "from dataclasses import dataclass",
            "from pathlib import Path",
            "import numpy as np",
            "from concurrent.futures import ThreadPoolExecutor"
          ],
          "functions": [
            "__init__",
            "run_grid_search",
            "_run_single_experiment",
            "_reconfigure_chunking",
            "_reprocess_documents",
            "_evaluate_retrieval",
            "_calculate_storage_overhead",
            "get_best_parameters",
            "save_results",
            "load_results",
            "run_parallel_experiments"
          ],
          "classes": [
            "ExperimentResult",
            "ChunkOptimizer"
          ]
        },
        "src/embedding/__init__.py": {
          "total_lines": 5,
          "code_lines": 3,
          "content_preview": "\"\"\"Embedding模块\"\"\"\n\nfrom .embedder import TextEmbedder\n\n__all__ = ['TextEmbedder']",
          "imports": [
            "from .embedder import TextEmbedder"
          ],
          "functions": [],
          "classes": []
        },
        "src/embedding/embedder.py": {
          "total_lines": 354,
          "code_lines": 267,
          "content_preview": "\"\"\"文本向量化模块\"\"\"\n\nimport os\nimport json\nimport pickle\nfrom typing import List, Dict, Any, Optional, Union\nimport numpy as np\nfrom pathlib import Path\n\n# 简化版本，使用基础的向量化实现\nimport hashlib\nimport re\nfrom collections import Counter\nimport math\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass TextEmbedder:\n    \"\"\"文本向量化器 - 简化版本使用TF-IDF\"\"\"\n    \n    def __init__(self, model_name: str = \"tfidf\", device: str = \"cpu\"):\n        \"\"\"\n        初始化文本向量化器\n        \n        Args:\n            model_name: 模型名称 ...",
          "imports": [
            "import os",
            "import json",
            "import pickle",
            "from typing import List, Dict, Any, Optional, Union",
            "import numpy as np",
            "from pathlib import Path",
            "import hashlib",
            "import re",
            "from collections import Counter",
            "import math",
            "import logging"
          ],
          "functions": [
            "__init__",
            "_preprocess_text",
            "_build_vocabulary",
            "_text_to_vector",
            "encode",
            "encode_batch",
            "similarity",
            "save_embeddings",
            "load_embeddings",
            "compute_similarity",
            "compute_similarity_matrix",
            "get_vector_dimension",
            "get_model_info"
          ],
          "classes": [
            "TextEmbedder"
          ]
        },
        "src/repositories/user.py": {
          "total_lines": 366,
          "code_lines": 312,
          "content_preview": "\"\"\"用户仓库\"\"\"\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom ..models.user import (\n    User,\n    UserCreate,\n    UserRole,\n    UserStatus,\n    UserUpdate\n)\nfrom .base import BaseRepository\n\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):...",
          "imports": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from werkzeug.security import check_password_hash, generate_password_hash",
            "from ..models.user import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_username",
            "get_by_email",
            "get_by_username_or_email",
            "authenticate",
            "create_user",
            "update_password",
            "update_last_login",
            "activate_user",
            "deactivate_user",
            "get_active_users",
            "get_users_by_role",
            "search_users",
            "get_password_hash",
            "verify_password",
            "is_active",
            "is_admin",
            "can_manage_users"
          ],
          "classes": [
            "UserRepository(BaseRepository[User, UserCreate, UserUpdate])"
          ]
        },
        "src/repositories/query.py": {
          "total_lines": 597,
          "code_lines": 506,
          "content_preview": "\"\"\"查询仓库\"\"\"\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\n\nfrom ..models.query import (\n    QueryHistory,\n    QueryHistoryCreate,\n    QueryHistoryUpdate,\n    QueryStatus,\n    QueryType,\n    SystemConfig,\n    SystemConfigCreate,\n    SystemConfigUpdate\n)\nfrom .base import BaseRepository\n\n\nclass QueryHistoryR...",
          "imports": [
            "from datetime import datetime, timedelta",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from ..models.query import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_user",
            "get_by_session",
            "get_by_status",
            "get_by_type",
            "search_queries",
            "get_recent_queries",
            "get_popular_queries",
            "get_failed_queries",
            "update_response",
            "get_query_stats",
            "__init__",
            "get_by_key",
            "get_by_category",
            "get_public_configs",
            "get_private_configs",
            "search_configs",
            "set_config",
            "get_config_value",
            "delete_config",
            "get_config_categories",
            "get_configs_dict"
          ],
          "classes": [
            "QueryHistoryRepository(BaseRepository[QueryHistory, QueryHistoryCreate, QueryHistoryUpdate])",
            "SystemConfigRepository(BaseRepository[SystemConfig, SystemConfigCreate, SystemConfigUpdate])"
          ]
        },
        "src/repositories/__init__.py": {
          "total_lines": 53,
          "code_lines": 35,
          "content_preview": "\"\"\"仓库模块\"\"\"\n\n# 基础仓库\nfrom .base import BaseRepository\n\n# 用户仓库\nfrom .user import UserRepository, user_repository\n\n# 文档仓库\nfrom .document import (\n    DocumentRepository,\n    DocumentChunkRepository,\n    document_repository,\n    document_chunk_repository\n)\n\n# 查询仓库\nfrom .query import (\n    QueryHistoryRepository,\n    SystemConfigRepository,\n    query_history_repository,\n    system_config_repository\n)\n\n__all__ = [\n    # 基础仓库类\n    \"BaseRepository\",\n    \n    # 用户仓库\n    \"UserRepository\",\n    \"user_reposit...",
          "imports": [
            "from .base import BaseRepository",
            "from .user import UserRepository, user_repository",
            "from .document import (",
            "from .query import ("
          ],
          "functions": [],
          "classes": []
        },
        "src/repositories/document.py": {
          "total_lines": 477,
          "code_lines": 401,
          "content_preview": "\"\"\"文档仓库\"\"\"\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, desc, func, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session, selectinload\n\nfrom ..models.document import (\n    Document,\n    DocumentChunk,\n    DocumentChunkCreate,\n    DocumentChunkUpdate,\n    DocumentCreate,\n    DocumentStatus,\n    DocumentType,\n    DocumentUpdate,\n    ProcessingStatus\n)\nfrom .base import BaseReposit...",
          "imports": [
            "from datetime import datetime",
            "from typing import Dict, List, Optional",
            "from uuid import UUID",
            "from sqlalchemy import and_, desc, func, or_, select",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session, selectinload",
            "from ..models.document import (",
            "from .base import BaseRepository"
          ],
          "functions": [
            "__init__",
            "get_by_title",
            "get_by_hash",
            "get_by_owner",
            "get_by_status",
            "get_by_type",
            "search_documents",
            "get_processing_documents",
            "get_failed_documents",
            "update_processing_status",
            "get_document_stats",
            "__init__",
            "get_by_document",
            "get_by_vector_id",
            "get_chunk_by_index",
            "search_chunks",
            "get_chunks_with_vectors",
            "get_chunks_without_vectors",
            "update_vector_id",
            "delete_by_document",
            "get_chunk_stats"
          ],
          "classes": [
            "DocumentRepository(BaseRepository[Document, DocumentCreate, DocumentUpdate])",
            "DocumentChunkRepository(BaseRepository[DocumentChunk, DocumentChunkCreate, DocumentChunkUpdate])"
          ]
        },
        "src/repositories/base.py": {
          "total_lines": 385,
          "code_lines": 313,
          "content_preview": "\"\"\"基础仓库类\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID\n\nfrom sqlalchemy import and_, delete, func, or_, select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlmodel import SQLModel\n\nfrom ..models.base import BaseModel\n\n# 类型变量\nModelType = TypeVar(\"ModelType\", bound=BaseModel)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=SQLModel)\nUpdateSchemaType = ...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union",
            "from uuid import UUID",
            "from sqlalchemy import and_, delete, func, or_, select, update",
            "from sqlalchemy.ext.asyncio import AsyncSession",
            "from sqlalchemy.orm import Session",
            "from sqlmodel import SQLModel",
            "from ..models.base import BaseModel"
          ],
          "functions": [
            "__init__",
            "create",
            "get",
            "get_multi",
            "update",
            "delete",
            "count",
            "exists"
          ],
          "classes": [
            "BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType], ABC)"
          ]
        },
        "src/document/pdf_parser.py": {
          "total_lines": 272,
          "code_lines": 198,
          "content_preview": "import fitz  # PyMuPDF\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass PDFParser(DocumentParser):\n    \"\"\"PDF文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.pdf']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_path: str) -> bo...",
          "imports": [
            "import fitz  # PyMuPDF",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_metadata_from_doc",
            "_parse_pdf_date",
            "extract_pages",
            "get_page_count"
          ],
          "classes": [
            "PDFParser(DocumentParser)"
          ]
        },
        "src/document/chunker.py": {
          "total_lines": 209,
          "code_lines": 148,
          "content_preview": "\"\"\"文本分块器\"\"\"\n\nimport re\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TextChunker:\n    \"\"\"文本分块器\"\"\"\n    \n    def __init__(self, \n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50,\n                 separators: Optional[List[str]] = None):\n        \"\"\"\n        初始化文本分块器\n        \n        Args:\n            chunk_size: 文本块大小（字符数）\n            chunk_overlap: 文本块重叠大小（字符数）\n            separators: 分割符列表，按优先级排序\n        \"\"\"\n        s...",
          "imports": [
            "import re",
            "from typing import List, Optional",
            "import logging"
          ],
          "functions": [
            "__init__",
            "chunk_text",
            "_clean_text",
            "_split_text_recursive",
            "_add_overlap",
            "get_chunk_info"
          ],
          "classes": [
            "TextChunker"
          ]
        },
        "src/document/docx_parser.py": {
          "total_lines": 303,
          "code_lines": 221,
          "content_preview": "from docx import Document\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass DocxParser(DocumentParser):\n    \"\"\"Word文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.docx', '.doc']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "from docx import Document",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_extract_text",
            "_extract_table_text",
            "_extract_metadata_from_doc",
            "_estimate_page_count",
            "extract_paragraphs",
            "extract_tables",
            "get_paragraph_count"
          ],
          "classes": [
            "DocxParser(DocumentParser)"
          ]
        },
        "src/document/__init__.py": {
          "total_lines": 23,
          "code_lines": 20,
          "content_preview": "\"\"\"文档解析模块\n\n提供各种文档格式的解析功能，包括PDF、Word、文本等格式的解析器。\n\"\"\"\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\nfrom .document_manager import DocumentManager, document_manager\nfrom .chunker import TextChunker\n\n__all__ = [\n    'DocumentParser',\n    'ParsedDocument', \n    'DocumentMetadata',\n    'PDFParser',\n    'DocxParser',\n    'TxtParser',\n    'DocumentManager',\n    'document_manager...",
          "imports": [
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser",
            "from .document_manager import DocumentManager, document_manager",
            "from .chunker import TextChunker"
          ],
          "functions": [],
          "classes": []
        },
        "src/document/parser.py": {
          "total_lines": 186,
          "code_lines": 146,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# 配置日志\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DocumentMetadata:\n    \"\"\"文档元数据类\"\"\"\n    title: Optional[str] = None\n    author: Optional[str] = None\n    creation_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    page_count: Optional[int] = None\n    file_size: Option...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import Dict, Any, Optional, List",
            "from pathlib import Path",
            "import logging",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "to_dict",
            "to_dict",
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "validate_file",
            "get_file_info",
            "clean_text",
            "detect_language"
          ],
          "classes": [
            "DocumentMetadata",
            "ParsedDocument",
            "DocumentParser(ABC)"
          ]
        },
        "src/document/txt_parser.py": {
          "total_lines": 306,
          "code_lines": 216,
          "content_preview": "import chardet\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, DocumentMetadata, ParsedDocument\n\nlogger = logging.getLogger(__name__)\n\nclass TxtParser(DocumentParser):\n    \"\"\"文本文档解析器\"\"\"\n    \n    SUPPORTED_EXTENSIONS = ['.txt', '.md', '.rst', '.log']\n    \n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def can_parse(self, file_pa...",
          "imports": [
            "import chardet",
            "from typing import List, Optional",
            "from datetime import datetime",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, DocumentMetadata, ParsedDocument"
          ],
          "functions": [
            "__init__",
            "can_parse",
            "parse",
            "extract_metadata",
            "_detect_encoding",
            "_extract_metadata_from_content",
            "extract_lines",
            "get_line_count",
            "get_word_count",
            "extract_paragraphs"
          ],
          "classes": [
            "TxtParser(DocumentParser)"
          ]
        },
        "src/document/document_manager.py": {
          "total_lines": 308,
          "code_lines": 231,
          "content_preview": "from typing import Dict, List, Optional, Type, Union\nfrom pathlib import Path\nimport logging\n\nfrom .parser import DocumentParser, ParsedDocument, DocumentMetadata\nfrom .pdf_parser import PDFParser\nfrom .docx_parser import DocxParser\nfrom .txt_parser import TxtParser\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentManager:\n    \"\"\"文档解析管理器\n    \n    统一管理所有类型的文档解析器，提供统一的文档解析接口\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._pars...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Union",
            "from pathlib import Path",
            "import logging",
            "from .parser import DocumentParser, ParsedDocument, DocumentMetadata",
            "from .pdf_parser import PDFParser",
            "from .docx_parser import DocxParser",
            "from .txt_parser import TxtParser"
          ],
          "functions": [
            "__init__",
            "_register_default_parsers",
            "register_parser",
            "get_parser",
            "can_parse",
            "parse_document",
            "extract_metadata",
            "parse_batch",
            "get_supported_extensions",
            "get_parser_info",
            "validate_files",
            "find_documents"
          ],
          "classes": [
            "DocumentManager"
          ]
        },
        "src/rag/rag_service.py": {
          "total_lines": 347,
          "code_lines": 270,
          "content_preview": "\"\"\"RAG服务模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport time\nfrom dataclasses import dataclass, asdict\n\nfrom .retriever import DocumentRetriever\nfrom .qa_generator import QAGenerator, QAResponse\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGRequest:\n    \"\"\"RAG请求\"\"\"\n    question: str\n    collection_name: str = \"documents\"\n    top_k: int = 5\n    score_thre...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import time",
            "from dataclasses import dataclass, asdict",
            "from .retriever import DocumentRetriever",
            "from .qa_generator import QAGenerator, QAResponse",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "__init__",
            "query_sync",
            "batch_query",
            "get_collection_stats",
            "validate_query",
            "get_system_status",
            "to_dict"
          ],
          "classes": [
            "RAGRequest",
            "RAGResponse",
            "RAGService"
          ]
        },
        "src/rag/retriever.py": {
          "total_lines": 194,
          "code_lines": 149,
          "content_preview": "\"\"\"文档检索器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore, SearchResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RetrievalResult:\n    \"\"\"检索结果\"\"\"\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str\n    chunk_index: int = 0\n\nclass DocumentRetriever:\n    \"\"\"文档检索器\n    \n    负责从向量数据库...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import numpy as np",
            "from dataclasses import dataclass",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore, SearchResult"
          ],
          "functions": [
            "__init__",
            "retrieve",
            "retrieve_with_rerank",
            "get_collection_stats",
            "format_context"
          ],
          "classes": [
            "RetrievalResult",
            "DocumentRetriever"
          ]
        },
        "src/rag/__init__.py": {
          "total_lines": 11,
          "code_lines": 9,
          "content_preview": "\"\"\"RAG系统核心模块\"\"\"\n\nfrom .rag_service import RAGService\nfrom .qa_generator import QAGenerator\nfrom .retriever import DocumentRetriever\n\n__all__ = [\n    \"RAGService\",\n    \"QAGenerator\", \n    \"DocumentRetriever\"\n]",
          "imports": [
            "from .rag_service import RAGService",
            "from .qa_generator import QAGenerator",
            "from .retriever import DocumentRetriever"
          ],
          "functions": [],
          "classes": []
        },
        "src/rag/qa_generator.py": {
          "total_lines": 306,
          "code_lines": 225,
          "content_preview": "\"\"\"问答生成器模块\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport time\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QAResponse:\n    \"\"\"问答响应\"\"\"\n    answer: str\n    confidence: float\n    sources: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass QAGenerator:\n    \"\"\"问答生成器\n    \n    基于检索到的上下文生成答案\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"gpt-3.5-turbo\",\n                 tempe...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "import logging",
            "import json",
            "import time",
            "from dataclasses import dataclass",
            "import re"
          ],
          "functions": [
            "__init__",
            "generate_answer",
            "_generate_template_answer",
            "_extract_topic",
            "_calculate_confidence",
            "_extract_sources",
            "generate_followup_questions",
            "validate_answer"
          ],
          "classes": [
            "QAResponse",
            "QAGenerator"
          ]
        },
        "src/vector_store/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"向量存储模块\"\"\"\n\nfrom .qdrant_client import QdrantVectorStore, SearchResult\nfrom .document_vectorizer import DocumentVectorizer\n\n__all__ = ['QdrantVectorStore', 'SearchResult', 'DocumentVectorizer']",
          "imports": [
            "from .qdrant_client import QdrantVectorStore, SearchResult",
            "from .document_vectorizer import DocumentVectorizer"
          ],
          "functions": [],
          "classes": []
        },
        "src/vector_store/document_vectorizer.py": {
          "total_lines": 386,
          "code_lines": 292,
          "content_preview": "\"\"\"文档向量化管理器\"\"\"\n\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport time\n\nfrom ..embedding.embedder import TextEmbedder\nfrom .qdrant_client import QdrantVectorStore\nfrom ..document.document_manager import document_manager\nfrom ..document.chunker import TextChunker\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentVectorizer:\n    \"\"\"文档向量化管理器\"\"\"\n    \n    def __init__(self, \n        ...",
          "imports": [
            "import os",
            "import json",
            "import hashlib",
            "from typing import List, Dict, Any, Optional, Tuple",
            "from pathlib import Path",
            "import logging",
            "from datetime import datetime",
            "import time",
            "from ..embedding.embedder import TextEmbedder",
            "from .qdrant_client import QdrantVectorStore",
            "from ..document.document_manager import document_manager",
            "from ..document.chunker import TextChunker"
          ],
          "functions": [
            "__init__",
            "_ensure_collection_exists",
            "_generate_chunk_id",
            "process_document",
            "batch_process_directory",
            "batch_process_documents",
            "search_documents",
            "get_collection_stats",
            "save_processing_log"
          ],
          "classes": [
            "DocumentVectorizer"
          ]
        },
        "src/vector_store/qdrant_client.py": {
          "total_lines": 340,
          "code_lines": 267,
          "content_preview": "\"\"\"Qdrant向量数据库客户端\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Union\nimport uuid\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, SearchRequest\n)\nfrom qdrant_client.http.exceptions import ResponseHandlingException\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SearchResult:\n    \"\"\"搜索结果\"\"\"\n    id: str\n    score...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union",
            "import uuid",
            "import numpy as np",
            "from qdrant_client import QdrantClient",
            "from qdrant_client.models import (",
            "from qdrant_client.http.exceptions import ResponseHandlingException",
            "import logging",
            "from dataclasses import dataclass"
          ],
          "functions": [
            "__init__",
            "create_collection",
            "insert_vectors",
            "search",
            "get_collection_info",
            "delete_collection",
            "list_collections",
            "count_points"
          ],
          "classes": [
            "SearchResult",
            "QdrantVectorStore"
          ]
        },
        "src/chunking/plugin_registry.py": {
          "total_lines": 214,
          "code_lines": 163,
          "content_preview": "\"\"\"插件注册系统\n\n实现切分策略插件的注册、发现、管理和调用机制。\n这是第19节课插件化架构的核心管理组件。\n\"\"\"\n\nfrom typing import Dict, List, Optional, Type, Any, Callable\nimport logging\nimport inspect\nfrom functools import wraps\nimport threading\n\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass StrategyRegistry:\n    \"\"\"策略注册器\n    \n    单例模式的策略注册和管理系统，支持策略的动态注册、发现和调用。\n    \"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(c...",
          "imports": [
            "from typing import Dict, List, Optional, Type, Any, Callable",
            "import logging",
            "import inspect",
            "from functools import wraps",
            "import threading",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import ChunkingConfig"
          ],
          "functions": [
            "__new__",
            "__init__",
            "register_strategy",
            "get_strategy",
            "get_cached_strategy",
            "list_strategies",
            "get_strategy_info",
            "_get_strategy_parameters",
            "search_strategies"
          ],
          "classes": [
            "StrategyRegistry"
          ]
        },
        "src/chunking/structure_chunker.py": {
          "total_lines": 574,
          "code_lines": 411,
          "content_preview": "import re\nfrom typing import List, Optional, Dict, Any, Tuple, Set\nimport logging\nfrom dataclasses import dataclass\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StructurePattern:\n    \"\"\"结构模式定义\"\"\"\n    name: str\n    pattern: str\n    priority: int\n    chunk_boundary: bool = True  # 是否作为块边界\n    \nclass StructureChunker(DocumentChunker):\n    \"\"\"基于文档结构的分块器\n    \n    根据标题、段落、列表等结构特征进行智能分块\n    \"\"\"\n    \n    def __init__(self, c...",
          "imports": [
            "import re",
            "from typing import List, Optional, Dict, Any, Tuple, Set",
            "import logging",
            "from dataclasses import dataclass",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "_init_structure_patterns",
            "chunk_text",
            "_analyze_document_structure",
            "_match_structure_pattern",
            "_create_structure_based_chunks",
            "_calculate_text_position",
            "_split_long_section",
            "_split_by_paragraphs",
            "_create_structure_chunk",
            "_can_merge_with_previous",
            "_merge_with_previous_chunk",
            "_post_process_chunks",
            "_clean_chunk_content",
            "_fallback_paragraph_chunking",
            "analyze_document_structure"
          ],
          "classes": [
            "StructurePattern",
            "StructureChunker(DocumentChunker)"
          ]
        },
        "src/chunking/chunker.py": {
          "total_lines": 346,
          "code_lines": 269,
          "content_preview": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport logging\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"文档块元数据\"\"\"\n    chunk_id: str = \"\"\n    source_file: str = \"\"\n    chunk_index: int = 0\n    start_position: int = 0\n    end_position: int = 0\n    chunk_type: str = \"text\"\n    language: str = \"unknown\"\n    word_count: int = 0\n    char_cou...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "import logging",
            "import hashlib",
            "import re",
            "from langdetect import detect"
          ],
          "functions": [
            "__post_init__",
            "_generate_chunk_id",
            "to_dict",
            "from_dict",
            "__init__",
            "chunk_text",
            "get_chunker_type",
            "chunk_document",
            "_update_chunk_metadata",
            "_post_process_chunks",
            "_normalize_whitespace",
            "_detect_language",
            "_create_chunk",
            "validate_config",
            "get_config_info"
          ],
          "classes": [
            "ChunkMetadata",
            "DocumentChunk",
            "ChunkingConfig",
            "DocumentChunker(ABC)"
          ]
        },
        "src/chunking/chunk_manager.py": {
          "total_lines": 409,
          "code_lines": 311,
          "content_preview": "from typing import List, Dict, Any, Optional, Union, Type\nimport logging\nfrom pathlib import Path\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\n\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"分块管理器\n    \n    统一管理所有分块器，提供统一的分块接口\n    \"\"\"\n    \n    def __init__(self):\n        self.chunkers: Dict[str, DocumentChunker] = {}\n...",
          "imports": [
            "from typing import List, Dict, Any, Optional, Union, Type",
            "import logging",
            "from pathlib import Path",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "import json",
            "import csv",
            "import io"
          ],
          "functions": [
            "__init__",
            "_register_default_chunkers",
            "register_chunker",
            "get_chunker",
            "list_chunkers",
            "chunk_text",
            "chunk_file",
            "batch_chunk_files",
            "compare_chunkers",
            "get_chunker_info",
            "create_chunker",
            "optimize_chunking_strategy",
            "export_chunks"
          ],
          "classes": [
            "ChunkManager"
          ]
        },
        "src/chunking/__init__.py": {
          "total_lines": 37,
          "code_lines": 28,
          "content_preview": "\"\"\"分块器模块\n\n提供多种文档分块策略：\n- 基于句子的分块器\n- 基于语义的分块器  \n- 基于结构的分块器\n- 统一的分块管理器\n\"\"\"\n\nfrom .chunker import (\n    DocumentChunker,\n    DocumentChunk,\n    ChunkMetadata,\n    ChunkingConfig\n)\n\nfrom .sentence_chunker import SentenceChunker\nfrom .semantic_chunker import SemanticChunker\nfrom .structure_chunker import StructureChunker\nfrom .chunk_manager import ChunkManager, chunk_manager\n\n__all__ = [\n    # 基础类\n    'DocumentChunker',\n    'DocumentChunk', \n    'ChunkMetadata',\n    'ChunkingConfig',\n    \n    # 分块器实现\n...",
          "imports": [
            "from .chunker import (",
            "from .sentence_chunker import SentenceChunker",
            "from .semantic_chunker import SemanticChunker",
            "from .structure_chunker import StructureChunker",
            "from .chunk_manager import ChunkManager, chunk_manager"
          ],
          "functions": [],
          "classes": []
        },
        "src/chunking/sentence_chunker.py": {
          "total_lines": 363,
          "code_lines": 257,
          "content_preview": "import re\nfrom typing import List, Optional, Tuple\nimport logging\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceChunker(DocumentChunker):\n    \"\"\"基于句子的文档分块器\n    \n    按照句子边界进行文档分块，保持句子的完整性\n    \"\"\"\n    \n    def __init__(self, config: Optional[ChunkingConfig] = None):\n        super().__init__(config)\n        \n        # 句子分割的正则表达式模式\n        self.sentence_patterns = {\n            'zh': r'[。！？；\\n]+',  # 中文句子结束符\n            'en'...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "import nltk",
            "from nltk.tokenize import sent_tokenize"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_detect_text_language",
            "_split_sentences",
            "_protect_abbreviations",
            "_restore_abbreviations",
            "_combine_sentences_to_chunks",
            "_create_chunk_from_sentences",
            "_get_overlap_sentences",
            "split_by_nltk",
            "_regex_sentence_split",
            "get_sentence_statistics"
          ],
          "classes": [
            "SentenceChunker(DocumentChunker)"
          ]
        },
        "src/chunking/strategy_interface.py": {
          "total_lines": 297,
          "code_lines": 223,
          "content_preview": "\"\"\"切分策略接口定义\n\n定义插件化切分策略的统一接口，支持策略的动态注册和管理。\n这是第19节课插件化架构的核心组件。\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport logging\n\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyMetrics:\n    \"\"\"策略执行指标\"\"\"\n    execution_time: float = 0.0  # 执行时间（秒）\n    chunk_count: int = 0  # 生成的块数量\n    avg_chunk_size: float = 0.0  # 平均块大小\n    min_c...",
          "imports": [
            "from abc import ABC, abstractmethod",
            "from typing import List, Dict, Any, Optional, Union",
            "from dataclasses import dataclass",
            "import time",
            "import logging",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig",
            "import psutil",
            "import os"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "chunk_with_metrics",
            "_calculate_overlap_ratio",
            "_calculate_quality_score",
            "get_strategy_info",
            "validate_config",
            "reset_metrics",
            "get_recommended_config"
          ],
          "classes": [
            "StrategyMetrics",
            "ChunkingStrategy(ABC)",
            "StrategyError(Exception)",
            "StrategyConfigError(Exception)"
          ]
        },
        "src/chunking/semantic_chunker.py": {
          "total_lines": 503,
          "code_lines": 334,
          "content_preview": "import numpy as np\nfrom typing import List, Optional, Tuple, Dict, Any\nimport logging\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport re\n\nfrom .chunker import DocumentChunker, DocumentChunk, ChunkingConfig\nfrom .sentence_chunker import SentenceChunker\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticChunker(DocumentChunker):\n    \"\"\"基于语义的文档分块器\n    \n    使用机器学习方法分析文本语义相似性，进行智能分块\n    \"\"\"\n...",
          "imports": [
            "import numpy as np",
            "from typing import List, Optional, Tuple, Dict, Any",
            "import logging",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans",
            "import re",
            "from .chunker import DocumentChunker, DocumentChunk, ChunkingConfig",
            "from .sentence_chunker import SentenceChunker"
          ],
          "functions": [
            "__init__",
            "get_chunker_type",
            "chunk_text",
            "_extract_sentences",
            "_compute_sentence_vectors",
            "_preprocess_sentence",
            "_group_sentences_by_similarity",
            "_greedy_similarity_grouping",
            "_cluster_based_grouping",
            "_should_use_clustering",
            "_sequential_grouping",
            "_post_process_groups",
            "_create_semantic_chunks",
            "_calculate_coherence_score",
            "analyze_semantic_structure",
            "_calculate_overall_coherence"
          ],
          "classes": [
            "SemanticChunker(DocumentChunker)"
          ]
        },
        "src/chunking/smart_paragraph_chunker.py": {
          "total_lines": 365,
          "code_lines": 260,
          "content_preview": "#!/usr/bin/env python3\n\"\"\"\n智能段落切分策略\n\n这是第19节课的核心实现文件，实现了智能段落切分策略。\n本文件基于插件化架构，提供了完整的段落识别、合并和分割功能。\n\n特点：\n1. 识别段落边界（双换行、列表项等）\n2. 智能合并短段落\n3. 分割过长段落\n4. 保持语义完整性\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\nimport logging\n\n# 导入基础类\nfrom .strategy_interface import ChunkingStrategy, StrategyError\nfrom .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig\n\nlogger = logging.getLogger(__name__)\n\nclass SmartParagraphStrategy(ChunkingStrategy):\n    \"\"\"\n    智能段落切分策略\n    \n    特点：\n    1. 识别段落边界（双换...",
          "imports": [
            "import re",
            "from typing import List, Optional, Tuple",
            "import logging",
            "from .strategy_interface import ChunkingStrategy, StrategyError",
            "from .chunker import DocumentChunk, ChunkMetadata, ChunkingConfig"
          ],
          "functions": [
            "__init__",
            "get_strategy_name",
            "get_strategy_description",
            "chunk_text",
            "_identify_paragraphs",
            "_merge_short_paragraphs",
            "_merge_paragraph_group",
            "_split_long_paragraphs",
            "_split_paragraph_by_sentences",
            "_split_by_length",
            "_create_chunks_from_paragraphs"
          ],
          "classes": [
            "SmartParagraphStrategy(ChunkingStrategy)"
          ]
        },
        "src/api/embedding.py": {
          "total_lines": 369,
          "code_lines": 289,
          "content_preview": "\"\"\"Embedding相关API接口\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, UploadFile, File, Form\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom src.embedding.embedder import TextEmbedder\nfrom src.vector_store.qdrant_client import QdrantVectorStore\nfrom src.vector_store.document_vectorizer import DocumentVectorizer\n\nrouter = APIRouter(prefix=\"/embedding\", tags=[\"emb...",
          "imports": [
            "from fastapi import APIRouter, HTTPException, UploadFile, File, Form",
            "from pydantic import BaseModel, Field",
            "from typing import List, Optional, Dict, Any",
            "from datetime import datetime",
            "import os",
            "import tempfile",
            "import shutil",
            "from pathlib import Path",
            "from src.embedding.embedder import TextEmbedder",
            "from src.vector_store.qdrant_client import QdrantVectorStore",
            "from src.vector_store.document_vectorizer import DocumentVectorizer",
            "import time",
            "import time",
            "import time",
            "import time"
          ],
          "functions": [
            "get_embedder",
            "get_vector_store",
            "get_vectorizer"
          ],
          "classes": [
            "EmbeddingRequest(BaseModel)",
            "EmbeddingResponse(BaseModel)",
            "BatchEmbeddingRequest(BaseModel)",
            "BatchEmbeddingResponse(BaseModel)",
            "SimilarityRequest(BaseModel)",
            "SimilarityResponse(BaseModel)",
            "DocumentUploadResponse(BaseModel)",
            "SearchRequest(BaseModel)",
            "SearchResult(BaseModel)",
            "SearchResponse(BaseModel)",
            "CollectionStatsResponse(BaseModel)"
          ]
        },
        "src/api/health.py": {
          "total_lines": 44,
          "code_lines": 35,
          "content_preview": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport sys\nimport platform\n\n# 导入路由\nfrom .embedding import router as embedding_router\n\napp = FastAPI(\n    title=\"RAG System API\",\n    description=\"Enterprise RAG System with Embedding Support\",\n    version=\"0.1.0\"\n)\n\n# 注册路由\napp.include_router(embedding_router)\n\nclass HealthResponse(BaseModel):\n    status: str\n    timestamp: datetime\n    version: str\n    python_version: str\n    platform: str\n\n@app.get(\"/health...",
          "imports": [
            "from fastapi import FastAPI",
            "from pydantic import BaseModel",
            "from datetime import datetime",
            "import sys",
            "import platform",
            "from .embedding import router as embedding_router",
            "import uvicorn"
          ],
          "functions": [],
          "classes": [
            "HealthResponse(BaseModel)"
          ]
        },
        "src/api/__init__.py": {
          "total_lines": 6,
          "code_lines": 4,
          "content_preview": "\"\"\"API模块初始化\"\"\"\n\nfrom .health import app\nfrom .embedding import router as embedding_router\n\n__all__ = ['app', 'embedding_router']",
          "imports": [
            "from .health import app",
            "from .embedding import router as embedding_router"
          ],
          "functions": [],
          "classes": []
        },
        "src/api/rag.py": {
          "total_lines": 345,
          "code_lines": 283,
          "content_preview": "\"\"\"RAG API接口\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport logging\nimport time\n\nfrom ..rag.rag_service import RAGService, RAGRequest, RAGResponse\nfrom ..rag.retriever import DocumentRetriever\nfrom ..rag.qa_generator import QAGenerator\nfrom ..embedding.embedder import TextEmbedder\nfrom ..vector_store.qdrant_client import QdrantVectorStore\n\nlogger = logging.getLogger(__name__)\n\n# ...",
          "imports": [
            "from typing import List, Dict, Any, Optional",
            "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks",
            "from pydantic import BaseModel, Field",
            "import logging",
            "import time",
            "from ..rag.rag_service import RAGService, RAGRequest, RAGResponse",
            "from ..rag.retriever import DocumentRetriever",
            "from ..rag.qa_generator import QAGenerator",
            "from ..embedding.embedder import TextEmbedder",
            "from ..vector_store.qdrant_client import QdrantVectorStore"
          ],
          "functions": [
            "get_rag_service",
            "query_sync",
            "batch_query",
            "validate_query",
            "get_system_status",
            "get_collection_stats",
            "health_check"
          ],
          "classes": [
            "QueryRequest(BaseModel)",
            "QueryResponse(BaseModel)",
            "BatchQueryRequest(BaseModel)",
            "BatchQueryResponse(BaseModel)",
            "ValidationResponse(BaseModel)",
            "SystemStatusResponse(BaseModel)"
          ]
        }
      }
    },
    "feature_analysis": {
      "structured_data": {
        "implemented": true,
        "evidence": [
          {
            "file": "lesson_requirements_analysis.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "lesson_requirements_analysis.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "test_connections.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "test_connections.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "test_database.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "test_database.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "keyword_search.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "keyword_search.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "compare_actual_vs_expected.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "deep_code_investigation.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "test_lesson07.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "analyze_branches.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "tests/test_batch_vectorization.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "scripts/optimize_database.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "scripts/optimize_database.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "scripts/migrate_data.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "scripts/start_dev.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "alembic/env.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "alembic/env.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/main.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/database/config.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/config.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/database/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/connection.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/connection.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/database/init_db.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/init_db.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/conflict_resolver.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/config.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/version_manager.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/indexer.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/change_detector.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/__init__.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/api_connector.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/api_connector.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/interactive_tuner.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/run_chunk_experiment.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiment_visualizer.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/chunk_optimizer.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/interactive_tuner.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/run_chunk_experiment.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/experiment_visualizer.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunk_experiment/experiments/chunk_optimization/chunk_optimizer.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/embedding/embedder.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/user.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/query.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/document.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/repositories/base.py",
            "keyword": "sql",
            "context": "Found in code content"
          },
          {
            "file": "src/rag/qa_generator.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/vector_store/document_vectorizer.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunk_manager.py",
            "keyword": "json",
            "context": "Found in code content"
          },
          {
            "file": "src/chunking/chunk_manager.py",
            "keyword": "csv",
            "context": "Found in code content"
          },
          {
            "file": "src/api/embedding.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/health.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/__init__.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "api",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "database": {
        "implemented": true,
        "evidence": [
          {
            "file": "test_database.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "keyword_search.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "test_lesson07.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "scripts/optimize_database.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "scripts/migrate_data.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "alembic/env.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/config.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/connection.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/database/init_db.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/integration.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/database_connector.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/__init__.py",
            "keyword": "database",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "database",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      },
      "api": {
        "implemented": true,
        "evidence": [
          {
            "file": "lesson_requirements_analysis.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "scripts/test_services.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "scripts/start_dev.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/config.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/__init__.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/main.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/incremental/monitoring.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/__init__.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/sync_manager.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/data_connectors/api_connector.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/embedding.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/health.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/__init__.py",
            "keyword": "api",
            "context": "Found in code content"
          },
          {
            "file": "src/api/rag.py",
            "keyword": "api",
            "context": "Found in code content"
          }
        ],
        "confidence": 1.0
      }
    },
    "code_quality": {
      "total_files": 92,
      "total_lines": 27524,
      "total_code_lines": 20729,
      "avg_file_size": 299.17391304347825,
      "code_ratio": 0.7531245458508937,
      "quality_score": 75.31245458508937
    },
    "missing_implementations": []
  }
}