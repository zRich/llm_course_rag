#!/usr/bin/env python3
"""
æ·±åº¦ä»£ç è°ƒæŸ¥è„šæœ¬
è¯¦ç»†åˆ†ææ¯ä¸ªæœ‰é—®é¢˜lessonåˆ†æ”¯çš„å®é™…ä»£ç å†…å®¹å’Œç¼ºå¤±æƒ…å†µ
"""

import os
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any
import difflib

class DeepCodeInvestigator:
    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.investigation_results = {}
        
    def get_branch_files(self, branch: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šåˆ†æ”¯çš„æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯"""
        try:
            # åˆ‡æ¢åˆ°æŒ‡å®šåˆ†æ”¯
            subprocess.run(['git', 'checkout', branch], 
                         cwd=self.repo_path, capture_output=True, check=True)
            
            # è·å–æ‰€æœ‰Pythonæ–‡ä»¶
            python_files = []
            for root, dirs, files in os.walk(self.repo_path):
                # è·³è¿‡.gitç›®å½•å’Œè™šæ‹Ÿç¯å¢ƒç›®å½•
                if '.git' in root or '.venv' in root or 'venv' in root or '__pycache__' in root:
                    continue
                for file in files:
                    if file.endswith('.py'):
                        file_path = Path(root) / file
                        rel_path = file_path.relative_to(self.repo_path)
                        python_files.append(str(rel_path))
            
            # è·å–æ–‡ä»¶å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯
            file_details = {}
            total_lines = 0
            
            for file_path in python_files:
                full_path = self.repo_path / file_path
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        lines = content.split('\n')
                        code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
                        
                        file_details[file_path] = {
                            'total_lines': len(lines),
                            'code_lines': len(code_lines),
                            'content_preview': content[:500] + '...' if len(content) > 500 else content,
                            'imports': self.extract_imports(content),
                            'functions': self.extract_functions(content),
                            'classes': self.extract_classes(content)
                        }
                        total_lines += len(lines)
                except Exception as e:
                    file_details[file_path] = {'error': str(e)}
            
            return {
                'python_files': python_files,
                'file_count': len(python_files),
                'total_lines': total_lines,
                'file_details': file_details
            }
            
        except subprocess.CalledProcessError as e:
            return {'error': f'Failed to checkout branch {branch}: {e}'}
    
    def extract_imports(self, content: str) -> List[str]:
        """æå–importè¯­å¥"""
        imports = []
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('import ') or line.startswith('from '):
                imports.append(line)
        return imports
    
    def extract_functions(self, content: str) -> List[str]:
        """æå–å‡½æ•°å®šä¹‰"""
        functions = []
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('def ') and '(' in line:
                functions.append(line.split('(')[0].replace('def ', ''))
        return functions
    
    def extract_classes(self, content: str) -> List[str]:
        """æå–ç±»å®šä¹‰"""
        classes = []
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('class ') and ':' in line:
                classes.append(line.split(':')[0].replace('class ', ''))
        return classes
    
    def analyze_lesson_implementation(self, lesson_id: str, expected_features: List[str]) -> Dict[str, Any]:
        """åˆ†ælessonçš„å…·ä½“å®ç°æƒ…å†µ"""
        branch_name = lesson_id
        branch_info = self.get_branch_files(branch_name)
        
        if 'error' in branch_info:
            return {
                'lesson': lesson_id,
                'status': 'branch_not_found',
                'error': branch_info['error']
            }
        
        # åˆ†æä»£ç å†…å®¹
        analysis = {
            'lesson': lesson_id,
            'branch_info': branch_info,
            'feature_analysis': {},
            'code_quality': {},
            'missing_implementations': []
        }
        
        # æ£€æŸ¥é¢„æœŸåŠŸèƒ½çš„å®ç°æƒ…å†µ
        for feature in expected_features:
            analysis['feature_analysis'][feature] = self.check_feature_implementation(
                branch_info['file_details'], feature
            )
        
        # ä»£ç è´¨é‡åˆ†æ
        analysis['code_quality'] = self.analyze_code_quality(branch_info['file_details'])
        
        return analysis
    
    def check_feature_implementation(self, file_details: Dict, feature: str) -> Dict[str, Any]:
        """æ£€æŸ¥ç‰¹å®šåŠŸèƒ½çš„å®ç°æƒ…å†µ"""
        feature_keywords = {
            'hybrid_search': ['bm25', 'vector', 'hybrid', 'fusion', 'rrf'],
            'metadata_filter': ['metadata', 'filter', 'where', 'condition'],
            'rerank': ['rerank', 'reranker', 'cross_encoder', 'cohere'],
            'cache': ['cache', 'redis', 'memory', 'lru'],
            'batch_processing': ['batch', 'bulk', 'queue', 'async'],
            'incremental_update': ['incremental', 'update', 'delta', 'version'],
            'structured_data': ['database', 'sql', 'api', 'json', 'csv'],
            'text_cleaning': ['clean', 'preprocess', 'normalize', 'denoise'],
            'chunking_strategy': ['chunk', 'split', 'segment', 'overlap'],
            'citation': ['citation', 'reference', 'source', 'provenance']
        }
        
        keywords = feature_keywords.get(feature, [feature.lower()])
        found_evidence = []
        
        for file_path, details in file_details.items():
            if 'error' in details:
                continue
                
            content = details.get('content_preview', '').lower()
            functions = [f.lower() for f in details.get('functions', [])]
            classes = [c.lower() for c in details.get('classes', [])]
            imports = [i.lower() for i in details.get('imports', [])]
            
            for keyword in keywords:
                if (keyword in content or 
                    any(keyword in f for f in functions) or
                    any(keyword in c for c in classes) or
                    any(keyword in i for i in imports)):
                    found_evidence.append({
                        'file': file_path,
                        'keyword': keyword,
                        'context': 'Found in code content'
                    })
        
        return {
            'implemented': len(found_evidence) > 0,
            'evidence': found_evidence,
            'confidence': min(len(found_evidence) * 0.3, 1.0)
        }
    
    def analyze_code_quality(self, file_details: Dict) -> Dict[str, Any]:
        """åˆ†æä»£ç è´¨é‡"""
        total_files = len(file_details)
        total_lines = sum(d.get('total_lines', 0) for d in file_details.values() if 'error' not in d)
        total_code_lines = sum(d.get('code_lines', 0) for d in file_details.values() if 'error' not in d)
        
        avg_file_size = total_lines / total_files if total_files > 0 else 0
        code_ratio = total_code_lines / total_lines if total_lines > 0 else 0
        
        return {
            'total_files': total_files,
            'total_lines': total_lines,
            'total_code_lines': total_code_lines,
            'avg_file_size': avg_file_size,
            'code_ratio': code_ratio,
            'quality_score': min(code_ratio * 100, 100)
        }
    
    def investigate_problematic_lessons(self):
        """è°ƒæŸ¥æœ‰é—®é¢˜çš„lesson"""
        # åŸºäºä¹‹å‰çš„åˆ†æç»“æœï¼Œå®šä¹‰æœ‰é—®é¢˜çš„lessonå’Œé¢„æœŸåŠŸèƒ½
        problematic_lessons = {
            'lesson02': ['docker', 'container', 'postgresql', 'redis'],
            'lesson03': ['database', 'model', 'migration', 'schema'],
            'lesson04': ['pdf', 'parse', 'chunk', 'split'],
            'lesson06': ['rag', 'retrieval', 'generation', 'query'],
            'lesson08': ['hybrid_search', 'bm25', 'vector', 'fusion'],
            'lesson09': ['metadata_filter', 'condition', 'where'],
            'lesson11': ['chunk', 'overlap', 'experiment', 'size'],
            'lesson12': ['multi_document', 'word', 'excel', 'ppt'],
            'lesson13': ['citation', 'reference', 'source', 'provenance'],
            'lesson14': ['cache', 'redis', 'performance'],
            'lesson15': ['batch_processing', 'bulk', 'resume'],
            'lesson16': ['incremental_update', 'delta', 'version'],
            'lesson17': ['structured_data', 'database', 'api']
        }
        
        results = {}
        
        for lesson_id, expected_features in problematic_lessons.items():
            print(f"\nğŸ” è°ƒæŸ¥ {lesson_id}...")
            analysis = self.analyze_lesson_implementation(lesson_id, expected_features)
            results[lesson_id] = analysis
            
            # æ‰“å°ç®€è¦ç»“æœ
            if 'error' in analysis:
                print(f"âŒ {lesson_id}: {analysis['error']}")
            else:
                implemented_features = sum(1 for f in analysis['feature_analysis'].values() if f['implemented'])
                total_features = len(expected_features)
                print(f"ğŸ“Š {lesson_id}: {implemented_features}/{total_features} åŠŸèƒ½å·²å®ç°")
                print(f"ğŸ“ æ–‡ä»¶æ•°: {analysis['branch_info']['file_count']}")
                print(f"ğŸ“ ä»£ç è¡Œæ•°: {analysis['branch_info']['total_lines']}")
        
        return results
    
    def save_investigation_results(self, results: Dict, filename: str = 'deep_investigation_results.json'):
        """ä¿å­˜è°ƒæŸ¥ç»“æœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è°ƒæŸ¥ç»“æœå·²ä¿å­˜åˆ° {filename}")

def main():
    print("ğŸ” å¼€å§‹æ·±åº¦ä»£ç è°ƒæŸ¥...")
    
    # åˆå§‹åŒ–è°ƒæŸ¥å™¨
    investigator = DeepCodeInvestigator('.')
    
    # è°ƒæŸ¥æœ‰é—®é¢˜çš„lesson
    results = investigator.investigate_problematic_lessons()
    
    # ä¿å­˜ç»“æœ
    investigator.save_investigation_results(results)
    
    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    print("\nğŸ“‹ è°ƒæŸ¥æ‘˜è¦:")
    print("=" * 50)
    
    for lesson_id, analysis in results.items():
        if 'error' in analysis:
            print(f"âŒ {lesson_id}: åˆ†æ”¯ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
        else:
            implemented = sum(1 for f in analysis['feature_analysis'].values() if f['implemented'])
            total = len(analysis['feature_analysis'])
            quality = analysis['code_quality']['quality_score']
            print(f"ğŸ“Š {lesson_id}: {implemented}/{total} åŠŸèƒ½å®ç°, è´¨é‡åˆ†æ•°: {quality:.1f}")
    
    print("\nâœ… æ·±åº¦è°ƒæŸ¥å®Œæˆï¼")

if __name__ == '__main__':
    main()