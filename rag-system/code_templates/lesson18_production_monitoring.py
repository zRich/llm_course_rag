#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Lesson18 ç”Ÿäº§ç¯å¢ƒç›‘æ§å®ç°æ¨¡æ¿
è§£å†³æ—¥å¿—ç®¡ç†ã€æ€§èƒ½åˆ†æå’Œæ•…éšœè¯Šæ–­ç¼ºå¤±é—®é¢˜

åŠŸèƒ½ç‰¹æ€§ï¼š
1. ç»“æ„åŒ–æ—¥å¿—ç®¡ç†å’Œåˆ†æ
2. å®æ—¶æ€§èƒ½ç›‘æ§å’Œåˆ†æ
3. è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œè¯Šæ–­
4. åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ª
5. ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§å’ŒæŠ¥è­¦
"""

import logging
import time
import json
import os
import uuid
import threading
import traceback
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager
import asyncio
import aiohttp
import psutil
import numpy as np
from collections import defaultdict, deque
import sqlite3
import redis
from elasticsearch import Elasticsearch
from prometheus_client import Counter, Histogram, Gauge, Summary, CollectorRegistry, generate_latest
import structlog
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
import matplotlib.pyplot as plt
import seaborn as sns
from flask import Flask, request, jsonify
import requests

# é…ç½®ç»“æ„åŒ–æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger(__name__)

class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«æšä¸¾"""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"

class AlertSeverity(Enum):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class HealthStatus(Enum):
    """å¥åº·çŠ¶æ€æšä¸¾"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: datetime
    level: LogLevel
    message: str
    service: str
    trace_id: Optional[str] = None
    span_id: Optional[str] = None
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    exception: Optional[str] = None
    stack_trace: Optional[str] = None

@dataclass
class MetricPoint:
    """æŒ‡æ ‡æ•°æ®ç‚¹"""
    name: str
    value: float
    timestamp: datetime
    labels: Dict[str, str] = field(default_factory=dict)
    metric_type: MetricType = MetricType.GAUGE

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: Dict[str, float]
    response_time: float
    throughput: float
    error_rate: float
    timestamp: datetime

@dataclass
class TraceSpan:
    """é“¾è·¯è¿½è¸ªè·¨åº¦"""
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    operation_name: str
    start_time: datetime
    end_time: Optional[datetime]
    duration: Optional[float]
    tags: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    status: str = "ok"

@dataclass
class Alert:
    """å‘Šè­¦ä¿¡æ¯"""
    id: str
    name: str
    description: str
    severity: AlertSeverity
    timestamp: datetime
    source: str
    metric_name: str
    current_value: float
    threshold: float
    status: str = "active"
    resolved_at: Optional[datetime] = None

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, service_name: str, log_level: LogLevel = LogLevel.INFO):
        self.service_name = service_name
        self.log_level = log_level
        self.logger = structlog.get_logger(service_name)
        self.log_storage: List[LogEntry] = []
        self.max_logs = 10000
        self._lock = threading.RLock()
        
        # é…ç½®æ–‡ä»¶æ—¥å¿—
        self._setup_file_logging()
        
        # é…ç½®Elasticsearchï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.es_client = self._setup_elasticsearch()
    
    def _setup_file_logging(self):
        """è®¾ç½®æ–‡ä»¶æ—¥å¿—"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            log_dir / f"{self.service_name}.log",
            encoding='utf-8'
        )
        file_handler.setLevel(getattr(logging, self.log_level.value.upper()))
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ åˆ°æ ¹æ—¥å¿—å™¨
        root_logger = logging.getLogger()
        root_logger.addHandler(file_handler)
        root_logger.setLevel(getattr(logging, self.log_level.value.upper()))
    
    def _setup_elasticsearch(self) -> Optional[Elasticsearch]:
        """è®¾ç½®Elasticsearchå®¢æˆ·ç«¯"""
        try:
            es_host = os.getenv('ELASTICSEARCH_HOST', 'localhost:9200')
            es_client = Elasticsearch([es_host])
            
            # æµ‹è¯•è¿æ¥
            if es_client.ping():
                logger.info("Elasticsearchè¿æ¥æˆåŠŸ", host=es_host)
                return es_client
            else:
                logger.warning("Elasticsearchè¿æ¥å¤±è´¥", host=es_host)
                return None
        except Exception as e:
            logger.warning("Elasticsearchåˆå§‹åŒ–å¤±è´¥", error=str(e))
            return None
    
    def log(self, level: LogLevel, message: str, **kwargs):
        """è®°å½•æ—¥å¿—"""
        # è·å–è°ƒç”¨ä¸Šä¸‹æ–‡
        trace_id = kwargs.pop('trace_id', None)
        span_id = kwargs.pop('span_id', None)
        user_id = kwargs.pop('user_id', None)
        request_id = kwargs.pop('request_id', None)
        
        # åˆ›å»ºæ—¥å¿—æ¡ç›®
        log_entry = LogEntry(
            timestamp=datetime.now(),
            level=level,
            message=message,
            service=self.service_name,
            trace_id=trace_id,
            span_id=span_id,
            user_id=user_id,
            request_id=request_id,
            metadata=kwargs
        )
        
        # å­˜å‚¨åˆ°å†…å­˜
        with self._lock:
            self.log_storage.append(log_entry)
            if len(self.log_storage) > self.max_logs:
                self.log_storage.pop(0)
        
        # è®°å½•åˆ°ç»“æ„åŒ–æ—¥å¿—
        log_data = {
            'service': self.service_name,
            'trace_id': trace_id,
            'span_id': span_id,
            'user_id': user_id,
            'request_id': request_id,
            **kwargs
        }
        
        getattr(self.logger, level.value)(message, **log_data)
        
        # å‘é€åˆ°Elasticsearch
        if self.es_client:
            self._send_to_elasticsearch(log_entry)
    
    def _send_to_elasticsearch(self, log_entry: LogEntry):
        """å‘é€æ—¥å¿—åˆ°Elasticsearch"""
        try:
            doc = {
                '@timestamp': log_entry.timestamp.isoformat(),
                'level': log_entry.level.value,
                'message': log_entry.message,
                'service': log_entry.service,
                'trace_id': log_entry.trace_id,
                'span_id': log_entry.span_id,
                'user_id': log_entry.user_id,
                'request_id': log_entry.request_id,
                'metadata': log_entry.metadata
            }
            
            index_name = f"rag-logs-{datetime.now().strftime('%Y.%m.%d')}"
            self.es_client.index(index=index_name, body=doc)
            
        except Exception as e:
            # é¿å…æ—¥å¿—è®°å½•å¤±è´¥å½±å“ä¸»ä¸šåŠ¡
            print(f"å‘é€æ—¥å¿—åˆ°Elasticsearchå¤±è´¥: {e}")
    
    def info(self, message: str, **kwargs):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.log(LogLevel.INFO, message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.log(LogLevel.WARNING, message, **kwargs)
    
    def error(self, message: str, **kwargs):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        self.log(LogLevel.ERROR, message, **kwargs)
    
    def critical(self, message: str, **kwargs):
        """è®°å½•ä¸¥é‡é”™è¯¯æ—¥å¿—"""
        self.log(LogLevel.CRITICAL, message, **kwargs)
    
    def search_logs(self, query: str = None, level: LogLevel = None, 
                   start_time: datetime = None, end_time: datetime = None,
                   limit: int = 100) -> List[LogEntry]:
        """æœç´¢æ—¥å¿—"""
        with self._lock:
            results = list(self.log_storage)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if level:
            results = [log for log in results if log.level == level]
        
        if start_time:
            results = [log for log in results if log.timestamp >= start_time]
        
        if end_time:
            results = [log for log in results if log.timestamp <= end_time]
        
        if query:
            query_lower = query.lower()
            results = [
                log for log in results 
                if query_lower in log.message.lower() or 
                   any(query_lower in str(v).lower() for v in log.metadata.values())
            ]
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        results.sort(key=lambda x: x.timestamp, reverse=True)
        
        return results[:limit]

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.registry = CollectorRegistry()
        self.metrics: Dict[str, Any] = {}
        self.metric_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        self._lock = threading.RLock()
        
        # åˆå§‹åŒ–åŸºç¡€æŒ‡æ ‡
        self._init_basic_metrics()
        
        # å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†
        self._start_system_metrics_collection()
    
    def _init_basic_metrics(self):
        """åˆå§‹åŒ–åŸºç¡€æŒ‡æ ‡"""
        # è¯·æ±‚è®¡æ•°å™¨
        self.request_count = Counter(
            'requests_total',
            'Total number of requests',
            ['method', 'endpoint', 'status'],
            registry=self.registry
        )
        
        # è¯·æ±‚å»¶è¿Ÿ
        self.request_duration = Histogram(
            'request_duration_seconds',
            'Request duration in seconds',
            ['method', 'endpoint'],
            registry=self.registry
        )
        
        # æ´»è·ƒè¿æ¥æ•°
        self.active_connections = Gauge(
            'active_connections',
            'Number of active connections',
            registry=self.registry
        )
        
        # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        self.cpu_usage = Gauge(
            'cpu_usage_percent',
            'CPU usage percentage',
            registry=self.registry
        )
        
        self.memory_usage = Gauge(
            'memory_usage_percent',
            'Memory usage percentage',
            registry=self.registry
        )
        
        self.disk_usage = Gauge(
            'disk_usage_percent',
            'Disk usage percentage',
            registry=self.registry
        )
        
        # ä¸šåŠ¡æŒ‡æ ‡
        self.search_requests = Counter(
            'search_requests_total',
            'Total search requests',
            ['query_type', 'status'],
            registry=self.registry
        )
        
        self.search_latency = Histogram(
            'search_latency_seconds',
            'Search latency in seconds',
            ['query_type'],
            registry=self.registry
        )
        
        self.vector_db_operations = Counter(
            'vector_db_operations_total',
            'Vector database operations',
            ['operation', 'status'],
            registry=self.registry
        )
        
        self.cache_hit_rate = Gauge(
            'cache_hit_rate',
            'Cache hit rate',
            registry=self.registry
        )
    
    def _start_system_metrics_collection(self):
        """å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
        def collect_system_metrics():
            while True:
                try:
                    # CPUä½¿ç”¨ç‡
                    cpu_percent = psutil.cpu_percent(interval=1)
                    self.cpu_usage.set(cpu_percent)
                    
                    # å†…å­˜ä½¿ç”¨ç‡
                    memory = psutil.virtual_memory()
                    self.memory_usage.set(memory.percent)
                    
                    # ç£ç›˜ä½¿ç”¨ç‡
                    disk = psutil.disk_usage('/')
                    self.disk_usage.set(disk.percent)
                    
                    # è®°å½•å†å²æ•°æ®
                    timestamp = datetime.now()
                    with self._lock:
                        self.metric_history['cpu_usage'].append((timestamp, cpu_percent))
                        self.metric_history['memory_usage'].append((timestamp, memory.percent))
                        self.metric_history['disk_usage'].append((timestamp, disk.percent))
                    
                    time.sleep(30)  # æ¯30ç§’æ”¶é›†ä¸€æ¬¡
                    
                except Exception as e:
                    logger.error("ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å¤±è´¥", error=str(e))
                    time.sleep(5)
        
        thread = threading.Thread(target=collect_system_metrics, daemon=True)
        thread.start()
    
    def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        self.request_count.labels(method=method, endpoint=endpoint, status=str(status)).inc()
        self.request_duration.labels(method=method, endpoint=endpoint).observe(duration)
    
    def record_search(self, query_type: str, status: str, latency: float):
        """è®°å½•æœç´¢æŒ‡æ ‡"""
        self.search_requests.labels(query_type=query_type, status=status).inc()
        self.search_latency.labels(query_type=query_type).observe(latency)
    
    def record_vector_db_operation(self, operation: str, status: str):
        """è®°å½•å‘é‡æ•°æ®åº“æ“ä½œ"""
        self.vector_db_operations.labels(operation=operation, status=status).inc()
    
    def update_cache_hit_rate(self, hit_rate: float):
        """æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡"""
        self.cache_hit_rate.set(hit_rate)
    
    def get_metrics_data(self) -> str:
        """è·å–Prometheusæ ¼å¼çš„æŒ‡æ ‡æ•°æ®"""
        return generate_latest(self.registry)
    
    def get_metric_history(self, metric_name: str, duration: timedelta = timedelta(hours=1)) -> List[tuple]:
        """è·å–æŒ‡æ ‡å†å²æ•°æ®"""
        cutoff_time = datetime.now() - duration
        
        with self._lock:
            history = self.metric_history.get(metric_name, deque())
            return [(ts, value) for ts, value in history if ts >= cutoff_time]
    
    def get_performance_summary(self) -> PerformanceMetrics:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        try:
            cpu_percent = psutil.cpu_percent()
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            network = psutil.net_io_counters()
            
            # è®¡ç®—ç½‘ç»œIO
            network_io = {
                'bytes_sent': network.bytes_sent,
                'bytes_recv': network.bytes_recv,
                'packets_sent': network.packets_sent,
                'packets_recv': network.packets_recv
            }
            
            # è·å–æœ€è¿‘çš„å“åº”æ—¶é—´å’Œååé‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            response_time = self._calculate_avg_response_time()
            throughput = self._calculate_throughput()
            error_rate = self._calculate_error_rate()
            
            return PerformanceMetrics(
                cpu_usage=cpu_percent,
                memory_usage=memory.percent,
                disk_usage=disk.percent,
                network_io=network_io,
                response_time=response_time,
                throughput=throughput,
                error_rate=error_rate,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error("è·å–æ€§èƒ½æ‘˜è¦å¤±è´¥", error=str(e))
            return PerformanceMetrics(
                cpu_usage=0, memory_usage=0, disk_usage=0,
                network_io={}, response_time=0, throughput=0,
                error_rate=0, timestamp=datetime.now()
            )
    
    def _calculate_avg_response_time(self) -> float:
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´"""
        # ç®€åŒ–å®ç°ï¼šä»PrometheusæŒ‡æ ‡ä¸­è·å–
        try:
            # è¿™é‡Œåº”è¯¥ä»å®é™…çš„æŒ‡æ ‡æ•°æ®ä¸­è®¡ç®—
            return 0.1  # ç¤ºä¾‹å€¼
        except:
            return 0.0
    
    def _calculate_throughput(self) -> float:
        """è®¡ç®—ååé‡ï¼ˆè¯·æ±‚/ç§’ï¼‰"""
        try:
            # è¿™é‡Œåº”è¯¥ä»å®é™…çš„æŒ‡æ ‡æ•°æ®ä¸­è®¡ç®—
            return 100.0  # ç¤ºä¾‹å€¼
        except:
            return 0.0
    
    def _calculate_error_rate(self) -> float:
        """è®¡ç®—é”™è¯¯ç‡"""
        try:
            # è¿™é‡Œåº”è¯¥ä»å®é™…çš„æŒ‡æ ‡æ•°æ®ä¸­è®¡ç®—
            return 0.01  # ç¤ºä¾‹å€¼ï¼š1%é”™è¯¯ç‡
        except:
            return 0.0

class DistributedTracer:
    """åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ª"""
    
    def __init__(self, service_name: str, jaeger_endpoint: str = None):
        self.service_name = service_name
        self.spans: Dict[str, TraceSpan] = {}
        self.active_spans: Dict[str, str] = {}  # thread_id -> span_id
        self._lock = threading.RLock()
        
        # è®¾ç½®OpenTelemetry
        self._setup_opentelemetry(jaeger_endpoint)
    
    def _setup_opentelemetry(self, jaeger_endpoint: str = None):
        """è®¾ç½®OpenTelemetryè¿½è¸ª"""
        try:
            # è®¾ç½®è¿½è¸ªæä¾›è€…
            trace.set_tracer_provider(TracerProvider())
            tracer = trace.get_tracer(__name__)
            
            # é…ç½®Jaegerå¯¼å‡ºå™¨
            if jaeger_endpoint:
                jaeger_exporter = JaegerExporter(
                    agent_host_name="localhost",
                    agent_port=6831,
                )
                
                span_processor = BatchSpanProcessor(jaeger_exporter)
                trace.get_tracer_provider().add_span_processor(span_processor)
            
            # è‡ªåŠ¨ä»ªè¡¨åŒ–
            RequestsInstrumentor().instrument()
            
            self.tracer = tracer
            logger.info("OpenTelemetryè¿½è¸ªå·²åˆå§‹åŒ–")
            
        except Exception as e:
            logger.warning("OpenTelemetryåˆå§‹åŒ–å¤±è´¥", error=str(e))
            self.tracer = None
    
    @contextmanager
    def start_span(self, operation_name: str, parent_span_id: str = None, **tags):
        """å¯åŠ¨æ–°çš„è·¨åº¦"""
        span_id = str(uuid.uuid4())
        trace_id = parent_span_id or str(uuid.uuid4())
        
        span = TraceSpan(
            trace_id=trace_id,
            span_id=span_id,
            parent_span_id=parent_span_id,
            operation_name=operation_name,
            start_time=datetime.now(),
            end_time=None,
            duration=None,
            tags=tags
        )
        
        with self._lock:
            self.spans[span_id] = span
            thread_id = threading.get_ident()
            self.active_spans[thread_id] = span_id
        
        try:
            yield span
        except Exception as e:
            span.status = "error"
            span.tags['error'] = True
            span.tags['error.message'] = str(e)
            span.logs.append({
                'timestamp': datetime.now().isoformat(),
                'level': 'error',
                'message': str(e),
                'stack_trace': traceback.format_exc()
            })
            raise
        finally:
            # ç»“æŸè·¨åº¦
            span.end_time = datetime.now()
            span.duration = (span.end_time - span.start_time).total_seconds()
            
            with self._lock:
                thread_id = threading.get_ident()
                if thread_id in self.active_spans:
                    del self.active_spans[thread_id]
    
    def get_current_span(self) -> Optional[TraceSpan]:
        """è·å–å½“å‰æ´»è·ƒçš„è·¨åº¦"""
        thread_id = threading.get_ident()
        with self._lock:
            span_id = self.active_spans.get(thread_id)
            if span_id:
                return self.spans.get(span_id)
        return None
    
    def add_log(self, message: str, level: str = "info", **fields):
        """å‘å½“å‰è·¨åº¦æ·»åŠ æ—¥å¿—"""
        span = self.get_current_span()
        if span:
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'level': level,
                'message': message,
                **fields
            }
            span.logs.append(log_entry)
    
    def set_tag(self, key: str, value: Any):
        """è®¾ç½®å½“å‰è·¨åº¦çš„æ ‡ç­¾"""
        span = self.get_current_span()
        if span:
            span.tags[key] = value
    
    def get_trace(self, trace_id: str) -> List[TraceSpan]:
        """è·å–å®Œæ•´çš„è¿½è¸ªé“¾è·¯"""
        with self._lock:
            return [span for span in self.spans.values() if span.trace_id == trace_id]
    
    def search_spans(self, operation_name: str = None, tags: Dict[str, Any] = None,
                    start_time: datetime = None, end_time: datetime = None) -> List[TraceSpan]:
        """æœç´¢è·¨åº¦"""
        with self._lock:
            results = list(self.spans.values())
        
        if operation_name:
            results = [span for span in results if operation_name in span.operation_name]
        
        if tags:
            results = [
                span for span in results
                if all(span.tags.get(k) == v for k, v in tags.items())
            ]
        
        if start_time:
            results = [span for span in results if span.start_time >= start_time]
        
        if end_time:
            results = [span for span in results if span.start_time <= end_time]
        
        return sorted(results, key=lambda x: x.start_time, reverse=True)

class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.metric_windows: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))
        self.thresholds: Dict[str, Dict[str, float]] = {}
        self.anomalies: List[Dict] = []
        self._lock = threading.RLock()
    
    def add_metric_point(self, metric_name: str, value: float, timestamp: datetime = None):
        """æ·»åŠ æŒ‡æ ‡æ•°æ®ç‚¹"""
        if timestamp is None:
            timestamp = datetime.now()
        
        with self._lock:
            self.metric_windows[metric_name].append((timestamp, value))
            
            # æ£€æµ‹å¼‚å¸¸
            self._detect_anomaly(metric_name, value, timestamp)
    
    def set_threshold(self, metric_name: str, min_value: float = None, max_value: float = None,
                    std_multiplier: float = None):
        """è®¾ç½®å¼‚å¸¸æ£€æµ‹é˜ˆå€¼"""
        self.thresholds[metric_name] = {
            'min_value': min_value,
            'max_value': max_value,
            'std_multiplier': std_multiplier
        }
    
    def _detect_anomaly(self, metric_name: str, value: float, timestamp: datetime):
        """æ£€æµ‹å¼‚å¸¸"""
        threshold_config = self.thresholds.get(metric_name, {})
        
        # åŸºäºå›ºå®šé˜ˆå€¼çš„æ£€æµ‹
        min_value = threshold_config.get('min_value')
        max_value = threshold_config.get('max_value')
        
        if min_value is not None and value < min_value:
            self._record_anomaly(metric_name, value, timestamp, f"å€¼ä½äºæœ€å°é˜ˆå€¼ {min_value}")
            return
        
        if max_value is not None and value > max_value:
            self._record_anomaly(metric_name, value, timestamp, f"å€¼è¶…è¿‡æœ€å¤§é˜ˆå€¼ {max_value}")
            return
        
        # åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹
        std_multiplier = threshold_config.get('std_multiplier')
        if std_multiplier and len(self.metric_windows[metric_name]) >= 10:
            values = [v for _, v in self.metric_windows[metric_name]]
            mean = np.mean(values)
            std = np.std(values)
            
            if abs(value - mean) > std_multiplier * std:
                self._record_anomaly(
                    metric_name, value, timestamp,
                    f"å€¼åç¦»å‡å€¼è¶…è¿‡ {std_multiplier} ä¸ªæ ‡å‡†å·®"
                )
    
    def _record_anomaly(self, metric_name: str, value: float, timestamp: datetime, reason: str):
        """è®°å½•å¼‚å¸¸"""
        anomaly = {
            'id': str(uuid.uuid4()),
            'metric_name': metric_name,
            'value': value,
            'timestamp': timestamp,
            'reason': reason,
            'severity': self._calculate_severity(metric_name, value)
        }
        
        self.anomalies.append(anomaly)
        logger.warning("æ£€æµ‹åˆ°å¼‚å¸¸", **anomaly)
    
    def _calculate_severity(self, metric_name: str, value: float) -> str:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        # ç®€åŒ–çš„ä¸¥é‡ç¨‹åº¦è®¡ç®—
        if 'cpu' in metric_name.lower() or 'memory' in metric_name.lower():
            if value > 90:
                return "critical"
            elif value > 80:
                return "high"
            elif value > 70:
                return "medium"
            else:
                return "low"
        
        return "medium"
    
    def get_recent_anomalies(self, duration: timedelta = timedelta(hours=1)) -> List[Dict]:
        """è·å–æœ€è¿‘çš„å¼‚å¸¸"""
        cutoff_time = datetime.now() - duration
        return [
            anomaly for anomaly in self.anomalies
            if anomaly['timestamp'] >= cutoff_time
        ]

class ProductionMonitor:
    """ç”Ÿäº§ç¯å¢ƒç›‘æ§ä¸»ç±»"""
    
    def __init__(self, service_name: str, config: Dict[str, Any] = None):
        self.service_name = service_name
        self.config = config or {}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger = StructuredLogger(service_name)
        self.metrics = MetricsCollector(service_name)
        self.tracer = DistributedTracer(service_name)
        self.anomaly_detector = AnomalyDetector()
        
        # å‘Šè­¦ç®¡ç†
        self.alerts: List[Alert] = []
        self.alert_callbacks: List[Callable] = []
        
        # å¥åº·æ£€æŸ¥
        self.health_status = HealthStatus.UNKNOWN
        self.health_checks: List[Callable] = []
        
        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        self._start_monitoring_tasks()
    
    def _start_monitoring_tasks(self):
        """å¯åŠ¨ç›‘æ§ä»»åŠ¡"""
        # å¯åŠ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡
        def anomaly_detection_loop():
            while True:
                try:
                    # è·å–æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡
                    perf_metrics = self.metrics.get_performance_summary()
                    
                    # æ·»åŠ åˆ°å¼‚å¸¸æ£€æµ‹å™¨
                    self.anomaly_detector.add_metric_point('cpu_usage', perf_metrics.cpu_usage)
                    self.anomaly_detector.add_metric_point('memory_usage', perf_metrics.memory_usage)
                    self.anomaly_detector.add_metric_point('response_time', perf_metrics.response_time)
                    self.anomaly_detector.add_metric_point('error_rate', perf_metrics.error_rate)
                    
                    # æ£€æŸ¥å¼‚å¸¸å¹¶ç”Ÿæˆå‘Šè­¦
                    recent_anomalies = self.anomaly_detector.get_recent_anomalies(timedelta(minutes=1))
                    for anomaly in recent_anomalies:
                        self._create_alert_from_anomaly(anomaly)
                    
                    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                    
                except Exception as e:
                    self.logger.error("å¼‚å¸¸æ£€æµ‹å¾ªç¯å¤±è´¥", error=str(e))
                    time.sleep(30)
        
        # å¯åŠ¨å¥åº·æ£€æŸ¥ä»»åŠ¡
        def health_check_loop():
            while True:
                try:
                    self._perform_health_checks()
                    time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                except Exception as e:
                    self.logger.error("å¥åº·æ£€æŸ¥å¾ªç¯å¤±è´¥", error=str(e))
                    time.sleep(10)
        
        # å¯åŠ¨åå°çº¿ç¨‹
        threading.Thread(target=anomaly_detection_loop, daemon=True).start()
        threading.Thread(target=health_check_loop, daemon=True).start()
    
    def _create_alert_from_anomaly(self, anomaly: Dict):
        """ä»å¼‚å¸¸åˆ›å»ºå‘Šè­¦"""
        alert = Alert(
            id=str(uuid.uuid4()),
            name=f"å¼‚å¸¸æ£€æµ‹: {anomaly['metric_name']}",
            description=anomaly['reason'],
            severity=AlertSeverity(anomaly['severity']),
            timestamp=anomaly['timestamp'],
            source=self.service_name,
            metric_name=anomaly['metric_name'],
            current_value=anomaly['value'],
            threshold=0  # å¼‚å¸¸æ£€æµ‹æ²¡æœ‰å›ºå®šé˜ˆå€¼
        )
        
        self.alerts.append(alert)
        self.logger.warning("ç”Ÿæˆå‘Šè­¦", alert_id=alert.id, metric=alert.metric_name)
        
        # è°ƒç”¨å‘Šè­¦å›è°ƒ
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                self.logger.error("å‘Šè­¦å›è°ƒå¤±è´¥", error=str(e))
    
    def add_health_check(self, check_func: Callable[[], bool], name: str = None):
        """æ·»åŠ å¥åº·æ£€æŸ¥å‡½æ•°"""
        self.health_checks.append((check_func, name or check_func.__name__))
    
    def _perform_health_checks(self):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        if not self.health_checks:
            self.health_status = HealthStatus.UNKNOWN
            return
        
        failed_checks = []
        
        for check_func, check_name in self.health_checks:
            try:
                if not check_func():
                    failed_checks.append(check_name)
            except Exception as e:
                failed_checks.append(f"{check_name} (å¼‚å¸¸: {e})")
        
        if not failed_checks:
            self.health_status = HealthStatus.HEALTHY
        elif len(failed_checks) < len(self.health_checks) / 2:
            self.health_status = HealthStatus.DEGRADED
        else:
            self.health_status = HealthStatus.UNHEALTHY
        
        if failed_checks:
            self.logger.warning("å¥åº·æ£€æŸ¥å¤±è´¥", failed_checks=failed_checks)
    
    def add_alert_callback(self, callback: Callable[[Alert], None]):
        """æ·»åŠ å‘Šè­¦å›è°ƒ"""
        self.alert_callbacks.append(callback)
    
    @contextmanager
    def trace_operation(self, operation_name: str, **tags):
        """è¿½è¸ªæ“ä½œ"""
        with self.tracer.start_span(operation_name, **tags) as span:
            start_time = time.time()
            try:
                yield span
            except Exception as e:
                self.logger.error(f"{operation_name} å¤±è´¥", error=str(e), trace_id=span.trace_id)
                raise
            finally:
                duration = time.time() - start_time
                self.logger.info(f"{operation_name} å®Œæˆ", duration=duration, trace_id=span.trace_id)
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§é¢æ¿æ•°æ®"""
        perf_metrics = self.metrics.get_performance_summary()
        recent_alerts = [alert for alert in self.alerts if alert.status == "active"]
        recent_anomalies = self.anomaly_detector.get_recent_anomalies()
        
        return {
            'service_name': self.service_name,
            'health_status': self.health_status.value,
            'performance_metrics': asdict(perf_metrics),
            'active_alerts_count': len(recent_alerts),
            'recent_anomalies_count': len(recent_anomalies),
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total,
                'disk_total': psutil.disk_usage('/').total
            },
            'uptime': time.time() - self._start_time if hasattr(self, '_start_time') else 0
        }
    
    def generate_report(self, duration: timedelta = timedelta(hours=24)) -> Dict[str, Any]:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        end_time = datetime.now()
        start_time = end_time - duration
        
        # è·å–å†å²æ•°æ®
        cpu_history = self.metrics.get_metric_history('cpu_usage', duration)
        memory_history = self.metrics.get_metric_history('memory_usage', duration)
        
        # ç»Ÿè®¡å‘Šè­¦
        period_alerts = [
            alert for alert in self.alerts
            if start_time <= alert.timestamp <= end_time
        ]
        
        # ç»Ÿè®¡å¼‚å¸¸
        period_anomalies = [
            anomaly for anomaly in self.anomaly_detector.anomalies
            if start_time <= anomaly['timestamp'] <= end_time
        ]
        
        return {
            'report_period': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_hours': duration.total_seconds() / 3600
            },
            'performance_summary': {
                'avg_cpu_usage': np.mean([v for _, v in cpu_history]) if cpu_history else 0,
                'max_cpu_usage': max([v for _, v in cpu_history]) if cpu_history else 0,
                'avg_memory_usage': np.mean([v for _, v in memory_history]) if memory_history else 0,
                'max_memory_usage': max([v for _, v in memory_history]) if memory_history else 0
            },
            'alerts_summary': {
                'total_alerts': len(period_alerts),
                'critical_alerts': len([a for a in period_alerts if a.severity == AlertSeverity.CRITICAL]),
                'high_alerts': len([a for a in period_alerts if a.severity == AlertSeverity.HIGH]),
                'medium_alerts': len([a for a in period_alerts if a.severity == AlertSeverity.MEDIUM]),
                'low_alerts': len([a for a in period_alerts if a.severity == AlertSeverity.LOW])
            },
            'anomalies_summary': {
                'total_anomalies': len(period_anomalies),
                'by_metric': self._group_anomalies_by_metric(period_anomalies)
            },
            'health_status': self.health_status.value
        }
    
    def _group_anomalies_by_metric(self, anomalies: List[Dict]) -> Dict[str, int]:
        """æŒ‰æŒ‡æ ‡åˆ†ç»„å¼‚å¸¸"""
        groups = defaultdict(int)
        for anomaly in anomalies:
            groups[anomaly['metric_name']] += 1
        return dict(groups)
    
    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        self._start_time = time.time()
        self.logger.info("ç”Ÿäº§ç¯å¢ƒç›‘æ§å·²å¯åŠ¨", service=self.service_name)
        
        # è®¾ç½®å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
        self.anomaly_detector.set_threshold('cpu_usage', max_value=90, std_multiplier=2)
        self.anomaly_detector.set_threshold('memory_usage', max_value=85, std_multiplier=2)
        self.anomaly_detector.set_threshold('response_time', max_value=5.0, std_multiplier=3)
        self.anomaly_detector.set_threshold('error_rate', max_value=0.05, std_multiplier=2)
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.logger.info("ç”Ÿäº§ç¯å¢ƒç›‘æ§å·²åœæ­¢", service=self.service_name)

def create_flask_app_with_monitoring(monitor: ProductionMonitor) -> Flask:
    """åˆ›å»ºå¸¦ç›‘æ§çš„Flaskåº”ç”¨"""
    app = Flask(__name__)
    
    @app.before_request
    def before_request():
        g.start_time = time.time()
        g.request_id = str(uuid.uuid4())
    
    @app.after_request
    def after_request(response):
        duration = time.time() - g.start_time
        
        # è®°å½•è¯·æ±‚æŒ‡æ ‡
        monitor.metrics.record_request(
            method=request.method,
            endpoint=request.endpoint or 'unknown',
            status=response.status_code,
            duration=duration
        )
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        monitor.logger.info(
            "HTTPè¯·æ±‚",
            method=request.method,
            path=request.path,
            status=response.status_code,
            duration=duration,
            request_id=g.request_id
        )
        
        return response
    
    @app.route('/health')
    def health_check():
        """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
        return jsonify({
            'status': monitor.health_status.value,
            'timestamp': datetime.now().isoformat(),
            'service': monitor.service_name
        })
    
    @app.route('/metrics')
    def metrics():
        """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
        return monitor.metrics.get_metrics_data(), 200, {
            'Content-Type': 'text/plain; charset=utf-8'
        }
    
    @app.route('/dashboard')
    def dashboard():
        """ç›‘æ§é¢æ¿æ•°æ®"""
        return jsonify(monitor.get_dashboard_data())
    
    @app.route('/report')
    def report():
        """ç›‘æ§æŠ¥å‘Š"""
        hours = request.args.get('hours', 24, type=int)
        duration = timedelta(hours=hours)
        return jsonify(monitor.generate_report(duration))
    
    return app

def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    print("ç”Ÿäº§ç¯å¢ƒç›‘æ§ç³»ç»Ÿæµ‹è¯•\n" + "="*50)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = ProductionMonitor("rag-system")
    
    # æ·»åŠ å¥åº·æ£€æŸ¥
    def check_database():
        # æ¨¡æ‹Ÿæ•°æ®åº“æ£€æŸ¥
        return True
    
    def check_vector_db():
        # æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“æ£€æŸ¥
        return True
    
    monitor.add_health_check(check_database, "database")
    monitor.add_health_check(check_vector_db, "vector_db")
    
    # æ·»åŠ å‘Šè­¦å›è°ƒ
    def alert_callback(alert: Alert):
        print(f"ğŸš¨ å‘Šè­¦: {alert.name} - {alert.description}")
    
    monitor.add_alert_callback(alert_callback)
    
    # å¯åŠ¨ç›‘æ§
    monitor.start()
    
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    print("\næ¨¡æ‹Ÿç›‘æ§æ“ä½œ...")
    
    # æ¨¡æ‹Ÿæœç´¢æ“ä½œ
    with monitor.trace_operation("search_documents", query_type="semantic") as span:
        time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        monitor.metrics.record_search("semantic", "success", 0.1)
        monitor.tracer.add_log("æœç´¢å®Œæˆ", results_count=5)
    
    # æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“æ“ä½œ
    monitor.metrics.record_vector_db_operation("insert", "success")
    monitor.metrics.record_vector_db_operation("search", "success")
    
    # æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
    monitor.metrics.update_cache_hit_rate(0.85)
    
    # ç”Ÿæˆä¸€äº›å¼‚å¸¸æ•°æ®æ¥æµ‹è¯•å¼‚å¸¸æ£€æµ‹
    monitor.anomaly_detector.add_metric_point("cpu_usage", 95.0)  # é«˜CPUä½¿ç”¨ç‡
    monitor.anomaly_detector.add_metric_point("response_time", 10.0)  # é«˜å“åº”æ—¶é—´
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç›‘æ§ç³»ç»Ÿå¤„ç†
    time.sleep(2)
    
    # è·å–ç›‘æ§é¢æ¿æ•°æ®
    dashboard_data = monitor.get_dashboard_data()
    print(f"\nç›‘æ§é¢æ¿æ•°æ®:")
    print(f"- æœåŠ¡çŠ¶æ€: {dashboard_data['health_status']}")
    print(f"- CPUä½¿ç”¨ç‡: {dashboard_data['performance_metrics']['cpu_usage']:.1f}%")
    print(f"- å†…å­˜ä½¿ç”¨ç‡: {dashboard_data['performance_metrics']['memory_usage']:.1f}%")
    print(f"- æ´»è·ƒå‘Šè­¦æ•°: {dashboard_data['active_alerts_count']}")
    print(f"- æœ€è¿‘å¼‚å¸¸æ•°: {dashboard_data['recent_anomalies_count']}")
    
    # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
    report = monitor.generate_report(timedelta(hours=1))
    print(f"\nç›‘æ§æŠ¥å‘Š (æœ€è¿‘1å°æ—¶):")
    print(f"- æ€»å‘Šè­¦æ•°: {report['alerts_summary']['total_alerts']}")
    print(f"- æ€»å¼‚å¸¸æ•°: {report['anomalies_summary']['total_anomalies']}")
    print(f"- å¹³å‡CPUä½¿ç”¨ç‡: {report['performance_summary']['avg_cpu_usage']:.1f}%")
    
    # åˆ›å»ºFlaskåº”ç”¨è¿›è¡Œæ¼”ç¤º
    app = create_flask_app_with_monitoring(monitor)
    
    print("\nç›‘æ§APIç«¯ç‚¹:")
    print("- å¥åº·æ£€æŸ¥: GET /health")
    print("- æŒ‡æ ‡æ•°æ®: GET /metrics")
    print("- ç›‘æ§é¢æ¿: GET /dashboard")
    print("- ç›‘æ§æŠ¥å‘Š: GET /report?hours=24")
    
    print("\nè¦å¯åŠ¨WebæœåŠ¡å™¨ï¼Œè¯·è¿è¡Œ:")
    print("python lesson18_production_monitoring.py")
    
    # åœæ­¢ç›‘æ§
    monitor.stop()

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œå¯åŠ¨æ¼”ç¤º
    if len(os.sys.argv) > 1 and os.sys.argv[1] == 'demo':
        main()
    else:
        # å¯åŠ¨WebæœåŠ¡å™¨
        monitor = ProductionMonitor("rag-system")
        monitor.start()
        
        app = create_flask_app_with_monitoring(monitor)
        print("å¯åŠ¨ç”Ÿäº§ç¯å¢ƒç›‘æ§æœåŠ¡...")
        print("è®¿é—® http://localhost:5000/dashboard æŸ¥çœ‹ç›‘æ§é¢æ¿")
        
        try:
            app.run(host='0.0.0.0', port=5000, debug=False)
        except KeyboardInterrupt:
            print("\næ­£åœ¨åœæ­¢ç›‘æ§æœåŠ¡...")
            monitor.stop()