# ç¬¬ä¸‰è¯¾ï¼šæ•°æ®æ¨¡å‹ä¸è¿ç§» - å­¦ç”ŸExerciseæŒ‡å¯¼

## ğŸ“‹ ä»£ç åŸºç¡€å‡†å¤‡

åœ¨å¼€å§‹æœ¬èŠ‚è¯¾çš„Exerciseä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åˆ‡æ¢åˆ°lesson03åˆ†æ”¯è·å–æœ¬è¯¾ç¨‹çš„ä»£ç ã€‚

### æ­¥éª¤1ï¼šè¿›å…¥é¡¹ç›®ç›®å½•

```bash
# è¿›å…¥rag-systemé¡¹ç›®ç›®å½•
cd rag-system
```

### æ­¥éª¤2ï¼šåˆ‡æ¢åˆ°lesson03åˆ†æ”¯

```bash
# åˆ‡æ¢åˆ°lesson03åˆ†æ”¯
git checkout lesson03

# éªŒè¯å½“å‰åˆ†æ”¯
git branch
# åº”è¯¥æ˜¾ç¤º * lesson03
```

### æ­¥éª¤3ï¼šéªŒè¯ä»£ç çŠ¶æ€

```bash
# æŸ¥çœ‹é¡¹ç›®ç»“æ„
ls -la
# åº”è¯¥çœ‹åˆ°ï¼šsrc/models/ src/database/ migrations/ ç­‰æ–°å¢çš„ç›®å½•

# æ£€æŸ¥æ•°æ®æ¨¡å‹æ–‡ä»¶
ls src/models/
# åº”è¯¥çœ‹åˆ°ï¼š__init__.py base.py user.py document.py ç­‰æ–‡ä»¶

# æ£€æŸ¥æ•°æ®åº“é…ç½®
cat src/database/connection.py
# åº”è¯¥çœ‹åˆ°æ•°æ®åº“è¿æ¥é…ç½®
```

### æ­¥éª¤4ï¼šå¯åŠ¨ä¾èµ–æœåŠ¡

```bash
# å¯åŠ¨ä¾èµ–æœåŠ¡ï¼ˆPostgreSQLç­‰ï¼‰
docker-compose up -d

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# éªŒè¯PostgreSQLè¿æ¥
docker-compose exec postgres psql -U postgres -d rag_db -c "SELECT version();"
```

**è¯´æ˜**ï¼š
- `lesson03`åˆ†æ”¯åŒ…å«äº†å‰ä¸¤è¯¾çš„æ‰€æœ‰ä»£ç ï¼Œå¹¶æ–°å¢äº†å®Œæ•´çš„æ•°æ®æ¨¡å‹è®¾è®¡
- åŒ…å«SQLModelæ•°æ®æ¨¡å‹ã€æ•°æ®åº“è¿æ¥é…ç½®å’ŒAlembicè¿ç§»è„šæœ¬
- åŒ…å«ç”¨æˆ·ç®¡ç†ã€æ–‡æ¡£ç®¡ç†ç­‰æ ¸å¿ƒæ•°æ®æ¨¡å‹
- è¿™ç§æ–¹å¼ç¡®ä¿äº†ä»£ç çš„æ¸è¿›å¼å¼€å‘å’Œç‰ˆæœ¬ä¸€è‡´æ€§

---

## ğŸ¯ Exerciseç›®æ ‡

é€šè¿‡æœ¬æ¬¡Exerciseï¼Œä½ å°†ï¼š
- æŒæ¡SQLModelæ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•
- å­¦ä¼šè®¾è®¡RAGç³»ç»Ÿçš„æ•°æ®æ¨¡å‹
- ç†è§£æ•°æ®åº“è¿æ¥æ± çš„é…ç½®å’Œç®¡ç†
- æŒæ¡Alembicæ•°æ®è¿ç§»å·¥å…·çš„ä½¿ç”¨
- å­¦ä¼šæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–çš„åŸºæœ¬æ–¹æ³•

---

## ğŸ”§ Exerciseç¯å¢ƒè¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- å®Œæˆå‰ä¸¤è¯¾çš„ç¯å¢ƒé…ç½®
- PostgreSQLæ•°æ®åº“æ­£å¸¸è¿è¡Œ
- Python 3.11+ ç¯å¢ƒ

### å¿…éœ€ä¾èµ–
```bash
# å®‰è£…SQLModelå’Œç›¸å…³ä¾èµ–
uv add sqlmodel
uv add alembic
uv add psycopg2-binary
uv add asyncpg  # å¼‚æ­¥æ”¯æŒ
```

---

## ğŸ”¬ Exerciseä¸€ï¼šSQLModelåŸºç¡€ä½¿ç”¨

### 1.1 åˆ›å»ºåŸºç¡€æ•°æ®æ¨¡å‹

**åˆ›å»ºæ¨¡å‹ç›®å½•ç»“æ„ï¼š**
```bash
mkdir -p src/models
touch src/models/__init__.py
```

**åˆ›å»º `src/models/base.py`ï¼š**
```python
from sqlmodel import SQLModel, Field
from typing import Optional
from datetime import datetime
import uuid

class BaseModel(SQLModel):
    """åŸºç¡€æ¨¡å‹ç±»"""
    id: Optional[int] = Field(default=None, primary_key=True)
    uuid: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        unique=True,
        index=True,
        description="å…¨å±€å”¯ä¸€æ ‡è¯†ç¬¦"
    )
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="åˆ›å»ºæ—¶é—´"
    )
    updated_at: Optional[datetime] = Field(
        default=None,
        description="æ›´æ–°æ—¶é—´"
    )
    
    class Config:
        """æ¨¡å‹é…ç½®"""
        # å…è®¸ä»ORMå¯¹è±¡åˆ›å»º
        from_attributes = True
        # JSONåºåˆ—åŒ–é…ç½®
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }
```

**åˆ›å»º `src/models/user.py`ï¼š**
```python
from sqlmodel import SQLModel, Field, Relationship
from typing import Optional, List
from enum import Enum
from .base import BaseModel

class UserRole(str, Enum):
    """ç”¨æˆ·è§’è‰²æšä¸¾"""
    ADMIN = "admin"
    USER = "user"
    GUEST = "guest"

class UserBase(SQLModel):
    """ç”¨æˆ·åŸºç¡€å­—æ®µ"""
    username: str = Field(
        max_length=50,
        unique=True,
        index=True,
        description="ç”¨æˆ·å"
    )
    email: str = Field(
        max_length=100,
        unique=True,
        index=True,
        description="é‚®ç®±åœ°å€"
    )
    role: UserRole = Field(
        default=UserRole.USER,
        description="ç”¨æˆ·è§’è‰²"
    )
    is_active: bool = Field(
        default=True,
        description="æ˜¯å¦æ¿€æ´»"
    )
    preferred_language: Optional[str] = Field(
        default="zh",
        max_length=10,
        description="é¦–é€‰è¯­è¨€"
    )
    max_query_length: int = Field(
        default=1000,
        ge=1,
        le=10000,
        description="æœ€å¤§æŸ¥è¯¢é•¿åº¦"
    )

class User(UserBase, BaseModel, table=True):
    """ç”¨æˆ·æ¨¡å‹"""
    __tablename__ = "users"
    
    password_hash: str = Field(
        max_length=255,
        description="å¯†ç å“ˆå¸Œ"
    )
    last_login_at: Optional[datetime] = Field(
        default=None,
        description="æœ€åç™»å½•æ—¶é—´"
    )
    
    # å…³ç³»å®šä¹‰
    documents: List["Document"] = Relationship(
        back_populates="user",
        sa_relationship_kwargs={"cascade": "all, delete-orphan"}
    )
    queries: List["QueryHistory"] = Relationship(
        back_populates="user",
        sa_relationship_kwargs={"cascade": "all, delete-orphan"}
    )

class UserCreate(UserBase):
    """åˆ›å»ºç”¨æˆ·çš„è¯·æ±‚æ¨¡å‹"""
    password: str = Field(
        min_length=8,
        max_length=100,
        description="å¯†ç "
    )

class UserRead(UserBase):
    """è¯»å–ç”¨æˆ·çš„å“åº”æ¨¡å‹"""
    id: int
    uuid: str
    created_at: datetime
    updated_at: Optional[datetime]
    last_login_at: Optional[datetime]

class UserUpdate(SQLModel):
    """æ›´æ–°ç”¨æˆ·çš„è¯·æ±‚æ¨¡å‹"""
    email: Optional[str] = None
    role: Optional[UserRole] = None
    is_active: Optional[bool] = None
    preferred_language: Optional[str] = None
    max_query_length: Optional[int] = None
```

**åˆ›å»º `src/models/document.py`ï¼š**
```python
from sqlmodel import SQLModel, Field, Relationship
from typing import Optional, List
from enum import Enum
from datetime import datetime
from .base import BaseModel

class DocumentStatus(str, Enum):
    """æ–‡æ¡£çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class DocumentBase(SQLModel):
    """æ–‡æ¡£åŸºç¡€å­—æ®µ"""
    title: str = Field(
        max_length=255,
        index=True,
        description="æ–‡æ¡£æ ‡é¢˜"
    )
    description: Optional[str] = Field(
        default=None,
        max_length=1000,
        description="æ–‡æ¡£æè¿°"
    )
    original_filename: str = Field(
        max_length=255,
        description="åŸå§‹æ–‡ä»¶å"
    )
    file_path: str = Field(
        max_length=500,
        description="æ–‡ä»¶è·¯å¾„"
    )
    file_size: int = Field(
        ge=0,
        description="æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"
    )
    mime_type: str = Field(
        max_length=100,
        description="MIMEç±»å‹"
    )
    file_hash: str = Field(
        max_length=64,
        index=True,
        description="æ–‡ä»¶SHA-256å“ˆå¸Œ"
    )
    status: DocumentStatus = Field(
        default=DocumentStatus.PENDING,
        index=True,
        description="å¤„ç†çŠ¶æ€"
    )

class Document(DocumentBase, BaseModel, table=True):
    """æ–‡æ¡£æ¨¡å‹"""
    __tablename__ = "documents"
    
    error_message: Optional[str] = Field(
        default=None,
        description="é”™è¯¯ä¿¡æ¯"
    )
    page_count: Optional[int] = Field(
        default=None,
        ge=0,
        description="é¡µæ•°"
    )
    word_count: Optional[int] = Field(
        default=None,
        ge=0,
        description="å­—æ•°"
    )
    chunk_count: Optional[int] = Field(
        default=0,
        ge=0,
        description="å—æ•°é‡"
    )
    processed_at: Optional[datetime] = Field(
        default=None,
        description="å¤„ç†å®Œæˆæ—¶é—´"
    )
    
    # å¤–é”®
    user_id: int = Field(
        foreign_key="users.id",
        index=True,
        description="æ‰€å±ç”¨æˆ·ID"
    )
    
    # å…³ç³»
    user: "User" = Relationship(back_populates="documents")
    chunks: List["DocumentChunk"] = Relationship(
        back_populates="document",
        sa_relationship_kwargs={"cascade": "all, delete-orphan"}
    )

class DocumentChunk(BaseModel, table=True):
    """æ–‡æ¡£å—æ¨¡å‹"""
    __tablename__ = "document_chunks"
    
    content: str = Field(
        description="å—å†…å®¹"
    )
    chunk_index: int = Field(
        ge=0,
        index=True,
        description="å—ç´¢å¼•"
    )
    page_number: Optional[int] = Field(
        default=None,
        ge=1,
        description="é¡µç "
    )
    token_count: int = Field(
        ge=0,
        description="ä»¤ç‰Œæ•°é‡"
    )
    character_count: int = Field(
        ge=0,
        description="å­—ç¬¦æ•°é‡"
    )
    vector_id: Optional[str] = Field(
        default=None,
        max_length=100,
        index=True,
        description="å‘é‡æ•°æ®åº“ä¸­çš„ID"
    )
    embedding_model: Optional[str] = Field(
        default=None,
        max_length=100,
        description="ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹"
    )
    
    # å¤–é”®
    document_id: int = Field(
        foreign_key="documents.id",
        index=True,
        description="æ‰€å±æ–‡æ¡£ID"
    )
    
    # å…³ç³»
    document: Document = Relationship(back_populates="chunks")

# è¯·æ±‚/å“åº”æ¨¡å‹
class DocumentCreate(DocumentBase):
    """åˆ›å»ºæ–‡æ¡£çš„è¯·æ±‚æ¨¡å‹"""
    user_id: int

class DocumentRead(DocumentBase):
    """è¯»å–æ–‡æ¡£çš„å“åº”æ¨¡å‹"""
    id: int
    uuid: str
    error_message: Optional[str]
    page_count: Optional[int]
    word_count: Optional[int]
    chunk_count: Optional[int]
    processed_at: Optional[datetime]
    user_id: int
    created_at: datetime
    updated_at: Optional[datetime]

class DocumentUpdate(SQLModel):
    """æ›´æ–°æ–‡æ¡£çš„è¯·æ±‚æ¨¡å‹"""
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[DocumentStatus] = None
    error_message: Optional[str] = None
```

**åˆ›å»º `src/models/query.py`ï¼š**
```python
from sqlmodel import SQLModel, Field, Relationship
from typing import Optional, List, Dict, Any
from enum import Enum
from datetime import datetime
from .base import BaseModel

class QueryStatus(str, Enum):
    """æŸ¥è¯¢çŠ¶æ€æšä¸¾"""
    SUCCESS = "success"
    FAILED = "failed"
    TIMEOUT = "timeout"

class QueryHistory(BaseModel, table=True):
    """æŸ¥è¯¢å†å²æ¨¡å‹"""
    __tablename__ = "query_history"
    
    query_text: str = Field(
        max_length=2000,
        description="æŸ¥è¯¢æ–‡æœ¬"
    )
    query_hash: str = Field(
        max_length=64,
        index=True,
        description="æŸ¥è¯¢æ–‡æœ¬çš„å“ˆå¸Œå€¼"
    )
    retrieved_chunks: Optional[str] = Field(
        default=None,
        description="æ£€ç´¢åˆ°çš„å—ï¼ˆJSONæ ¼å¼ï¼‰"
    )
    response_text: Optional[str] = Field(
        default=None,
        description="å“åº”æ–‡æœ¬"
    )
    
    # æ€§èƒ½æŒ‡æ ‡
    retrieval_time_ms: Optional[int] = Field(
        default=None,
        ge=0,
        description="æ£€ç´¢è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"
    )
    generation_time_ms: Optional[int] = Field(
        default=None,
        ge=0,
        description="ç”Ÿæˆè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"
    )
    total_time_ms: Optional[int] = Field(
        default=None,
        ge=0,
        description="æ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"
    )
    
    # çŠ¶æ€å’Œè¯„åˆ†
    status: QueryStatus = Field(
        default=QueryStatus.SUCCESS,
        index=True,
        description="æŸ¥è¯¢çŠ¶æ€"
    )
    user_rating: Optional[int] = Field(
        default=None,
        ge=1,
        le=5,
        description="ç”¨æˆ·è¯„åˆ†ï¼ˆ1-5æ˜Ÿï¼‰"
    )
    user_feedback: Optional[str] = Field(
        default=None,
        max_length=1000,
        description="ç”¨æˆ·åé¦ˆ"
    )
    
    # å¤–é”®
    user_id: int = Field(
        foreign_key="users.id",
        index=True,
        description="ç”¨æˆ·ID"
    )
    
    # å…³ç³»
    user: "User" = Relationship(back_populates="queries")

class SystemConfig(BaseModel, table=True):
    """ç³»ç»Ÿé…ç½®æ¨¡å‹"""
    __tablename__ = "system_config"
    
    key: str = Field(
        max_length=100,
        unique=True,
        index=True,
        description="é…ç½®é”®"
    )
    value: str = Field(
        max_length=1000,
        description="é…ç½®å€¼"
    )
    description: Optional[str] = Field(
        default=None,
        max_length=500,
        description="é…ç½®æè¿°"
    )
    is_active: bool = Field(
        default=True,
        description="æ˜¯å¦æ¿€æ´»"
    )
```

**åˆ›å»º `src/models/__init__.py`ï¼š**
```python
"""æ•°æ®æ¨¡å‹æ¨¡å—"""

from .base import BaseModel
from .user import User, UserCreate, UserRead, UserUpdate, UserRole
from .document import (
    Document,
    DocumentChunk,
    DocumentCreate,
    DocumentRead,
    DocumentUpdate,
    DocumentStatus
)
from .query import QueryHistory, SystemConfig, QueryStatus

__all__ = [
    "BaseModel",
    "User",
    "UserCreate",
    "UserRead",
    "UserUpdate",
    "UserRole",
    "Document",
    "DocumentChunk",
    "DocumentCreate",
    "DocumentRead",
    "DocumentUpdate",
    "DocumentStatus",
    "QueryHistory",
    "SystemConfig",
    "QueryStatus",
]
```

### 1.2 æµ‹è¯•æ•°æ®æ¨¡å‹

**åˆ›å»º `test_models.py`ï¼š**
```python
"""æµ‹è¯•æ•°æ®æ¨¡å‹"""

from src.models import User, Document, DocumentChunk, UserRole, DocumentStatus
from datetime import datetime

def test_user_model():
    """æµ‹è¯•ç”¨æˆ·æ¨¡å‹"""
    # åˆ›å»ºç”¨æˆ·å®ä¾‹
    user = User(
        username="test_user",
        email="test@example.com",
        password_hash="hashed_password",
        role=UserRole.USER
    )
    
    print(f"ç”¨æˆ·æ¨¡å‹åˆ›å»ºæˆåŠŸ:")
    print(f"  ç”¨æˆ·å: {user.username}")
    print(f"  é‚®ç®±: {user.email}")
    print(f"  è§’è‰²: {user.role}")
    print(f"  UUID: {user.uuid}")
    print(f"  åˆ›å»ºæ—¶é—´: {user.created_at}")
    
    # æµ‹è¯•JSONåºåˆ—åŒ–
    user_dict = user.model_dump()
    print(f"\nåºåˆ—åŒ–ç»“æœ: {user_dict}")
    
    return user

def test_document_model():
    """æµ‹è¯•æ–‡æ¡£æ¨¡å‹"""
    document = Document(
        title="æµ‹è¯•æ–‡æ¡£",
        description="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
        original_filename="test.pdf",
        file_path="/path/to/test.pdf",
        file_size=1024000,
        mime_type="application/pdf",
        file_hash="abc123def456",
        status=DocumentStatus.PENDING,
        user_id=1
    )
    
    print(f"\næ–‡æ¡£æ¨¡å‹åˆ›å»ºæˆåŠŸ:")
    print(f"  æ ‡é¢˜: {document.title}")
    print(f"  æ–‡ä»¶å¤§å°: {document.file_size} bytes")
    print(f"  çŠ¶æ€: {document.status}")
    print(f"  UUID: {document.uuid}")
    
    return document

def test_document_chunk_model():
    """æµ‹è¯•æ–‡æ¡£å—æ¨¡å‹"""
    chunk = DocumentChunk(
        content="è¿™æ˜¯æ–‡æ¡£çš„ç¬¬ä¸€ä¸ªå—å†…å®¹",
        chunk_index=0,
        page_number=1,
        token_count=50,
        character_count=200,
        document_id=1
    )
    
    print(f"\næ–‡æ¡£å—æ¨¡å‹åˆ›å»ºæˆåŠŸ:")
    print(f"  å†…å®¹é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
    print(f"  å—ç´¢å¼•: {chunk.chunk_index}")
    print(f"  ä»¤ç‰Œæ•°: {chunk.token_count}")
    print(f"  UUID: {chunk.uuid}")
    
    return chunk

if __name__ == "__main__":
    print("=== æµ‹è¯•æ•°æ®æ¨¡å‹ ===")
    
    user = test_user_model()
    document = test_document_model()
    chunk = test_document_chunk_model()
    
    print("\nâœ… æ‰€æœ‰æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_models.py
```

---

## ğŸ”¬ ExerciseäºŒï¼šæ•°æ®åº“è¿æ¥é…ç½®

### 2.1 åˆ›å»ºæ•°æ®åº“é…ç½®

**åˆ›å»º `src/database/__init__.py`ï¼š**
```python
"""æ•°æ®åº“æ¨¡å—"""

from .connection import engine, get_session, get_async_session
from .config import DatabaseConfig

__all__ = [
    "engine",
    "get_session",
    "get_async_session",
    "DatabaseConfig",
]
```

**åˆ›å»º `src/database/config.py`ï¼š**
```python
"""æ•°æ®åº“é…ç½®"""

from pydantic import BaseSettings, Field
from typing import Optional

class DatabaseConfig(BaseSettings):
    """æ•°æ®åº“é…ç½®ç±»"""
    
    # åŸºç¡€è¿æ¥é…ç½®
    database_url: str = Field(
        default="postgresql://rag:ragpass@localhost:5432/rag_db",
        description="æ•°æ®åº“è¿æ¥URL"
    )
    
    # è¿æ¥æ± é…ç½®
    pool_size: int = Field(
        default=10,
        ge=1,
        le=50,
        description="è¿æ¥æ± å¤§å°"
    )
    max_overflow: int = Field(
        default=20,
        ge=0,
        le=100,
        description="æœ€å¤§æº¢å‡ºè¿æ¥æ•°"
    )
    pool_timeout: int = Field(
        default=30,
        ge=1,
        le=300,
        description="è¿æ¥æ± è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
    )
    pool_recycle: int = Field(
        default=3600,
        ge=300,
        le=86400,
        description="è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰"
    )
    pool_pre_ping: bool = Field(
        default=True,
        description="è¿æ¥å‰æ˜¯å¦pingæµ‹è¯•"
    )
    
    # è°ƒè¯•é…ç½®
    echo: bool = Field(
        default=False,
        description="æ˜¯å¦è¾“å‡ºSQLè¯­å¥"
    )
    echo_pool: bool = Field(
        default=False,
        description="æ˜¯å¦è¾“å‡ºè¿æ¥æ± æ—¥å¿—"
    )
    
    # è¿æ¥å‚æ•°
    connect_timeout: int = Field(
        default=10,
        ge=1,
        le=60,
        description="è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
    )
    statement_timeout: str = Field(
        default="30s",
        description="è¯­å¥è¶…æ—¶æ—¶é—´"
    )
    lock_timeout: str = Field(
        default="10s",
        description="é”è¶…æ—¶æ—¶é—´"
    )
    
    class Config:
        env_prefix = "DB_"
        env_file = ".env"

# å…¨å±€é…ç½®å®ä¾‹
db_config = DatabaseConfig()
```

**åˆ›å»º `src/database/connection.py`ï¼š**
```python
"""æ•°æ®åº“è¿æ¥ç®¡ç†"""

from sqlmodel import create_engine, Session
from sqlalchemy.pool import QueuePool
from sqlalchemy import event
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager
from typing import Generator
import logging

from .config import db_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºåŒæ­¥å¼•æ“
engine = create_engine(
    db_config.database_url,
    echo=db_config.echo,
    echo_pool=db_config.echo_pool,
    poolclass=QueuePool,
    pool_size=db_config.pool_size,
    max_overflow=db_config.max_overflow,
    pool_timeout=db_config.pool_timeout,
    pool_recycle=db_config.pool_recycle,
    pool_pre_ping=db_config.pool_pre_ping,
    connect_args={
        "connect_timeout": db_config.connect_timeout,
        "application_name": "rag_system",
        "options": f"-c timezone=UTC -c statement_timeout={db_config.statement_timeout} -c lock_timeout={db_config.lock_timeout}"
    }
)

# åˆ›å»ºå¼‚æ­¥å¼•æ“
async_database_url = db_config.database_url.replace("postgresql://", "postgresql+asyncpg://")
async_engine = create_async_engine(
    async_database_url,
    echo=db_config.echo,
    pool_size=db_config.pool_size,
    max_overflow=db_config.max_overflow,
    pool_timeout=db_config.pool_timeout,
    pool_recycle=db_config.pool_recycle,
    pool_pre_ping=db_config.pool_pre_ping
)

# å¼‚æ­¥ä¼šè¯å·¥å‚
AsyncSessionLocal = sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# è¿æ¥äº‹ä»¶ç›‘å¬å™¨
@event.listens_for(engine, "connect")
def set_postgresql_pragma(dbapi_connection, connection_record):
    """è®¾ç½®PostgreSQLè¿æ¥å‚æ•°"""
    logger.debug("è®¾ç½®PostgreSQLè¿æ¥å‚æ•°")

@event.listens_for(engine.pool, "connect")
def receive_connect(dbapi_connection, connection_record):
    """è¿æ¥å»ºç«‹äº‹ä»¶"""
    logger.debug("å»ºç«‹æ–°çš„æ•°æ®åº“è¿æ¥")

@event.listens_for(engine.pool, "checkout")
def receive_checkout(dbapi_connection, connection_record, connection_proxy):
    """è¿æ¥æ£€å‡ºäº‹ä»¶"""
    logger.debug("ä»è¿æ¥æ± æ£€å‡ºè¿æ¥")

@event.listens_for(engine.pool, "checkin")
def receive_checkin(dbapi_connection, connection_record):
    """è¿æ¥æ£€å…¥äº‹ä»¶"""
    logger.debug("è¿æ¥è¿”å›è¿æ¥æ± ")

# åŒæ­¥ä¼šè¯ç®¡ç†
def get_session() -> Generator[Session, None, None]:
    """è·å–æ•°æ®åº“ä¼šè¯ï¼ˆä¾èµ–æ³¨å…¥ç”¨ï¼‰"""
    with Session(engine) as session:
        try:
            yield session
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()

@contextmanager
def get_db_session():
    """è·å–æ•°æ®åº“ä¼šè¯ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
    session = Session(engine)
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

# å¼‚æ­¥ä¼šè¯ç®¡ç†
async def get_async_session() -> AsyncSession:
    """è·å–å¼‚æ­¥æ•°æ®åº“ä¼šè¯"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

# è¿æ¥æ± çŠ¶æ€ç›‘æ§
def get_pool_status():
    """è·å–è¿æ¥æ± çŠ¶æ€"""
    pool = engine.pool
    return {
        "pool_size": pool.size(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "invalidated": pool.invalidated()
    }

# æ•°æ®åº“å¥åº·æ£€æŸ¥
def check_database_health() -> bool:
    """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
    try:
        with get_db_session() as session:
            session.exec("SELECT 1")
        return True
    except Exception as e:
        logger.error(f"æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False
```

### 2.2 åˆ›å»ºæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬

**åˆ›å»º `src/database/init_db.py`ï¼š**
```python
"""æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬"""

from sqlmodel import SQLModel
from .connection import engine
from src.models import *  # å¯¼å…¥æ‰€æœ‰æ¨¡å‹
import logging

logger = logging.getLogger(__name__)

def create_tables():
    """åˆ›å»ºæ‰€æœ‰è¡¨"""
    try:
        logger.info("å¼€å§‹åˆ›å»ºæ•°æ®åº“è¡¨...")
        SQLModel.metadata.create_all(engine)
        logger.info("âœ… æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
        raise

def drop_tables():
    """åˆ é™¤æ‰€æœ‰è¡¨ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
    try:
        logger.warning("å¼€å§‹åˆ é™¤æ•°æ®åº“è¡¨...")
        SQLModel.metadata.drop_all(engine)
        logger.info("âœ… æ•°æ®åº“è¡¨åˆ é™¤æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤æ•°æ®åº“è¡¨å¤±è´¥: {e}")
        raise

def reset_database():
    """é‡ç½®æ•°æ®åº“ï¼ˆåˆ é™¤å¹¶é‡æ–°åˆ›å»ºæ‰€æœ‰è¡¨ï¼‰"""
    logger.warning("å¼€å§‹é‡ç½®æ•°æ®åº“...")
    drop_tables()
    create_tables()
    logger.info("âœ… æ•°æ®åº“é‡ç½®å®Œæˆ")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "create":
            create_tables()
        elif command == "drop":
            drop_tables()
        elif command == "reset":
            reset_database()
        else:
            print("ç”¨æ³•: python init_db.py [create|drop|reset]")
    else:
        create_tables()
```

### 2.3 æµ‹è¯•æ•°æ®åº“è¿æ¥

**åˆ›å»º `test_database.py`ï¼š**
```python
"""æµ‹è¯•æ•°æ®åº“è¿æ¥"""

from src.database import engine, get_session, get_pool_status, check_database_health
from src.models import User, UserRole
from sqlmodel import select
import time

def test_basic_connection():
    """æµ‹è¯•åŸºç¡€è¿æ¥"""
    print("=== æµ‹è¯•åŸºç¡€æ•°æ®åº“è¿æ¥ ===")
    
    try:
        with get_session() as session:
            result = session.exec(select(1)).first()
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼ŒæŸ¥è¯¢ç»“æœ: {result}")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False
    
    return True

def test_pool_status():
    """æµ‹è¯•è¿æ¥æ± çŠ¶æ€"""
    print("\n=== æµ‹è¯•è¿æ¥æ± çŠ¶æ€ ===")
    
    status = get_pool_status()
    print(f"è¿æ¥æ± å¤§å°: {status['pool_size']}")
    print(f"å·²æ£€å‡ºè¿æ¥: {status['checked_out']}")
    print(f"æº¢å‡ºè¿æ¥: {status['overflow']}")
    print(f"æ— æ•ˆè¿æ¥: {status['invalidated']}")

def test_crud_operations():
    """æµ‹è¯•CRUDæ“ä½œ"""
    print("\n=== æµ‹è¯•CRUDæ“ä½œ ===")
    
    try:
        with get_session() as session:
            # åˆ›å»ºç”¨æˆ·
            user = User(
                username="test_user",
                email="test@example.com",
                password_hash="hashed_password",
                role=UserRole.USER
            )
            
            session.add(user)
            session.commit()
            session.refresh(user)
            
            print(f"âœ… ç”¨æˆ·åˆ›å»ºæˆåŠŸï¼ŒID: {user.id}")
            
            # æŸ¥è¯¢ç”¨æˆ·
            statement = select(User).where(User.username == "test_user")
            found_user = session.exec(statement).first()
            
            if found_user:
                print(f"âœ… ç”¨æˆ·æŸ¥è¯¢æˆåŠŸ: {found_user.username}")
            else:
                print("âŒ ç”¨æˆ·æŸ¥è¯¢å¤±è´¥")
                return False
            
            # æ›´æ–°ç”¨æˆ·
            found_user.email = "updated@example.com"
            session.add(found_user)
            session.commit()
            
            print(f"âœ… ç”¨æˆ·æ›´æ–°æˆåŠŸï¼Œæ–°é‚®ç®±: {found_user.email}")
            
            # åˆ é™¤ç”¨æˆ·
            session.delete(found_user)
            session.commit()
            
            print("âœ… ç”¨æˆ·åˆ é™¤æˆåŠŸ")
            
    except Exception as e:
        print(f"âŒ CRUDæ“ä½œå¤±è´¥: {e}")
        return False
    
    return True

def test_concurrent_connections():
    """æµ‹è¯•å¹¶å‘è¿æ¥"""
    print("\n=== æµ‹è¯•å¹¶å‘è¿æ¥ ===")
    
    import threading
    import time
    
    results = []
    
    def worker(worker_id):
        try:
            with get_session() as session:
                # æ¨¡æ‹Ÿä¸€äº›æ•°æ®åº“æ“ä½œ
                time.sleep(0.1)
                result = session.exec(select(1)).first()
                results.append(f"Worker {worker_id}: {result}")
        except Exception as e:
            results.append(f"Worker {worker_id} failed: {e}")
    
    # åˆ›å»ºå¤šä¸ªçº¿ç¨‹
    threads = []
    for i in range(5):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)
        thread.start()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    print(f"å¹¶å‘æµ‹è¯•ç»“æœ:")
    for result in results:
        print(f"  {result}")
    
    return len(results) == 5

def test_health_check():
    """æµ‹è¯•å¥åº·æ£€æŸ¥"""
    print("\n=== æµ‹è¯•å¥åº·æ£€æŸ¥ ===")
    
    is_healthy = check_database_health()
    if is_healthy:
        print("âœ… æ•°æ®åº“å¥åº·çŠ¶æ€è‰¯å¥½")
    else:
        print("âŒ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥")
    
    return is_healthy

if __name__ == "__main__":
    print("å¼€å§‹æ•°æ®åº“è¿æ¥æµ‹è¯•...")
    
    tests = [
        ("åŸºç¡€è¿æ¥", test_basic_connection),
        ("è¿æ¥æ± çŠ¶æ€", test_pool_status),
        ("CRUDæ“ä½œ", test_crud_operations),
        ("å¹¶å‘è¿æ¥", test_concurrent_connections),
        ("å¥åº·æ£€æŸ¥", test_health_check),
    ]
    
    passed = 0
    for name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"æ‰§è¡Œæµ‹è¯•: {name}")
        print(f"{'='*50}")
        
        try:
            if test_func():
                passed += 1
                print(f"âœ… {name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print(f"\n{'='*50}")
    print(f"æµ‹è¯•æ€»ç»“: {passed}/{len(tests)} ä¸ªæµ‹è¯•é€šè¿‡")
    print(f"{'='*50}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
# é¦–å…ˆåˆ›å»ºæ•°æ®åº“è¡¨
python -m src.database.init_db create

# è¿è¡Œè¿æ¥æµ‹è¯•
python test_database.py
```

---

## ğŸ”¬ Exerciseä¸‰ï¼šæ•°æ®ä»“åº“æ¨¡å¼å®ç°

### 3.1 åˆ›å»ºåŸºç¡€ä»“åº“ç±»

**åˆ›å»º `src/repositories/__init__.py`ï¼š**
```python
"""æ•°æ®ä»“åº“æ¨¡å—"""

from .base import BaseRepository
from .user import UserRepository
from .document import DocumentRepository
from .query import QueryRepository

__all__ = [
    "BaseRepository",
    "UserRepository",
    "DocumentRepository",
    "QueryRepository",
]
```

**åˆ›å»º `src/repositories/base.py`ï¼š**
```python
"""åŸºç¡€ä»“åº“ç±»"""

from typing import Type, TypeVar, Generic, List, Optional, Dict, Any
from sqlmodel import SQLModel, Session, select
from sqlalchemy.orm import selectinload
from datetime import datetime

ModelType = TypeVar("ModelType", bound=SQLModel)

class BaseRepository(Generic[ModelType]):
    """åŸºç¡€ä»“åº“ç±»"""
    
    def __init__(self, model: Type[ModelType], session: Session):
        self.model = model
        self.session = session
    
    def create(self, obj_in: ModelType) -> ModelType:
        """åˆ›å»ºå¯¹è±¡"""
        self.session.add(obj_in)
        self.session.commit()
        self.session.refresh(obj_in)
        return obj_in
    
    def get(self, id: int) -> Optional[ModelType]:
        """æ ¹æ®IDè·å–å¯¹è±¡"""
        return self.session.get(self.model, id)
    
    def get_by_uuid(self, uuid: str) -> Optional[ModelType]:
        """æ ¹æ®UUIDè·å–å¯¹è±¡"""
        statement = select(self.model).where(self.model.uuid == uuid)
        return self.session.exec(statement).first()
    
    def get_multi(
        self, 
        skip: int = 0, 
        limit: int = 100,
        order_by: Optional[str] = None
    ) -> List[ModelType]:
        """è·å–å¤šä¸ªå¯¹è±¡"""
        statement = select(self.model).offset(skip).limit(limit)
        
        if order_by:
            if hasattr(self.model, order_by):
                statement = statement.order_by(getattr(self.model, order_by))
        
        return self.session.exec(statement).all()
    
    def update(self, db_obj: ModelType, obj_in: Dict[str, Any]) -> ModelType:
        """æ›´æ–°å¯¹è±¡"""
        for field, value in obj_in.items():
            if hasattr(db_obj, field) and value is not None:
                setattr(db_obj, field, value)
        
        # è‡ªåŠ¨æ›´æ–°æ—¶é—´æˆ³
        if hasattr(db_obj, 'updated_at'):
            db_obj.updated_at = datetime.utcnow()
        
        self.session.add(db_obj)
        self.session.commit()
        self.session.refresh(db_obj)
        return db_obj
    
    def delete(self, id: int) -> bool:
        """åˆ é™¤å¯¹è±¡"""
        obj = self.session.get(self.model, id)
        if obj:
            self.session.delete(obj)
            self.session.commit()
            return True
        return False
    
    def count(self, **filters) -> int:
        """ç»Ÿè®¡å¯¹è±¡æ•°é‡"""
        statement = select(self.model)
        
        for field, value in filters.items():
            if hasattr(self.model, field):
                statement = statement.where(getattr(self.model, field) == value)
        
        return len(self.session.exec(statement).all())
    
    def exists(self, **filters) -> bool:
        """æ£€æŸ¥å¯¹è±¡æ˜¯å¦å­˜åœ¨"""
        statement = select(self.model)
        
        for field, value in filters.items():
            if hasattr(self.model, field):
                statement = statement.where(getattr(self.model, field) == value)
        
        return self.session.exec(statement).first() is not None
    
    def bulk_create(self, objects: List[ModelType]) -> List[ModelType]:
        """æ‰¹é‡åˆ›å»ºå¯¹è±¡"""
        self.session.add_all(objects)
        self.session.commit()
        
        for obj in objects:
            self.session.refresh(obj)
        
        return objects
    
    def bulk_update(self, updates: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ›´æ–°å¯¹è±¡"""
        count = 0
        for update_data in updates:
            obj_id = update_data.pop('id', None)
            if obj_id:
                obj = self.get(obj_id)
                if obj:
                    self.update(obj, update_data)
                    count += 1
        return count
```

**åˆ›å»º `src/repositories/user.py`ï¼š**
```python
"""ç”¨æˆ·ä»“åº“"""

from typing import Optional, List
from sqlmodel import Session, select
from sqlalchemy.orm import selectinload

from .base import BaseRepository
from src.models import User, UserRole

class UserRepository(BaseRepository[User]):
    """ç”¨æˆ·ä»“åº“ç±»"""
    
    def __init__(self, session: Session):
        super().__init__(User, session)
    
    def get_by_username(self, username: str) -> Optional[User]:
        """æ ¹æ®ç”¨æˆ·åè·å–ç”¨æˆ·"""
        statement = select(User).where(User.username == username)
        return self.session.exec(statement).first()
    
    def get_by_email(self, email: str) -> Optional[User]:
        """æ ¹æ®é‚®ç®±è·å–ç”¨æˆ·"""
        statement = select(User).where(User.email == email)
        return self.session.exec(statement).first()
    
    def get_active_users(self, skip: int = 0, limit: int = 100) -> List[User]:
        """è·å–æ´»è·ƒç”¨æˆ·"""
        statement = (
            select(User)
            .where(User.is_active == True)
            .offset(skip)
            .limit(limit)
            .order_by(User.created_at.desc())
        )
        return self.session.exec(statement).all()
    
    def get_users_by_role(self, role: UserRole) -> List[User]:
        """æ ¹æ®è§’è‰²è·å–ç”¨æˆ·"""
        statement = select(User).where(User.role == role)
        return self.session.exec(statement).all()
    
    def get_with_documents(self, user_id: int) -> Optional[User]:
        """è·å–ç”¨æˆ·åŠå…¶æ–‡æ¡£"""
        statement = (
            select(User)
            .options(selectinload(User.documents))
            .where(User.id == user_id)
        )
        return self.session.exec(statement).first()
    
    def get_with_queries(self, user_id: int) -> Optional[User]:
        """è·å–ç”¨æˆ·åŠå…¶æŸ¥è¯¢å†å²"""
        statement = (
            select(User)
            .options(selectinload(User.queries))
            .where(User.id == user_id)
        )
        return self.session.exec(statement).first()
    
    def update_last_login(self, user_id: int) -> bool:
        """æ›´æ–°æœ€åç™»å½•æ—¶é—´"""
        user = self.get(user_id)
        if user:
            from datetime import datetime
            user.last_login_at = datetime.utcnow()
            self.session.add(user)
            self.session.commit()
            return True
        return False
    
    def deactivate_user(self, user_id: int) -> bool:
        """åœç”¨ç”¨æˆ·"""
        user = self.get(user_id)
        if user:
            user.is_active = False
            self.session.add(user)
            self.session.commit()
            return True
        return False
    
    def search_users(self, query: str, limit: int = 10) -> List[User]:
        """æœç´¢ç”¨æˆ·"""
        statement = (
            select(User)
            .where(
                (User.username.ilike(f"%{query}%")) |
                (User.email.ilike(f"%{query}%"))
            )
            .limit(limit)
        )
        return self.session.exec(statement).all()
```

**åˆ›å»º `src/repositories/document.py`ï¼š**
```python
"""æ–‡æ¡£ä»“åº“"""

from typing import Optional, List
from sqlmodel import Session, select
from sqlalchemy.orm import selectinload
from datetime import datetime

from .base import BaseRepository
from src.models import Document, DocumentChunk, DocumentStatus

class DocumentRepository(BaseRepository[Document]):
    """æ–‡æ¡£ä»“åº“ç±»"""
    
    def __init__(self, session: Session):
        super().__init__(Document, session)
    
    def get_by_user(
        self, 
        user_id: int, 
        status: Optional[DocumentStatus] = None,
        skip: int = 0,
        limit: int = 100
    ) -> List[Document]:
        """è·å–ç”¨æˆ·çš„æ–‡æ¡£"""
        statement = (
            select(Document)
            .where(Document.user_id == user_id)
            .offset(skip)
            .limit(limit)
            .order_by(Document.created_at.desc())
        )
        
        if status:
            statement = statement.where(Document.status == status)
        
        return self.session.exec(statement).all()
    
    def get_with_chunks(self, document_id: int) -> Optional[Document]:
        """è·å–æ–‡æ¡£åŠå…¶å—"""
        statement = (
            select(Document)
            .options(selectinload(Document.chunks))
            .where(Document.id == document_id)
        )
        return self.session.exec(statement).first()
    
    def get_by_file_hash(self, file_hash: str) -> Optional[Document]:
        """æ ¹æ®æ–‡ä»¶å“ˆå¸Œè·å–æ–‡æ¡£"""
        statement = select(Document).where(Document.file_hash == file_hash)
        return self.session.exec(statement).first()
    
    def get_by_status(self, status: DocumentStatus) -> List[Document]:
        """æ ¹æ®çŠ¶æ€è·å–æ–‡æ¡£"""
        statement = (
            select(Document)
            .where(Document.status == status)
            .order_by(Document.created_at.asc())
        )
        return self.session.exec(statement).all()
    
    def update_status(
        self, 
        document_id: int, 
        status: DocumentStatus, 
        error_message: Optional[str] = None
    ) -> bool:
        """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
        document = self.get(document_id)
        if document:
            document.status = status
            if error_message:
                document.error_message = error_message
            if status == DocumentStatus.COMPLETED:
                document.processed_at = datetime.utcnow()
            
            self.session.add(document)
            self.session.commit()
            return True
        return False
    
    def update_stats(
        self, 
        document_id: int, 
        page_count: Optional[int] = None,
        word_count: Optional[int] = None,
        chunk_count: Optional[int] = None
    ) -> bool:
        """æ›´æ–°æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        document = self.get(document_id)
        if document:
            if page_count is not None:
                document.page_count = page_count
            if word_count is not None:
                document.word_count = word_count
            if chunk_count is not None:
                document.chunk_count = chunk_count
            
            self.session.add(document)
            self.session.commit()
            return True
        return False
    
    def search_documents(self, user_id: int, query: str, limit: int = 10) -> List[Document]:
        """æœç´¢ç”¨æˆ·çš„æ–‡æ¡£"""
        statement = (
            select(Document)
            .where(Document.user_id == user_id)
            .where(
                (Document.title.ilike(f"%{query}%")) |
                (Document.description.ilike(f"%{query}%"))
            )
            .limit(limit)
        )
        return self.session.exec(statement).all()
    
    def get_recent_documents(self, user_id: int, days: int = 7, limit: int = 10) -> List[Document]:
        """è·å–ç”¨æˆ·æœ€è¿‘çš„æ–‡æ¡£"""
        from datetime import timedelta
        
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        statement = (
            select(Document)
            .where(Document.user_id == user_id)
            .where(Document.created_at >= cutoff_date)
            .order_by(Document.created_at.desc())
            .limit(limit)
        )
        return self.session.exec(statement).all()

class DocumentChunkRepository(BaseRepository[DocumentChunk]):
    """æ–‡æ¡£å—ä»“åº“ç±»"""
    
    def __init__(self, session: Session):
        super().__init__(DocumentChunk, session)
    
    def get_by_document(
        self, 
        document_id: int,
        skip: int = 0,
        limit: int = 100
    ) -> List[DocumentChunk]:
        """è·å–æ–‡æ¡£çš„æ‰€æœ‰å—"""
        statement = (
            select(DocumentChunk)
            .where(DocumentChunk.document_id == document_id)
            .order_by(DocumentChunk.chunk_index.asc())
            .offset(skip)
            .limit(limit)
        )
        return self.session.exec(statement).all()
    
    def get_by_vector_id(self, vector_id: str) -> Optional[DocumentChunk]:
        """æ ¹æ®å‘é‡IDè·å–å—"""
        statement = select(DocumentChunk).where(DocumentChunk.vector_id == vector_id)
        return self.session.exec(statement).first()
    
    def update_vector_info(
        self, 
        chunk_id: int, 
        vector_id: str, 
        embedding_model: str
    ) -> bool:
        """æ›´æ–°å—çš„å‘é‡ä¿¡æ¯"""
        chunk = self.get(chunk_id)
        if chunk:
            chunk.vector_id = vector_id
            chunk.embedding_model = embedding_model
            self.session.add(chunk)
            self.session.commit()
            return True
        return False
    
    def bulk_create_chunks(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """æ‰¹é‡åˆ›å»ºæ–‡æ¡£å—"""
        return self.bulk_create(chunks)
    
    def search_chunks(self, document_id: int, query: str, limit: int = 10) -> List[DocumentChunk]:
        """æœç´¢æ–‡æ¡£å—"""
        statement = (
            select(DocumentChunk)
            .where(DocumentChunk.document_id == document_id)
            .where(DocumentChunk.content.ilike(f"%{query}%"))
            .limit(limit)
        )
        return self.session.exec(statement).all()
```

### 3.2 æµ‹è¯•ä»“åº“æ¨¡å¼

**åˆ›å»º `test_repositories.py`ï¼š**
```python
"""æµ‹è¯•ä»“åº“æ¨¡å¼"""

from src.database import get_db_session
from src.repositories import UserRepository, DocumentRepository, DocumentChunkRepository
from src.models import User, Document, DocumentChunk, UserRole, DocumentStatus
import hashlib

def test_user_repository():
    """æµ‹è¯•ç”¨æˆ·ä»“åº“"""
    print("=== æµ‹è¯•ç”¨æˆ·ä»“åº“ ===")
    
    with get_db_session() as session:
        user_repo = UserRepository(session)
        
        # åˆ›å»ºç”¨æˆ·
        user = User(
            username="repo_test_user",
            email="repo_test@example.com",
            password_hash="hashed_password",
            role=UserRole.USER
        )
        
        created_user = user_repo.create(user)
        print(f"âœ… ç”¨æˆ·åˆ›å»ºæˆåŠŸï¼ŒID: {created_user.id}")
        
        # æ ¹æ®ç”¨æˆ·åæŸ¥è¯¢
        found_user = user_repo.get_by_username("repo_test_user")
        if found_user:
            print(f"âœ… æ ¹æ®ç”¨æˆ·åæŸ¥è¯¢æˆåŠŸ: {found_user.username}")
        
        # æ ¹æ®é‚®ç®±æŸ¥è¯¢
        found_user = user_repo.get_by_email("repo_test@example.com")
        if found_user:
            print(f"âœ… æ ¹æ®é‚®ç®±æŸ¥è¯¢æˆåŠŸ: {found_user.email}")
        
        # æ›´æ–°ç”¨æˆ·
        updated_user = user_repo.update(found_user, {
            "preferred_language": "en",
            "max_query_length": 2000
        })
        print(f"âœ… ç”¨æˆ·æ›´æ–°æˆåŠŸï¼Œé¦–é€‰è¯­è¨€: {updated_user.preferred_language}")
        
        # æ›´æ–°æœ€åç™»å½•æ—¶é—´
        user_repo.update_last_login(found_user.id)
        print("âœ… æœ€åç™»å½•æ—¶é—´æ›´æ–°æˆåŠŸ")
        
        # æœç´¢ç”¨æˆ·
        search_results = user_repo.search_users("repo_test")
        print(f"âœ… ç”¨æˆ·æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        user_repo.delete(found_user.id)
        print("âœ… æµ‹è¯•ç”¨æˆ·åˆ é™¤æˆåŠŸ")

def test_document_repository():
    """æµ‹è¯•æ–‡æ¡£ä»“åº“"""
    print("\n=== æµ‹è¯•æ–‡æ¡£ä»“åº“ ===")
    
    with get_db_session() as session:
        user_repo = UserRepository(session)
        doc_repo = DocumentRepository(session)
        
        # å…ˆåˆ›å»ºä¸€ä¸ªç”¨æˆ·
        user = User(
            username="doc_test_user",
            email="doc_test@example.com",
            password_hash="hashed_password"
        )
        created_user = user_repo.create(user)
        
        # åˆ›å»ºæ–‡æ¡£
        content = "è¿™æ˜¯æµ‹è¯•æ–‡æ¡£å†…å®¹"
        file_hash = hashlib.sha256(content.encode()).hexdigest()
        
        document = Document(
            title="æµ‹è¯•æ–‡æ¡£",
            description="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
            original_filename="test.txt",
            file_path="/tmp/test.txt",
            file_size=len(content.encode()),
            mime_type="text/plain",
            file_hash=file_hash,
            user_id=created_user.id
        )
        
        created_doc = doc_repo.create(document)
        print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸï¼ŒID: {created_doc.id}")
        
        # æ ¹æ®ç”¨æˆ·è·å–æ–‡æ¡£
        user_docs = doc_repo.get_by_user(created_user.id)
        print(f"âœ… ç”¨æˆ·æ–‡æ¡£æŸ¥è¯¢æˆåŠŸï¼Œæ‰¾åˆ° {len(user_docs)} ä¸ªæ–‡æ¡£")
        
        # æ ¹æ®æ–‡ä»¶å“ˆå¸ŒæŸ¥è¯¢
        found_doc = doc_repo.get_by_file_hash(file_hash)
        if found_doc:
            print(f"âœ… æ ¹æ®æ–‡ä»¶å“ˆå¸ŒæŸ¥è¯¢æˆåŠŸ: {found_doc.title}")
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        doc_repo.update_status(created_doc.id, DocumentStatus.PROCESSING)
        print("âœ… æ–‡æ¡£çŠ¶æ€æ›´æ–°æˆåŠŸ")
        
        # æ›´æ–°æ–‡æ¡£ç»Ÿè®¡
        doc_repo.update_stats(created_doc.id, page_count=1, word_count=100)
        print("âœ… æ–‡æ¡£ç»Ÿè®¡æ›´æ–°æˆåŠŸ")
        
        # æœç´¢æ–‡æ¡£
        search_results = doc_repo.search_documents(created_user.id, "æµ‹è¯•")
        print(f"âœ… æ–‡æ¡£æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        doc_repo.delete(created_doc.id)
        user_repo.delete(created_user.id)
        print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†æˆåŠŸ")

def test_document_chunk_repository():
    """æµ‹è¯•æ–‡æ¡£å—ä»“åº“"""
    print("\n=== æµ‹è¯•æ–‡æ¡£å—ä»“åº“ ===")
    
    with get_db_session() as session:
        user_repo = UserRepository(session)
        doc_repo = DocumentRepository(session)
        chunk_repo = DocumentChunkRepository(session)
        
        # åˆ›å»ºç”¨æˆ·å’Œæ–‡æ¡£
        user = User(
            username="chunk_test_user",
            email="chunk_test@example.com",
            password_hash="hashed_password"
        )
        created_user = user_repo.create(user)
        
        document = Document(
            title="å—æµ‹è¯•æ–‡æ¡£",
            original_filename="chunk_test.txt",
            file_path="/tmp/chunk_test.txt",
            file_size=1000,
            mime_type="text/plain",
            file_hash="chunk_test_hash",
            user_id=created_user.id
        )
        created_doc = doc_repo.create(document)
        
        # åˆ›å»ºæ–‡æ¡£å—
        chunks = []
        for i in range(3):
            chunk = DocumentChunk(
                content=f"è¿™æ˜¯ç¬¬ {i+1} ä¸ªæ–‡æ¡£å—çš„å†…å®¹",
                chunk_index=i,
                page_number=1,
                token_count=20,
                character_count=50,
                document_id=created_doc.id
            )
            chunks.append(chunk)
        
        # æ‰¹é‡åˆ›å»ºå—
        created_chunks = chunk_repo.bulk_create_chunks(chunks)
        print(f"âœ… æ‰¹é‡åˆ›å»ºæ–‡æ¡£å—æˆåŠŸï¼Œåˆ›å»ºäº† {len(created_chunks)} ä¸ªå—")
        
        # è·å–æ–‡æ¡£çš„æ‰€æœ‰å—
        doc_chunks = chunk_repo.get_by_document(created_doc.id)
        print(f"âœ… æ–‡æ¡£å—æŸ¥è¯¢æˆåŠŸï¼Œæ‰¾åˆ° {len(doc_chunks)} ä¸ªå—")
        
        # æ›´æ–°å—çš„å‘é‡ä¿¡æ¯
        if doc_chunks:
            chunk_repo.update_vector_info(
                doc_chunks[0].id, 
                "vector_123", 
                "bge-m3"
            )
            print("âœ… å—å‘é‡ä¿¡æ¯æ›´æ–°æˆåŠŸ")
        
        # æœç´¢å—
        search_results = chunk_repo.search_chunks(created_doc.id, "ç¬¬ 1 ä¸ª")
        print(f"âœ… å—æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        for chunk in created_chunks:
            chunk_repo.delete(chunk.id)
        doc_repo.delete(created_doc.id)
        user_repo.delete(created_user.id)
        print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†æˆåŠŸ")

if __name__ == "__main__":
    print("å¼€å§‹ä»“åº“æ¨¡å¼æµ‹è¯•...")
    
    try:
        test_user_repository()
        test_document_repository()
        test_document_chunk_repository()
        print("\nâœ… æ‰€æœ‰ä»“åº“æµ‹è¯•é€šè¿‡ï¼")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_repositories.py
```

---

## ğŸ”¬ Exerciseå››ï¼šAlembicæ•°æ®è¿ç§»

### 4.1 é…ç½®Alembic

**å®‰è£…Alembicï¼š**
```bash
uv add alembic
```

**åˆå§‹åŒ–Alembicï¼š**
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆå§‹åŒ–
alembic init alembic
```

**é…ç½® `alembic.ini`ï¼š**
```ini
# Alembic Config file

[alembic]
# path to migration scripts
script_location = alembic

# template used to generate migration file names
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
timezone = UTC

# max length of characters to apply to the "slug" field
truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
sourceless = false

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses
# os.pathsep. If this key is omitted entirely, it falls back to the legacy
# behavior of splitting on spaces and/or commas.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
output_encoding = utf-8

# æ•°æ®åº“è¿æ¥URLï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
sqlalchemy.url = postgresql://rag:ragpass@localhost:5432/rag_db

[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**é…ç½® `alembic/env.py`ï¼š**
```python
"""Alembicç¯å¢ƒé…ç½®"""

from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥æ¨¡å‹
from src.models import *
from sqlmodel import SQLModel

# Alembicé…ç½®å¯¹è±¡
config = context.config

# ä»ç¯å¢ƒå˜é‡è¯»å–æ•°æ®åº“URL
if not config.get_main_option("sqlalchemy.url"):
    from src.database.config import db_config
    config.set_main_option("sqlalchemy.url", db_config.database_url)

# é…ç½®æ—¥å¿—
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# è®¾ç½®ç›®æ ‡å…ƒæ•°æ®
target_metadata = SQLModel.metadata

def run_migrations_offline() -> None:
    """ç¦»çº¿æ¨¡å¼è¿è¡Œè¿ç§»"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    """åœ¨çº¿æ¨¡å¼è¿è¡Œè¿ç§»"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### 4.2 åˆ›å»ºåˆå§‹è¿ç§»

**ç”Ÿæˆåˆå§‹è¿ç§»ï¼š**
```bash
# ç”Ÿæˆåˆå§‹è¿ç§»æ–‡ä»¶
alembic revision --autogenerate -m "Initial migration"
```

**åº”ç”¨è¿ç§»ï¼š**
```bash
# åº”ç”¨è¿ç§»åˆ°æ•°æ®åº“
alembic upgrade head
```

**æŸ¥çœ‹è¿ç§»çŠ¶æ€ï¼š**
```bash
# æŸ¥çœ‹å½“å‰è¿ç§»çŠ¶æ€
alembic current

# æŸ¥çœ‹è¿ç§»å†å²
alembic history --verbose
```

### 4.3 åˆ›å»ºæ•°æ®è¿ç§»è„šæœ¬

**åˆ›å»º `scripts/migrate_data.py`ï¼š**
```python
"""æ•°æ®è¿ç§»è„šæœ¬"""

from src.database import get_db_session
from src.models import User, SystemConfig, UserRole
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_default_admin():
    """åˆ›å»ºé»˜è®¤ç®¡ç†å‘˜ç”¨æˆ·"""
    with get_db_session() as session:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç®¡ç†å‘˜
        from sqlmodel import select
        statement = select(User).where(User.role == UserRole.ADMIN)
        admin_exists = session.exec(statement).first()
        
        if not admin_exists:
            admin_user = User(
                username="admin",
                email="admin@example.com",
                password_hash="$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewdBPj3bp.gm.04i",  # password: admin123
                role=UserRole.ADMIN,
                is_active=True,
                preferred_language="zh",
                max_query_length=5000
            )
            
            session.add(admin_user)
            session.commit()
            logger.info("âœ… é»˜è®¤ç®¡ç†å‘˜ç”¨æˆ·åˆ›å»ºæˆåŠŸ")
        else:
            logger.info("â„¹ï¸ ç®¡ç†å‘˜ç”¨æˆ·å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")

def create_system_configs():
    """åˆ›å»ºç³»ç»Ÿé…ç½®"""
    with get_db_session() as session:
        configs = [
            {
                "key": "max_file_size",
                "value": "10485760",  # 10MB
                "description": "æœ€å¤§æ–‡ä»¶ä¸Šä¼ å¤§å°ï¼ˆå­—èŠ‚ï¼‰"
            },
            {
                "key": "supported_file_types",
                "value": "pdf,txt,docx,md",
                "description": "æ”¯æŒçš„æ–‡ä»¶ç±»å‹"
            },
            {
                "key": "max_chunk_size",
                "value": "1000",
                "description": "æœ€å¤§å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰"
            },
            {
                "key": "chunk_overlap",
                "value": "200",
                "description": "å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰"
            },
            {
                "key": "embedding_model",
                "value": "BAAI/bge-m3",
                "description": "é»˜è®¤åµŒå…¥æ¨¡å‹"
            },
            {
                "key": "llm_model",
                "value": "gpt-3.5-turbo",
                "description": "é»˜è®¤LLMæ¨¡å‹"
            },
            {
                "key": "max_query_length",
                "value": "2000",
                "description": "æœ€å¤§æŸ¥è¯¢é•¿åº¦"
            },
            {
                "key": "retrieval_top_k",
                "value": "5",
                "description": "æ£€ç´¢è¿”å›çš„æœ€å¤§ç»“æœæ•°"
            }
        ]
        
        from sqlmodel import select
        for config_data in configs:
            # æ£€æŸ¥é…ç½®æ˜¯å¦å·²å­˜åœ¨
            statement = select(SystemConfig).where(SystemConfig.key == config_data["key"])
            existing_config = session.exec(statement).first()
            
            if not existing_config:
                config = SystemConfig(**config_data)
                session.add(config)
                logger.info(f"âœ… ç³»ç»Ÿé…ç½®åˆ›å»ºæˆåŠŸ: {config_data['key']}")
            else:
                logger.info(f"â„¹ï¸ ç³»ç»Ÿé…ç½®å·²å­˜åœ¨: {config_data['key']}")
        
        session.commit()

def migrate_existing_data():
    """è¿ç§»ç°æœ‰æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰"""
    with get_db_session() as session:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®è¿ç§»é€»è¾‘
        # ä¾‹å¦‚ï¼šæ›´æ–°ç°æœ‰ç”¨æˆ·çš„é»˜è®¤è®¾ç½®
        from sqlmodel import select
        
        statement = select(User).where(User.preferred_language.is_(None))
        users_without_lang = session.exec(statement).all()
        
        for user in users_without_lang:
            user.preferred_language = "zh"
            session.add(user)
        
        if users_without_lang:
            session.commit()
            logger.info(f"âœ… æ›´æ–°äº† {len(users_without_lang)} ä¸ªç”¨æˆ·çš„é»˜è®¤è¯­è¨€è®¾ç½®")

def run_data_migration():
    """è¿è¡Œæ•°æ®è¿ç§»"""
    logger.info("å¼€å§‹æ•°æ®è¿ç§»...")
    
    try:
        create_default_admin()
        create_system_configs()
        migrate_existing_data()
        
        logger.info("âœ… æ•°æ®è¿ç§»å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    run_data_migration()
```

**è¿è¡Œæ•°æ®è¿ç§»ï¼š**
```bash
python scripts/migrate_data.py
```

---

## ğŸ”¬ Exerciseäº”ï¼šæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

### 5.1 åˆ›å»ºç´¢å¼•ä¼˜åŒ–è„šæœ¬

**åˆ›å»º `scripts/optimize_database.py`ï¼š**
```python
"""æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–è„šæœ¬"""

from src.database import engine
from sqlalchemy import text
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_indexes():
    """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•"""
    indexes = [
        # ç”¨æˆ·è¡¨ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email_active ON users(email) WHERE is_active = true;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_role_active ON users(role) WHERE is_active = true;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_last_login ON users(last_login_at DESC NULLS LAST);",
        
        # æ–‡æ¡£è¡¨ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_user_status ON documents(user_id, status);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_file_hash ON documents(file_hash);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_created_desc ON documents(created_at DESC);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_title_gin ON documents USING gin(to_tsvector('english', title));",
        
        # æ–‡æ¡£å—è¡¨ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_document_index ON document_chunks(document_id, chunk_index);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_vector_id ON document_chunks(vector_id) WHERE vector_id IS NOT NULL;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_content_gin ON document_chunks USING gin(to_tsvector('english', content));",
        
        # æŸ¥è¯¢å†å²è¡¨ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_user_created ON query_history(user_id, created_at DESC);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_hash ON query_history(query_hash);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_status ON query_history(status);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_rating ON query_history(user_rating) WHERE user_rating IS NOT NULL;",
        
        # ç³»ç»Ÿé…ç½®è¡¨ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_system_config_active ON system_config(key) WHERE is_active = true;"
    ]
    
    with engine.connect() as conn:
        for index_sql in indexes:
            try:
                logger.info(f"åˆ›å»ºç´¢å¼•: {index_sql.split('IF NOT EXISTS')[1].split('ON')[0].strip()}")
                conn.execute(text(index_sql))
                conn.commit()
                logger.info("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")

def analyze_tables():
    """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
    tables = [
        "users",
        "documents", 
        "document_chunks",
        "query_history",
        "system_config"
    ]
    
    with engine.connect() as conn:
        for table in tables:
            try:
                logger.info(f"åˆ†æè¡¨: {table}")
                conn.execute(text(f"ANALYZE {table};"))
                conn.commit()
                logger.info(f"âœ… è¡¨ {table} åˆ†æå®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ è¡¨ {table} åˆ†æå¤±è´¥: {e}")

def vacuum_tables():
    """æ¸…ç†è¡¨ç©ºé—´"""
    tables = [
        "users",
        "documents", 
        "document_chunks",
        "query_history",
        "system_config"
    ]
    
    with engine.connect() as conn:
        for table in tables:
            try:
                logger.info(f"æ¸…ç†è¡¨: {table}")
                conn.execute(text(f"VACUUM ANALYZE {table};"))
                conn.commit()
                logger.info(f"âœ… è¡¨ {table} æ¸…ç†å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ è¡¨ {table} æ¸…ç†å¤±è´¥: {e}")

def get_table_stats():
    """è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"""
    stats_query = """
    SELECT 
        schemaname,
        tablename,
        attname,
        n_distinct,
        correlation
    FROM pg_stats 
    WHERE schemaname = 'public'
    ORDER BY tablename, attname;
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(stats_query))
        rows = result.fetchall()
        
        logger.info("\n=== è¡¨ç»Ÿè®¡ä¿¡æ¯ ===")
        current_table = None
        for row in rows:
            if row.tablename != current_table:
                current_table = row.tablename
                logger.info(f"\nè¡¨: {current_table}")
            
            logger.info(f"  {row.attname}: distinct={row.n_distinct}, correlation={row.correlation}")

def optimize_database():
    """æ‰§è¡Œæ•°æ®åº“ä¼˜åŒ–"""
    logger.info("å¼€å§‹æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–...")
    
    try:
        create_indexes()
        analyze_tables()
        vacuum_tables()
        get_table_stats()
        
        logger.info("âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    optimize_database()
```

### 5.2 åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬

**åˆ›å»º `scripts/monitor_performance.py`ï¼š**
```python
"""æ•°æ®åº“æ€§èƒ½ç›‘æ§è„šæœ¬"""

from src.database import engine, get_pool_status
from sqlalchemy import text
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def monitor_connections():
    """ç›‘æ§æ•°æ®åº“è¿æ¥"""
    query = """
    SELECT 
        count(*) as total_connections,
        count(*) FILTER (WHERE state = 'active') as active_connections,
        count(*) FILTER (WHERE state = 'idle') as idle_connections,
        count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction
    FROM pg_stat_activity 
    WHERE datname = current_database();
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query)).fetchone()
        
        logger.info("=== æ•°æ®åº“è¿æ¥çŠ¶æ€ ===")
        logger.info(f"æ€»è¿æ¥æ•°: {result.total_connections}")
        logger.info(f"æ´»è·ƒè¿æ¥: {result.active_connections}")
        logger.info(f"ç©ºé—²è¿æ¥: {result.idle_connections}")
        logger.info(f"äº‹åŠ¡ä¸­ç©ºé—²: {result.idle_in_transaction}")
        
        # è¿æ¥æ± çŠ¶æ€
        pool_status = get_pool_status()
        logger.info("\n=== è¿æ¥æ± çŠ¶æ€ ===")
        logger.info(f"æ± å¤§å°: {pool_status['pool_size']}")
        logger.info(f"å·²æ£€å‡º: {pool_status['checked_out']}")
        logger.info(f"æº¢å‡ºè¿æ¥: {pool_status['overflow']}")
        logger.info(f"æ— æ•ˆè¿æ¥: {pool_status['invalidated']}")

def monitor_slow_queries():
    """ç›‘æ§æ…¢æŸ¥è¯¢"""
    query = """
    SELECT 
        query,
        calls,
        total_time,
        mean_time,
        rows,
        100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
    FROM pg_stat_statements 
    WHERE mean_time > 100  -- å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡100ms
    ORDER BY mean_time DESC 
    LIMIT 10;
    """
    
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query)).fetchall()
            
            if result:
                logger.info("\n=== æ…¢æŸ¥è¯¢ç›‘æ§ ===")
                for row in result:
                    logger.info(f"æŸ¥è¯¢: {row.query[:100]}...")
                    logger.info(f"  è°ƒç”¨æ¬¡æ•°: {row.calls}")
                    logger.info(f"  æ€»æ—¶é—´: {row.total_time:.2f}ms")
                    logger.info(f"  å¹³å‡æ—¶é—´: {row.mean_time:.2f}ms")
                    logger.info(f"  ç¼“å­˜å‘½ä¸­ç‡: {row.hit_percent:.2f}%")
                    logger.info("-" * 50)
            else:
                logger.info("\n=== æ…¢æŸ¥è¯¢ç›‘æ§ ===\næš‚æ— æ…¢æŸ¥è¯¢")
    except Exception as e:
        logger.warning(f"æ…¢æŸ¥è¯¢ç›‘æ§å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦å®‰è£…pg_stat_statementsæ‰©å±•ï¼‰: {e}")

def monitor_table_sizes():
    """ç›‘æ§è¡¨å¤§å°"""
    query = """
    SELECT 
        schemaname,
        tablename,
        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
        pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
    FROM pg_tables 
    WHERE schemaname = 'public'
    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query)).fetchall()
        
        logger.info("\n=== è¡¨å¤§å°ç›‘æ§ ===")
        for row in result:
            logger.info(f"{row.tablename}: {row.size}")

def monitor_index_usage():
    """ç›‘æ§ç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
    query = """
    SELECT 
        schemaname,
        tablename,
        indexname,
        idx_tup_read,
        idx_tup_fetch,
        idx_scan
    FROM pg_stat_user_indexes 
    WHERE schemaname = 'public'
    ORDER BY idx_scan DESC;
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query)).fetchall()
        
        logger.info("\n=== ç´¢å¼•ä½¿ç”¨ç›‘æ§ ===")
        for row in result:
            logger.info(f"{row.tablename}.{row.indexname}:")
            logger.info(f"  æ‰«ææ¬¡æ•°: {row.idx_scan}")
            logger.info(f"  è¯»å–å…ƒç»„: {row.idx_tup_read}")
            logger.info(f"  è·å–å…ƒç»„: {row.idx_tup_fetch}")

def run_performance_monitor():
    """è¿è¡Œæ€§èƒ½ç›‘æ§"""
    logger.info("å¼€å§‹æ•°æ®åº“æ€§èƒ½ç›‘æ§...")
    
    try:
        monitor_connections()
        monitor_slow_queries()
        monitor_table_sizes()
        monitor_index_usage()
        
        logger.info("\nâœ… æ€§èƒ½ç›‘æ§å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    run_performance_monitor()
```

**è¿è¡Œæ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§ï¼š**
```bash
# è¿è¡Œæ•°æ®åº“ä¼˜åŒ–
python scripts/optimize_database.py

# è¿è¡Œæ€§èƒ½ç›‘æ§
python scripts/monitor_performance.py
```

---

## ğŸ¤” æ€è€ƒé¢˜

1. **æ•°æ®æ¨¡å‹è®¾è®¡**ï¼š
   - ä¸ºä»€ä¹ˆè¦ä½¿ç”¨UUIDä½œä¸ºå…¨å±€å”¯ä¸€æ ‡è¯†ç¬¦ï¼Ÿ
   - å¦‚ä½•è®¾è®¡æ–‡æ¡£å’Œå—ä¹‹é—´çš„å…³ç³»ä»¥æ”¯æŒé«˜æ•ˆæŸ¥è¯¢ï¼Ÿ
   - è½¯åˆ é™¤å’Œç¡¬åˆ é™¤å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

2. **æ•°æ®åº“è¿æ¥ç®¡ç†**ï¼š
   - è¿æ¥æ± çš„å¤§å°åº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿ
   - ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦ä½¿ç”¨å¼‚æ­¥æ•°æ®åº“è¿æ¥ï¼Ÿ
   - å¦‚ä½•å¤„ç†æ•°æ®åº“è¿æ¥è¶…æ—¶å’Œé‡è¿ï¼Ÿ

3. **ä»“åº“æ¨¡å¼**ï¼š
   - ä»“åº“æ¨¡å¼ç›¸æ¯”ç›´æ¥ä½¿ç”¨ORMæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
   - å¦‚ä½•åœ¨ä»“åº“å±‚å®ç°ç¼“å­˜æœºåˆ¶ï¼Ÿ
   - æ‰¹é‡æ“ä½œå¦‚ä½•ä¼˜åŒ–æ€§èƒ½ï¼Ÿ

4. **æ•°æ®è¿ç§»**ï¼š
   - å¦‚ä½•è®¾è®¡å‘åå…¼å®¹çš„æ•°æ®åº“è¿ç§»ï¼Ÿ
   - å¤§è¡¨è¿ç§»æ—¶å¦‚ä½•é¿å…é”è¡¨ï¼Ÿ
   - å¦‚ä½•å›æ»šå¤±è´¥çš„è¿ç§»ï¼Ÿ

5. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥åˆ›å»ºå¤åˆç´¢å¼•ï¼Ÿ
   - å¦‚ä½•ç›‘æ§å’Œä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Ÿ
   - åˆ†åŒºè¡¨åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æœ‰ç”¨ï¼Ÿ

---

## âœ… Exerciseæ£€æŸ¥æ¸…å•

### åŸºç¡€åŠŸèƒ½æ£€æŸ¥
- [ ] SQLModelæ¨¡å‹å®šä¹‰æ­£ç¡®
- [ ] æ•°æ®åº“è¿æ¥é…ç½®æ­£å¸¸
- [ ] åŸºç¡€CRUDæ“ä½œæ­£å¸¸
- [ ] å…³ç³»æ˜ å°„æ­£ç¡®

### é«˜çº§åŠŸèƒ½æ£€æŸ¥
- [ ] ä»“åº“æ¨¡å¼å®ç°å®Œæ•´
- [ ] æ‰¹é‡æ“ä½œæ€§èƒ½è‰¯å¥½
- [ ] äº‹åŠ¡å¤„ç†æ­£ç¡®
- [ ] å¼‚æ­¥æ“ä½œæ”¯æŒ

### è¿ç§»å’Œä¼˜åŒ–æ£€æŸ¥
- [ ] Alembicé…ç½®æ­£ç¡®
- [ ] è¿ç§»è„šæœ¬å¯æ‰§è¡Œ
- [ ] ç´¢å¼•åˆ›å»ºæˆåŠŸ
- [ ] æ€§èƒ½ç›‘æ§æ­£å¸¸

### ä»£ç è´¨é‡æ£€æŸ¥
- [ ] ç±»å‹æ³¨è§£å®Œæ•´
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] æ—¥å¿—è®°å½•è¯¦ç»†
- [ ] æµ‹è¯•è¦†ç›–å……åˆ†

---

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

### 1. æ•°æ®åº“è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥PostgreSQLæœåŠ¡çŠ¶æ€
docker ps | grep postgres

# æ£€æŸ¥è¿æ¥å‚æ•°
psql -h localhost -p 5432 -U rag -d rag_db
```

### 2. è¿ç§»å¤±è´¥
```bash
# æŸ¥çœ‹è¿ç§»çŠ¶æ€
alembic current

# å¼ºåˆ¶æ ‡è®°è¿ç§»çŠ¶æ€
alembic stamp head

# é‡æ–°ç”Ÿæˆè¿ç§»
alembic revision --autogenerate -m "Fix migration"
```

### 3. æ€§èƒ½é—®é¢˜
```bash
# æ£€æŸ¥æ…¢æŸ¥è¯¢
psql -d rag_db -c "SELECT * FROM pg_stat_statements ORDER BY mean_time DESC LIMIT 5;"

# åˆ†æè¡¨ç»Ÿè®¡
psql -d rag_db -c "ANALYZE;"
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [SQLModelå®˜æ–¹æ–‡æ¡£](https://sqlmodel.tiangolo.com/)
- [PostgreSQLæ€§èƒ½ä¼˜åŒ–](https://www.postgresql.org/docs/current/performance-tips.html)
- [Alembicè¿ç§»æŒ‡å—](https://alembic.sqlalchemy.org/en/latest/tutorial.html)
- [SQLAlchemyè¿æ¥æ± ](https://docs.sqlalchemy.org/en/14/core/pooling.html)
- [æ•°æ®åº“è®¾è®¡æœ€ä½³å®è·µ](https://www.postgresql.org/docs/current/ddl-best-practices.html)

---

## ğŸ¯ Exerciseå®Œæˆæ ‡å¿—

å®Œæˆæœ¬Exerciseåï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- âœ… ç†Ÿç»ƒä½¿ç”¨SQLModelå®šä¹‰æ•°æ®æ¨¡å‹
- âœ… é…ç½®å’Œç®¡ç†æ•°æ®åº“è¿æ¥æ± 
- âœ… å®ç°å®Œæ•´çš„ä»“åº“æ¨¡å¼
- âœ… ä½¿ç”¨Alembicè¿›è¡Œæ•°æ®è¿ç§»
- âœ… è¿›è¡Œæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§

## Exerciseå®Œæˆåçš„Gitæ“ä½œ

### ä¸ºä»€ä¹ˆè¦è¿›è¡ŒGitæäº¤ï¼Ÿ

å®Œæˆlesson03 Exerciseåï¼Œè¿›è¡ŒGitæäº¤éå¸¸é‡è¦ï¼š

- **æ•°æ®æ¨¡å‹ä¿å­˜**: ä¿å­˜SQLModelæ•°æ®æ¨¡å‹å®šä¹‰å’Œå…³ç³»æ˜ å°„
- **è¿ç§»è„šæœ¬ç®¡ç†**: ç‰ˆæœ¬æ§åˆ¶æ•°æ®åº“è¿ç§»è„šæœ¬ï¼Œç¡®ä¿æ•°æ®åº“ç»“æ„å˜æ›´å¯è¿½è¸ª
- **ä»“åº“æ¨¡å¼å®ç°**: ä¿å­˜å®Œæ•´çš„æ•°æ®è®¿é—®å±‚ä»£ç 
- **é…ç½®ç®¡ç†**: ä¿å­˜æ•°æ®åº“è¿æ¥å’Œæ€§èƒ½ä¼˜åŒ–é…ç½®
- **å›¢é˜Ÿåä½œ**: ç¡®ä¿å›¢é˜Ÿæˆå‘˜ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ¨¡å‹å’Œè¿ç§»ç‰ˆæœ¬

### Gitæäº¤æ“ä½œæ­¥éª¤

å®Œæˆæœ¬è¯¾Exerciseåï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æäº¤ä½ çš„ä»£ç ï¼š

#### æ­¥éª¤1ï¼šæ£€æŸ¥å½“å‰çŠ¶æ€

```bash
# æŸ¥çœ‹å½“å‰GitçŠ¶æ€
git status

# æŸ¥çœ‹å…·ä½“çš„æ–‡ä»¶å˜æ›´
git diff
```

**é¢„æœŸçœ‹åˆ°çš„æ–‡ä»¶å˜æ›´**:
- `models/` - æ•°æ®æ¨¡å‹å®šä¹‰æ–‡ä»¶
- `database/` - æ•°æ®åº“è¿æ¥å’Œé…ç½®
- `repositories/` - ä»“åº“æ¨¡å¼å®ç°
- `alembic/` - æ•°æ®åº“è¿ç§»ç›¸å…³æ–‡ä»¶
- `scripts/` - æ•°æ®åº“ç®¡ç†å’Œä¼˜åŒ–è„šæœ¬
- `alembic.ini` - Alembicé…ç½®æ–‡ä»¶

#### æ­¥éª¤2ï¼šæ·»åŠ æ–‡ä»¶åˆ°æš‚å­˜åŒº

```bash
# æ·»åŠ æ‰€æœ‰æ›´æ”¹çš„æ–‡ä»¶
git add .

# æˆ–è€…é€‰æ‹©æ€§æ·»åŠ é‡è¦æ–‡ä»¶
git add models/
git add database/
git add repositories/
git add alembic/
git add scripts/
git add alembic.ini

# å†æ¬¡æ£€æŸ¥çŠ¶æ€
git status
```

**é‡è¦æé†’**: ç¡®ä¿ä¸è¦æäº¤åŒ…å«æ•æ„Ÿæ•°æ®åº“å¯†ç çš„é…ç½®æ–‡ä»¶ï¼Œåªæäº¤æ¨¡æ¿æ–‡ä»¶ã€‚

#### æ­¥éª¤3ï¼šæäº¤æ›´æ”¹

```bash
# æäº¤æ›´æ”¹å¹¶æ·»åŠ æè¿°ä¿¡æ¯
git commit -m "å®Œæˆlesson03 Exerciseï¼šå®ç°æ•°æ®æ¨¡å‹ä¸è¿ç§»åŠŸèƒ½"

# æˆ–è€…ä½¿ç”¨æ›´è¯¦ç»†çš„æäº¤ä¿¡æ¯
git commit -m "å®Œæˆlesson03 Exerciseï¼šæ•°æ®æ¨¡å‹ä¸è¿ç§»

- ä½¿ç”¨SQLModelå®šä¹‰Documentå’ŒChunkæ•°æ®æ¨¡å‹
- å®ç°æ•°æ®åº“è¿æ¥æ± å’Œé…ç½®ç®¡ç†
- åˆ›å»ºå®Œæ•´çš„ä»“åº“æ¨¡å¼æ•°æ®è®¿é—®å±‚
- é…ç½®Alembicæ•°æ®åº“è¿ç§»å·¥å…·
- å®ç°æ‰¹é‡æ“ä½œå’Œäº‹åŠ¡å¤„ç†
- æ·»åŠ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§åŠŸèƒ½
- åˆ›å»ºæ•°æ®åº“ç®¡ç†å’Œæµ‹è¯•è„šæœ¬"
```

#### æ­¥éª¤4ï¼šéªŒè¯æäº¤ç»“æœ

```bash
# æŸ¥çœ‹æäº¤å†å²
git log --oneline -5

# æŸ¥çœ‹æœ€è¿‘ä¸€æ¬¡æäº¤çš„è¯¦ç»†ä¿¡æ¯
git show HEAD

# æŸ¥çœ‹æäº¤çš„æ–‡ä»¶åˆ—è¡¨
git show --name-only HEAD
```

### æ•°æ®åº“é¡¹ç›®çš„ç‰¹æ®Šæ³¨æ„äº‹é¡¹

#### 1. è¿ç§»æ–‡ä»¶ç®¡ç†
```bash
# ç¡®ä¿è¿ç§»æ–‡ä»¶è¢«æ­£ç¡®æäº¤
git add alembic/versions/*.py

# æ£€æŸ¥è¿ç§»æ–‡ä»¶çš„å®Œæ•´æ€§
ls -la alembic/versions/
```

#### 2. æ•æ„Ÿä¿¡æ¯ä¿æŠ¤
```bash
# ç¡®ä¿æ•°æ®åº“å¯†ç ä¸è¢«æäº¤
echo "*.env" >> .gitignore
echo "database_url_with_password.txt" >> .gitignore

# åªæäº¤é…ç½®æ¨¡æ¿
git add .env.example
```

#### 3. æ•°æ®åº“çŠ¶æ€æ–‡ä»¶
```bash
# é€šå¸¸ä¸éœ€è¦æäº¤æ•°æ®åº“æ–‡ä»¶
echo "*.db" >> .gitignore
echo "*.sqlite" >> .gitignore
echo "data/" >> .gitignore
```

### éªŒè¯æ•°æ®æ¨¡å‹å’Œè¿ç§»

æäº¤å‰ï¼Œå»ºè®®è¿›è¡Œæœ€åä¸€æ¬¡å®Œæ•´éªŒè¯ï¼š

```bash
# é‡ç½®æ•°æ®åº“ï¼ˆåœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼‰
docker-compose down -v
docker-compose up -d postgres

# ç­‰å¾…æ•°æ®åº“å¯åŠ¨
sleep 10

# è¿è¡Œè¿ç§»
alembic upgrade head

# éªŒè¯æ•°æ®æ¨¡å‹
python scripts/test_models.py

# æµ‹è¯•ä»“åº“åŠŸèƒ½
python scripts/test_repositories.py

# å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œå†è¿›è¡Œæäº¤
git add .
git commit -m "éªŒè¯é€šè¿‡ï¼šå®Œæˆlesson03æ•°æ®æ¨¡å‹å’Œè¿ç§»"
```

### å¯é€‰ï¼šæ¨é€åˆ°è¿œç¨‹ä»“åº“

```bash
# æ¨é€åˆ°è¿œç¨‹ä»“åº“
git push origin lesson03

# å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ¨é€è¯¥åˆ†æ”¯
git push -u origin lesson03
```

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### Q1: è¿ç§»æ–‡ä»¶å†²çª
**é—®é¢˜**: å¤šäººå¼€å‘æ—¶è¿ç§»æ–‡ä»¶ç‰ˆæœ¬å†²çª
**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥çœ‹è¿ç§»å†å²
alembic history

# åˆå¹¶è¿ç§»æ–‡ä»¶
alembic merge -m "åˆå¹¶è¿ç§»" head1 head2

# é‡æ–°ç”Ÿæˆè¿ç§»
alembic revision --autogenerate -m "ä¿®å¤è¿ç§»å†²çª"
```

#### Q2: æ¨¡å‹å®šä¹‰é”™è¯¯
**é—®é¢˜**: SQLModelæ¨¡å‹å®šä¹‰å¯¼è‡´è¿ç§»å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ¨¡å‹è¯­æ³•
python -c "from models.document import Document; print('æ¨¡å‹å®šä¹‰æ­£ç¡®')"

# é‡æ–°ç”Ÿæˆè¿ç§»
alembic revision --autogenerate -m "ä¿®å¤æ¨¡å‹å®šä¹‰"

# æ£€æŸ¥ç”Ÿæˆçš„è¿ç§»è„šæœ¬
cat alembic/versions/æœ€æ–°è¿ç§»æ–‡ä»¶.py
```

#### Q3: æ•°æ®åº“è¿æ¥é—®é¢˜
**é—®é¢˜**: æäº¤åå…¶ä»–äººæ— æ³•è¿æ¥æ•°æ®åº“
**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿ç¯å¢ƒå˜é‡æ¨¡æ¿å®Œæ•´
echo "DATABASE_URL=postgresql://user:password@localhost:5432/dbname" >> .env.example
echo "DB_POOL_SIZE=10" >> .env.example
echo "DB_MAX_OVERFLOW=20" >> .env.example

# æä¾›æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
git add scripts/init_database.py
```

#### Q4: æ€§èƒ½ä¼˜åŒ–é…ç½®
**é—®é¢˜**: ç´¢å¼•å’Œä¼˜åŒ–é…ç½®åœ¨ä¸åŒç¯å¢ƒä¸‹è¡¨ç°ä¸ä¸€è‡´
**è§£å†³æ–¹æ¡ˆ**:
```bash
# åˆ›å»ºç¯å¢ƒç‰¹å®šçš„é…ç½®
echo "# å¼€å‘ç¯å¢ƒé…ç½®" >> config/development.py
echo "# ç”Ÿäº§ç¯å¢ƒé…ç½®" >> config/production.py

# æäº¤é…ç½®æ–‡ä»¶
git add config/
```

### æ•°æ®åº“é¡¹ç›®Gitæœ€ä½³å®è·µ

1. **è¿ç§»æ–‡ä»¶ç®¡ç†**: æ¯ä¸ªè¿ç§»éƒ½åº”è¯¥æœ‰æ¸…æ™°çš„æè¿°å’Œç‰ˆæœ¬å·
   ```bash
   alembic revision --autogenerate -m "æ·»åŠ æ–‡æ¡£è¡¨ç´¢å¼•ä¼˜åŒ–"
   ```

2. **æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶**: é‡è¦çš„æ¨¡å‹å˜æ›´åº”è¯¥æ‰“æ ‡ç­¾
   ```bash
   git tag -a v0.3.0 -m "å®Œæˆæ•°æ®æ¨¡å‹è®¾è®¡"
   git push origin v0.3.0
   ```

3. **æµ‹è¯•æ•°æ®ç®¡ç†**: æä¾›æµ‹è¯•æ•°æ®çš„ç”Ÿæˆè„šæœ¬
   ```bash
   git add scripts/generate_test_data.py
   ```

4. **æ–‡æ¡£åŒæ­¥**: ç¡®ä¿æ•°æ®æ¨¡å‹æ–‡æ¡£ä¸ä»£ç ä¿æŒåŒæ­¥
   ```bash
   git add docs/database_schema.md
   ```

5. **å¤‡ä»½ç­–ç•¥**: æä¾›æ•°æ®åº“å¤‡ä»½å’Œæ¢å¤è„šæœ¬
   ```bash
   git add scripts/backup_database.py
   git add scripts/restore_database.py
   ```

### æ•°æ®åº“è¿ç§»çš„Gitå·¥ä½œæµ

```bash
# 1. åˆ›å»ºæ–°çš„è¿ç§»
alembic revision --autogenerate -m "æ·»åŠ æ–°åŠŸèƒ½"

# 2. æ£€æŸ¥ç”Ÿæˆçš„è¿ç§»æ–‡ä»¶
cat alembic/versions/æœ€æ–°æ–‡ä»¶.py

# 3. æµ‹è¯•è¿ç§»
alembic upgrade head

# 4. æµ‹è¯•å›æ»š
alembic downgrade -1
alembic upgrade head

# 5. æäº¤è¿ç§»æ–‡ä»¶
git add alembic/versions/æœ€æ–°æ–‡ä»¶.py
git commit -m "æ·»åŠ æ•°æ®åº“è¿ç§»ï¼šæ–°åŠŸèƒ½"
```

### ä¸‹ä¸€æ­¥

å®ŒæˆGitæäº¤åï¼Œä½ çš„æ•°æ®æ¨¡å‹å’Œè¿ç§»åŠŸèƒ½å·²ç»å®‰å…¨ä¿å­˜ã€‚åœ¨lesson04ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- åˆ‡æ¢åˆ° `lesson04` åˆ†æ”¯
- å­¦ä¹ PDFæ–‡æ¡£è§£ææŠ€æœ¯
- å®ç°æ–‡æ¡£å†…å®¹çš„æ™ºèƒ½åˆ†å—
- å°†è§£æç»“æœå­˜å‚¨åˆ°æ•°æ®åº“ä¸­

æ•°æ®æ¨¡å‹æ˜¯RAGç³»ç»Ÿçš„åŸºç¡€ï¼Œè‰¯å¥½çš„Gitç®¡ç†ä¹ æƒ¯å°†å¸®åŠ©ä½ åœ¨å¤æ‚çš„æ•°æ®åº“å˜æ›´ä¸­ä¿æŒä»£ç çš„ç¨³å®šæ€§å’Œå¯è¿½æº¯æ€§ã€‚è®°ä½ï¼Œæ¯æ¬¡æ•°æ®åº“ç»“æ„å˜æ›´éƒ½åº”è¯¥é€šè¿‡è¿ç§»è„šæœ¬è¿›è¡Œï¼Œå¹¶åŠæ—¶æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿä¸­ã€‚

æ­å–œä½ å®Œæˆäº†æ•°æ®æ¨¡å‹ä¸è¿ç§»çš„å­¦ä¹ ï¼ğŸ‰