# 第三课：数据模型与迁移 - 学生Exercise指导

## 📋 代码基础准备

在开始本节课的Exercise之前，我们需要切换到lesson03分支获取本课程的代码。

### 步骤1：进入项目目录

```bash
# 进入rag-system项目目录
cd rag-system
```

### 步骤2：切换到lesson03分支

```bash
# 切换到lesson03分支
git checkout lesson03

# 验证当前分支
git branch
# 应该显示 * lesson03
```

### 步骤3：验证代码状态

```bash
# 查看项目结构
ls -la
# 应该看到：src/models/ src/database/ migrations/ 等新增的目录

# 检查数据模型文件
ls src/models/
# 应该看到：__init__.py base.py user.py document.py 等文件

# 检查数据库配置
cat src/database/connection.py
# 应该看到数据库连接配置
```

### 步骤4：启动依赖服务

```bash
# 启动依赖服务（PostgreSQL等）
docker-compose up -d

# 检查服务状态
docker-compose ps

# 验证PostgreSQL连接
docker-compose exec postgres psql -U postgres -d rag_db -c "SELECT version();"
```

**说明**：
- `lesson03`分支包含了前两课的所有代码，并新增了完整的数据模型设计
- 包含SQLModel数据模型、数据库连接配置和Alembic迁移脚本
- 包含用户管理、文档管理等核心数据模型
- 这种方式确保了代码的渐进式开发和版本一致性

---

## 🎯 Exercise目标

通过本次Exercise，你将：
- 掌握SQLModel框架的使用方法
- 学会设计RAG系统的数据模型
- 理解数据库连接池的配置和管理
- 掌握Alembic数据迁移工具的使用
- 学会数据库性能优化的基本方法

---

## 🔧 Exercise环境要求

### 系统要求
- 完成前两课的环境配置
- PostgreSQL数据库正常运行
- Python 3.11+ 环境

### 必需依赖
```bash
# 安装SQLModel和相关依赖
uv add sqlmodel
uv add alembic
uv add psycopg2-binary
uv add asyncpg  # 异步支持
```

---

## 🔬 Exercise一：SQLModel基础使用

### 1.1 创建基础数据模型

**创建模型目录结构：**
```bash
mkdir -p src/models
touch src/models/__init__.py
```

**创建 `src/models/base.py`：**
```python
from sqlmodel import SQLModel, Field
from typing import Optional
from datetime import datetime
import uuid

class BaseModel(SQLModel):
    """基础模型类"""
    id: Optional[int] = Field(default=None, primary_key=True)
    uuid: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        unique=True,
        index=True,
        description="全局唯一标识符"
    )
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="创建时间"
    )
    updated_at: Optional[datetime] = Field(
        default=None,
        description="更新时间"
    )
    
    class Config:
        """模型配置"""
        # 允许从ORM对象创建
        from_attributes = True
        # JSON序列化配置
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }
```

**创建 `src/models/user.py`：**
```python
from sqlmodel import SQLModel, Field, Relationship
from typing import Optional, List
from enum import Enum
from .base import BaseModel

class UserRole(str, Enum):
    """用户角色枚举"""
    ADMIN = "admin"
    USER = "user"
    GUEST = "guest"

class UserBase(SQLModel):
    """用户基础字段"""
    username: str = Field(
        max_length=50,
        unique=True,
        index=True,
        description="用户名"
    )
    email: str = Field(
        max_length=100,
        unique=True,
        index=True,
        description="邮箱地址"
    )
    role: UserRole = Field(
        default=UserRole.USER,
        description="用户角色"
    )
    is_active: bool = Field(
        default=True,
        description="是否激活"
    )
    preferred_language: Optional[str] = Field(
        default="zh",
        max_length=10,
        description="首选语言"
    )
    max_query_length: int = Field(
        default=1000,
        ge=1,
        le=10000,
        description="最大查询长度"
    )

class User(UserBase, BaseModel, table=True):
    """用户模型"""
    __tablename__ = "users"
    
    password_hash: str = Field(
        max_length=255,
        description="密码哈希"
    )
    last_login_at: Optional[datetime] = Field(
        default=None,
        description="最后登录时间"
    )
    
    # 关系定义
    documents: List["Document"] = Relationship(
        back_populates="user",
        sa_relationship_kwargs={"cascade": "all, delete-orphan"}
    )
    queries: List["QueryHistory"] = Relationship(
        back_populates="user",
        sa_relationship_kwargs={"cascade": "all, delete-orphan"}
    )

class UserCreate(UserBase):
    """创建用户的请求模型"""
    password: str = Field(
        min_length=8,
        max_length=100,
        description="密码"
    )

class UserRead(UserBase):
    """读取用户的响应模型"""
    id: int
    uuid: str
    created_at: datetime
    updated_at: Optional[datetime]
    last_login_at: Optional[datetime]

class UserUpdate(SQLModel):
    """更新用户的请求模型"""
    email: Optional[str] = None
    role: Optional[UserRole] = None
    is_active: Optional[bool] = None
    preferred_language: Optional[str] = None
    max_query_length: Optional[int] = None
```

**创建 `src/models/document.py`：**
```python
from sqlmodel import SQLModel, Field, Relationship
from typing import Optional, List
from enum import Enum
from datetime import datetime
from .base import BaseModel

class DocumentStatus(str, Enum):
    """文档状态枚举"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class DocumentBase(SQLModel):
    """文档基础字段"""
    title: str = Field(
        max_length=255,
        index=True,
        description="文档标题"
    )
    description: Optional[str] = Field(
        default=None,
        max_length=1000,
        description="文档描述"
    )
    original_filename: str = Field(
        max_length=255,
        description="原始文件名"
    )
    file_path: str = Field(
        max_length=500,
        description="文件路径"
    )
    file_size: int = Field(
        ge=0,
        description="文件大小（字节）"
    )
    mime_type: str = Field(
        max_length=100,
        description="MIME类型"
    )
    file_hash: str = Field(
        max_length=64,
        index=True,
        description="文件SHA-256哈希"
    )
    status: DocumentStatus = Field(
        default=DocumentStatus.PENDING,
        index=True,
        description="处理状态"
    )

class Document(DocumentBase, BaseModel, table=True):
    """文档模型"""
    __tablename__ = "documents"
    
    error_message: Optional[str] = Field(
        default=None,
        description="错误信息"
    )
    page_count: Optional[int] = Field(
        default=None,
        ge=0,
        description="页数"
    )
    word_count: Optional[int] = Field(
        default=None,
        ge=0,
        description="字数"
    )
    chunk_count: Optional[int] = Field(
        default=0,
        ge=0,
        description="块数量"
    )
    processed_at: Optional[datetime] = Field(
        default=None,
        description="处理完成时间"
    )
    
    # 外键
    user_id: int = Field(
        foreign_key="users.id",
        index=True,
        description="所属用户ID"
    )
    
    # 关系
    user: "User" = Relationship(back_populates="documents")
    chunks: List["DocumentChunk"] = Relationship(
        back_populates="document",
        sa_relationship_kwargs={"cascade": "all, delete-orphan"}
    )

class DocumentChunk(BaseModel, table=True):
    """文档块模型"""
    __tablename__ = "document_chunks"
    
    content: str = Field(
        description="块内容"
    )
    chunk_index: int = Field(
        ge=0,
        index=True,
        description="块索引"
    )
    page_number: Optional[int] = Field(
        default=None,
        ge=1,
        description="页码"
    )
    token_count: int = Field(
        ge=0,
        description="令牌数量"
    )
    character_count: int = Field(
        ge=0,
        description="字符数量"
    )
    vector_id: Optional[str] = Field(
        default=None,
        max_length=100,
        index=True,
        description="向量数据库中的ID"
    )
    embedding_model: Optional[str] = Field(
        default=None,
        max_length=100,
        description="使用的嵌入模型"
    )
    
    # 外键
    document_id: int = Field(
        foreign_key="documents.id",
        index=True,
        description="所属文档ID"
    )
    
    # 关系
    document: Document = Relationship(back_populates="chunks")

# 请求/响应模型
class DocumentCreate(DocumentBase):
    """创建文档的请求模型"""
    user_id: int

class DocumentRead(DocumentBase):
    """读取文档的响应模型"""
    id: int
    uuid: str
    error_message: Optional[str]
    page_count: Optional[int]
    word_count: Optional[int]
    chunk_count: Optional[int]
    processed_at: Optional[datetime]
    user_id: int
    created_at: datetime
    updated_at: Optional[datetime]

class DocumentUpdate(SQLModel):
    """更新文档的请求模型"""
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[DocumentStatus] = None
    error_message: Optional[str] = None
```

**创建 `src/models/query.py`：**
```python
from sqlmodel import SQLModel, Field, Relationship
from typing import Optional, List, Dict, Any
from enum import Enum
from datetime import datetime
from .base import BaseModel

class QueryStatus(str, Enum):
    """查询状态枚举"""
    SUCCESS = "success"
    FAILED = "failed"
    TIMEOUT = "timeout"

class QueryHistory(BaseModel, table=True):
    """查询历史模型"""
    __tablename__ = "query_history"
    
    query_text: str = Field(
        max_length=2000,
        description="查询文本"
    )
    query_hash: str = Field(
        max_length=64,
        index=True,
        description="查询文本的哈希值"
    )
    retrieved_chunks: Optional[str] = Field(
        default=None,
        description="检索到的块（JSON格式）"
    )
    response_text: Optional[str] = Field(
        default=None,
        description="响应文本"
    )
    
    # 性能指标
    retrieval_time_ms: Optional[int] = Field(
        default=None,
        ge=0,
        description="检索耗时（毫秒）"
    )
    generation_time_ms: Optional[int] = Field(
        default=None,
        ge=0,
        description="生成耗时（毫秒）"
    )
    total_time_ms: Optional[int] = Field(
        default=None,
        ge=0,
        description="总耗时（毫秒）"
    )
    
    # 状态和评分
    status: QueryStatus = Field(
        default=QueryStatus.SUCCESS,
        index=True,
        description="查询状态"
    )
    user_rating: Optional[int] = Field(
        default=None,
        ge=1,
        le=5,
        description="用户评分（1-5星）"
    )
    user_feedback: Optional[str] = Field(
        default=None,
        max_length=1000,
        description="用户反馈"
    )
    
    # 外键
    user_id: int = Field(
        foreign_key="users.id",
        index=True,
        description="用户ID"
    )
    
    # 关系
    user: "User" = Relationship(back_populates="queries")

class SystemConfig(BaseModel, table=True):
    """系统配置模型"""
    __tablename__ = "system_config"
    
    key: str = Field(
        max_length=100,
        unique=True,
        index=True,
        description="配置键"
    )
    value: str = Field(
        max_length=1000,
        description="配置值"
    )
    description: Optional[str] = Field(
        default=None,
        max_length=500,
        description="配置描述"
    )
    is_active: bool = Field(
        default=True,
        description="是否激活"
    )
```

**创建 `src/models/__init__.py`：**
```python
"""数据模型模块"""

from .base import BaseModel
from .user import User, UserCreate, UserRead, UserUpdate, UserRole
from .document import (
    Document,
    DocumentChunk,
    DocumentCreate,
    DocumentRead,
    DocumentUpdate,
    DocumentStatus
)
from .query import QueryHistory, SystemConfig, QueryStatus

__all__ = [
    "BaseModel",
    "User",
    "UserCreate",
    "UserRead",
    "UserUpdate",
    "UserRole",
    "Document",
    "DocumentChunk",
    "DocumentCreate",
    "DocumentRead",
    "DocumentUpdate",
    "DocumentStatus",
    "QueryHistory",
    "SystemConfig",
    "QueryStatus",
]
```

### 1.2 测试数据模型

**创建 `test_models.py`：**
```python
"""测试数据模型"""

from src.models import User, Document, DocumentChunk, UserRole, DocumentStatus
from datetime import datetime

def test_user_model():
    """测试用户模型"""
    # 创建用户实例
    user = User(
        username="test_user",
        email="test@example.com",
        password_hash="hashed_password",
        role=UserRole.USER
    )
    
    print(f"用户模型创建成功:")
    print(f"  用户名: {user.username}")
    print(f"  邮箱: {user.email}")
    print(f"  角色: {user.role}")
    print(f"  UUID: {user.uuid}")
    print(f"  创建时间: {user.created_at}")
    
    # 测试JSON序列化
    user_dict = user.model_dump()
    print(f"\n序列化结果: {user_dict}")
    
    return user

def test_document_model():
    """测试文档模型"""
    document = Document(
        title="测试文档",
        description="这是一个测试文档",
        original_filename="test.pdf",
        file_path="/path/to/test.pdf",
        file_size=1024000,
        mime_type="application/pdf",
        file_hash="abc123def456",
        status=DocumentStatus.PENDING,
        user_id=1
    )
    
    print(f"\n文档模型创建成功:")
    print(f"  标题: {document.title}")
    print(f"  文件大小: {document.file_size} bytes")
    print(f"  状态: {document.status}")
    print(f"  UUID: {document.uuid}")
    
    return document

def test_document_chunk_model():
    """测试文档块模型"""
    chunk = DocumentChunk(
        content="这是文档的第一个块内容",
        chunk_index=0,
        page_number=1,
        token_count=50,
        character_count=200,
        document_id=1
    )
    
    print(f"\n文档块模型创建成功:")
    print(f"  内容长度: {len(chunk.content)} 字符")
    print(f"  块索引: {chunk.chunk_index}")
    print(f"  令牌数: {chunk.token_count}")
    print(f"  UUID: {chunk.uuid}")
    
    return chunk

if __name__ == "__main__":
    print("=== 测试数据模型 ===")
    
    user = test_user_model()
    document = test_document_model()
    chunk = test_document_chunk_model()
    
    print("\n✅ 所有模型测试通过！")
```

**运行测试：**
```bash
python test_models.py
```

---

## 🔬 Exercise二：数据库连接配置

### 2.1 创建数据库配置

**创建 `src/database/__init__.py`：**
```python
"""数据库模块"""

from .connection import engine, get_session, get_async_session
from .config import DatabaseConfig

__all__ = [
    "engine",
    "get_session",
    "get_async_session",
    "DatabaseConfig",
]
```

**创建 `src/database/config.py`：**
```python
"""数据库配置"""

from pydantic import BaseSettings, Field
from typing import Optional

class DatabaseConfig(BaseSettings):
    """数据库配置类"""
    
    # 基础连接配置
    database_url: str = Field(
        default="postgresql://rag:ragpass@localhost:5432/rag_db",
        description="数据库连接URL"
    )
    
    # 连接池配置
    pool_size: int = Field(
        default=10,
        ge=1,
        le=50,
        description="连接池大小"
    )
    max_overflow: int = Field(
        default=20,
        ge=0,
        le=100,
        description="最大溢出连接数"
    )
    pool_timeout: int = Field(
        default=30,
        ge=1,
        le=300,
        description="连接池超时时间（秒）"
    )
    pool_recycle: int = Field(
        default=3600,
        ge=300,
        le=86400,
        description="连接回收时间（秒）"
    )
    pool_pre_ping: bool = Field(
        default=True,
        description="连接前是否ping测试"
    )
    
    # 调试配置
    echo: bool = Field(
        default=False,
        description="是否输出SQL语句"
    )
    echo_pool: bool = Field(
        default=False,
        description="是否输出连接池日志"
    )
    
    # 连接参数
    connect_timeout: int = Field(
        default=10,
        ge=1,
        le=60,
        description="连接超时时间（秒）"
    )
    statement_timeout: str = Field(
        default="30s",
        description="语句超时时间"
    )
    lock_timeout: str = Field(
        default="10s",
        description="锁超时时间"
    )
    
    class Config:
        env_prefix = "DB_"
        env_file = ".env"

# 全局配置实例
db_config = DatabaseConfig()
```

**创建 `src/database/connection.py`：**
```python
"""数据库连接管理"""

from sqlmodel import create_engine, Session
from sqlalchemy.pool import QueuePool
from sqlalchemy import event
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager
from typing import Generator
import logging

from .config import db_config

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 创建同步引擎
engine = create_engine(
    db_config.database_url,
    echo=db_config.echo,
    echo_pool=db_config.echo_pool,
    poolclass=QueuePool,
    pool_size=db_config.pool_size,
    max_overflow=db_config.max_overflow,
    pool_timeout=db_config.pool_timeout,
    pool_recycle=db_config.pool_recycle,
    pool_pre_ping=db_config.pool_pre_ping,
    connect_args={
        "connect_timeout": db_config.connect_timeout,
        "application_name": "rag_system",
        "options": f"-c timezone=UTC -c statement_timeout={db_config.statement_timeout} -c lock_timeout={db_config.lock_timeout}"
    }
)

# 创建异步引擎
async_database_url = db_config.database_url.replace("postgresql://", "postgresql+asyncpg://")
async_engine = create_async_engine(
    async_database_url,
    echo=db_config.echo,
    pool_size=db_config.pool_size,
    max_overflow=db_config.max_overflow,
    pool_timeout=db_config.pool_timeout,
    pool_recycle=db_config.pool_recycle,
    pool_pre_ping=db_config.pool_pre_ping
)

# 异步会话工厂
AsyncSessionLocal = sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# 连接事件监听器
@event.listens_for(engine, "connect")
def set_postgresql_pragma(dbapi_connection, connection_record):
    """设置PostgreSQL连接参数"""
    logger.debug("设置PostgreSQL连接参数")

@event.listens_for(engine.pool, "connect")
def receive_connect(dbapi_connection, connection_record):
    """连接建立事件"""
    logger.debug("建立新的数据库连接")

@event.listens_for(engine.pool, "checkout")
def receive_checkout(dbapi_connection, connection_record, connection_proxy):
    """连接检出事件"""
    logger.debug("从连接池检出连接")

@event.listens_for(engine.pool, "checkin")
def receive_checkin(dbapi_connection, connection_record):
    """连接检入事件"""
    logger.debug("连接返回连接池")

# 同步会话管理
def get_session() -> Generator[Session, None, None]:
    """获取数据库会话（依赖注入用）"""
    with Session(engine) as session:
        try:
            yield session
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()

@contextmanager
def get_db_session():
    """获取数据库会话（上下文管理器）"""
    session = Session(engine)
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

# 异步会话管理
async def get_async_session() -> AsyncSession:
    """获取异步数据库会话"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

# 连接池状态监控
def get_pool_status():
    """获取连接池状态"""
    pool = engine.pool
    return {
        "pool_size": pool.size(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "invalidated": pool.invalidated()
    }

# 数据库健康检查
def check_database_health() -> bool:
    """检查数据库健康状态"""
    try:
        with get_db_session() as session:
            session.exec("SELECT 1")
        return True
    except Exception as e:
        logger.error(f"数据库健康检查失败: {e}")
        return False
```

### 2.2 创建数据库初始化脚本

**创建 `src/database/init_db.py`：**
```python
"""数据库初始化脚本"""

from sqlmodel import SQLModel
from .connection import engine
from src.models import *  # 导入所有模型
import logging

logger = logging.getLogger(__name__)

def create_tables():
    """创建所有表"""
    try:
        logger.info("开始创建数据库表...")
        SQLModel.metadata.create_all(engine)
        logger.info("✅ 数据库表创建成功")
    except Exception as e:
        logger.error(f"❌ 创建数据库表失败: {e}")
        raise

def drop_tables():
    """删除所有表（谨慎使用）"""
    try:
        logger.warning("开始删除数据库表...")
        SQLModel.metadata.drop_all(engine)
        logger.info("✅ 数据库表删除成功")
    except Exception as e:
        logger.error(f"❌ 删除数据库表失败: {e}")
        raise

def reset_database():
    """重置数据库（删除并重新创建所有表）"""
    logger.warning("开始重置数据库...")
    drop_tables()
    create_tables()
    logger.info("✅ 数据库重置完成")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "create":
            create_tables()
        elif command == "drop":
            drop_tables()
        elif command == "reset":
            reset_database()
        else:
            print("用法: python init_db.py [create|drop|reset]")
    else:
        create_tables()
```

### 2.3 测试数据库连接

**创建 `test_database.py`：**
```python
"""测试数据库连接"""

from src.database import engine, get_session, get_pool_status, check_database_health
from src.models import User, UserRole
from sqlmodel import select
import time

def test_basic_connection():
    """测试基础连接"""
    print("=== 测试基础数据库连接 ===")
    
    try:
        with get_session() as session:
            result = session.exec(select(1)).first()
            print(f"✅ 数据库连接成功，查询结果: {result}")
    except Exception as e:
        print(f"❌ 数据库连接失败: {e}")
        return False
    
    return True

def test_pool_status():
    """测试连接池状态"""
    print("\n=== 测试连接池状态 ===")
    
    status = get_pool_status()
    print(f"连接池大小: {status['pool_size']}")
    print(f"已检出连接: {status['checked_out']}")
    print(f"溢出连接: {status['overflow']}")
    print(f"无效连接: {status['invalidated']}")

def test_crud_operations():
    """测试CRUD操作"""
    print("\n=== 测试CRUD操作 ===")
    
    try:
        with get_session() as session:
            # 创建用户
            user = User(
                username="test_user",
                email="test@example.com",
                password_hash="hashed_password",
                role=UserRole.USER
            )
            
            session.add(user)
            session.commit()
            session.refresh(user)
            
            print(f"✅ 用户创建成功，ID: {user.id}")
            
            # 查询用户
            statement = select(User).where(User.username == "test_user")
            found_user = session.exec(statement).first()
            
            if found_user:
                print(f"✅ 用户查询成功: {found_user.username}")
            else:
                print("❌ 用户查询失败")
                return False
            
            # 更新用户
            found_user.email = "updated@example.com"
            session.add(found_user)
            session.commit()
            
            print(f"✅ 用户更新成功，新邮箱: {found_user.email}")
            
            # 删除用户
            session.delete(found_user)
            session.commit()
            
            print("✅ 用户删除成功")
            
    except Exception as e:
        print(f"❌ CRUD操作失败: {e}")
        return False
    
    return True

def test_concurrent_connections():
    """测试并发连接"""
    print("\n=== 测试并发连接 ===")
    
    import threading
    import time
    
    results = []
    
    def worker(worker_id):
        try:
            with get_session() as session:
                # 模拟一些数据库操作
                time.sleep(0.1)
                result = session.exec(select(1)).first()
                results.append(f"Worker {worker_id}: {result}")
        except Exception as e:
            results.append(f"Worker {worker_id} failed: {e}")
    
    # 创建多个线程
    threads = []
    for i in range(5):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)
        thread.start()
    
    # 等待所有线程完成
    for thread in threads:
        thread.join()
    
    print(f"并发测试结果:")
    for result in results:
        print(f"  {result}")
    
    return len(results) == 5

def test_health_check():
    """测试健康检查"""
    print("\n=== 测试健康检查 ===")
    
    is_healthy = check_database_health()
    if is_healthy:
        print("✅ 数据库健康状态良好")
    else:
        print("❌ 数据库健康检查失败")
    
    return is_healthy

if __name__ == "__main__":
    print("开始数据库连接测试...")
    
    tests = [
        ("基础连接", test_basic_connection),
        ("连接池状态", test_pool_status),
        ("CRUD操作", test_crud_operations),
        ("并发连接", test_concurrent_connections),
        ("健康检查", test_health_check),
    ]
    
    passed = 0
    for name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"执行测试: {name}")
        print(f"{'='*50}")
        
        try:
            if test_func():
                passed += 1
                print(f"✅ {name} 测试通过")
            else:
                print(f"❌ {name} 测试失败")
        except Exception as e:
            print(f"❌ {name} 测试异常: {e}")
    
    print(f"\n{'='*50}")
    print(f"测试总结: {passed}/{len(tests)} 个测试通过")
    print(f"{'='*50}")
```

**运行测试：**
```bash
# 首先创建数据库表
python -m src.database.init_db create

# 运行连接测试
python test_database.py
```

---

## 🔬 Exercise三：数据仓库模式实现

### 3.1 创建基础仓库类

**创建 `src/repositories/__init__.py`：**
```python
"""数据仓库模块"""

from .base import BaseRepository
from .user import UserRepository
from .document import DocumentRepository
from .query import QueryRepository

__all__ = [
    "BaseRepository",
    "UserRepository",
    "DocumentRepository",
    "QueryRepository",
]
```

**创建 `src/repositories/base.py`：**
```python
"""基础仓库类"""

from typing import Type, TypeVar, Generic, List, Optional, Dict, Any
from sqlmodel import SQLModel, Session, select
from sqlalchemy.orm import selectinload
from datetime import datetime

ModelType = TypeVar("ModelType", bound=SQLModel)

class BaseRepository(Generic[ModelType]):
    """基础仓库类"""
    
    def __init__(self, model: Type[ModelType], session: Session):
        self.model = model
        self.session = session
    
    def create(self, obj_in: ModelType) -> ModelType:
        """创建对象"""
        self.session.add(obj_in)
        self.session.commit()
        self.session.refresh(obj_in)
        return obj_in
    
    def get(self, id: int) -> Optional[ModelType]:
        """根据ID获取对象"""
        return self.session.get(self.model, id)
    
    def get_by_uuid(self, uuid: str) -> Optional[ModelType]:
        """根据UUID获取对象"""
        statement = select(self.model).where(self.model.uuid == uuid)
        return self.session.exec(statement).first()
    
    def get_multi(
        self, 
        skip: int = 0, 
        limit: int = 100,
        order_by: Optional[str] = None
    ) -> List[ModelType]:
        """获取多个对象"""
        statement = select(self.model).offset(skip).limit(limit)
        
        if order_by:
            if hasattr(self.model, order_by):
                statement = statement.order_by(getattr(self.model, order_by))
        
        return self.session.exec(statement).all()
    
    def update(self, db_obj: ModelType, obj_in: Dict[str, Any]) -> ModelType:
        """更新对象"""
        for field, value in obj_in.items():
            if hasattr(db_obj, field) and value is not None:
                setattr(db_obj, field, value)
        
        # 自动更新时间戳
        if hasattr(db_obj, 'updated_at'):
            db_obj.updated_at = datetime.utcnow()
        
        self.session.add(db_obj)
        self.session.commit()
        self.session.refresh(db_obj)
        return db_obj
    
    def delete(self, id: int) -> bool:
        """删除对象"""
        obj = self.session.get(self.model, id)
        if obj:
            self.session.delete(obj)
            self.session.commit()
            return True
        return False
    
    def count(self, **filters) -> int:
        """统计对象数量"""
        statement = select(self.model)
        
        for field, value in filters.items():
            if hasattr(self.model, field):
                statement = statement.where(getattr(self.model, field) == value)
        
        return len(self.session.exec(statement).all())
    
    def exists(self, **filters) -> bool:
        """检查对象是否存在"""
        statement = select(self.model)
        
        for field, value in filters.items():
            if hasattr(self.model, field):
                statement = statement.where(getattr(self.model, field) == value)
        
        return self.session.exec(statement).first() is not None
    
    def bulk_create(self, objects: List[ModelType]) -> List[ModelType]:
        """批量创建对象"""
        self.session.add_all(objects)
        self.session.commit()
        
        for obj in objects:
            self.session.refresh(obj)
        
        return objects
    
    def bulk_update(self, updates: List[Dict[str, Any]]) -> int:
        """批量更新对象"""
        count = 0
        for update_data in updates:
            obj_id = update_data.pop('id', None)
            if obj_id:
                obj = self.get(obj_id)
                if obj:
                    self.update(obj, update_data)
                    count += 1
        return count
```

**创建 `src/repositories/user.py`：**
```python
"""用户仓库"""

from typing import Optional, List
from sqlmodel import Session, select
from sqlalchemy.orm import selectinload

from .base import BaseRepository
from src.models import User, UserRole

class UserRepository(BaseRepository[User]):
    """用户仓库类"""
    
    def __init__(self, session: Session):
        super().__init__(User, session)
    
    def get_by_username(self, username: str) -> Optional[User]:
        """根据用户名获取用户"""
        statement = select(User).where(User.username == username)
        return self.session.exec(statement).first()
    
    def get_by_email(self, email: str) -> Optional[User]:
        """根据邮箱获取用户"""
        statement = select(User).where(User.email == email)
        return self.session.exec(statement).first()
    
    def get_active_users(self, skip: int = 0, limit: int = 100) -> List[User]:
        """获取活跃用户"""
        statement = (
            select(User)
            .where(User.is_active == True)
            .offset(skip)
            .limit(limit)
            .order_by(User.created_at.desc())
        )
        return self.session.exec(statement).all()
    
    def get_users_by_role(self, role: UserRole) -> List[User]:
        """根据角色获取用户"""
        statement = select(User).where(User.role == role)
        return self.session.exec(statement).all()
    
    def get_with_documents(self, user_id: int) -> Optional[User]:
        """获取用户及其文档"""
        statement = (
            select(User)
            .options(selectinload(User.documents))
            .where(User.id == user_id)
        )
        return self.session.exec(statement).first()
    
    def get_with_queries(self, user_id: int) -> Optional[User]:
        """获取用户及其查询历史"""
        statement = (
            select(User)
            .options(selectinload(User.queries))
            .where(User.id == user_id)
        )
        return self.session.exec(statement).first()
    
    def update_last_login(self, user_id: int) -> bool:
        """更新最后登录时间"""
        user = self.get(user_id)
        if user:
            from datetime import datetime
            user.last_login_at = datetime.utcnow()
            self.session.add(user)
            self.session.commit()
            return True
        return False
    
    def deactivate_user(self, user_id: int) -> bool:
        """停用用户"""
        user = self.get(user_id)
        if user:
            user.is_active = False
            self.session.add(user)
            self.session.commit()
            return True
        return False
    
    def search_users(self, query: str, limit: int = 10) -> List[User]:
        """搜索用户"""
        statement = (
            select(User)
            .where(
                (User.username.ilike(f"%{query}%")) |
                (User.email.ilike(f"%{query}%"))
            )
            .limit(limit)
        )
        return self.session.exec(statement).all()
```

**创建 `src/repositories/document.py`：**
```python
"""文档仓库"""

from typing import Optional, List
from sqlmodel import Session, select
from sqlalchemy.orm import selectinload
from datetime import datetime

from .base import BaseRepository
from src.models import Document, DocumentChunk, DocumentStatus

class DocumentRepository(BaseRepository[Document]):
    """文档仓库类"""
    
    def __init__(self, session: Session):
        super().__init__(Document, session)
    
    def get_by_user(
        self, 
        user_id: int, 
        status: Optional[DocumentStatus] = None,
        skip: int = 0,
        limit: int = 100
    ) -> List[Document]:
        """获取用户的文档"""
        statement = (
            select(Document)
            .where(Document.user_id == user_id)
            .offset(skip)
            .limit(limit)
            .order_by(Document.created_at.desc())
        )
        
        if status:
            statement = statement.where(Document.status == status)
        
        return self.session.exec(statement).all()
    
    def get_with_chunks(self, document_id: int) -> Optional[Document]:
        """获取文档及其块"""
        statement = (
            select(Document)
            .options(selectinload(Document.chunks))
            .where(Document.id == document_id)
        )
        return self.session.exec(statement).first()
    
    def get_by_file_hash(self, file_hash: str) -> Optional[Document]:
        """根据文件哈希获取文档"""
        statement = select(Document).where(Document.file_hash == file_hash)
        return self.session.exec(statement).first()
    
    def get_by_status(self, status: DocumentStatus) -> List[Document]:
        """根据状态获取文档"""
        statement = (
            select(Document)
            .where(Document.status == status)
            .order_by(Document.created_at.asc())
        )
        return self.session.exec(statement).all()
    
    def update_status(
        self, 
        document_id: int, 
        status: DocumentStatus, 
        error_message: Optional[str] = None
    ) -> bool:
        """更新文档状态"""
        document = self.get(document_id)
        if document:
            document.status = status
            if error_message:
                document.error_message = error_message
            if status == DocumentStatus.COMPLETED:
                document.processed_at = datetime.utcnow()
            
            self.session.add(document)
            self.session.commit()
            return True
        return False
    
    def update_stats(
        self, 
        document_id: int, 
        page_count: Optional[int] = None,
        word_count: Optional[int] = None,
        chunk_count: Optional[int] = None
    ) -> bool:
        """更新文档统计信息"""
        document = self.get(document_id)
        if document:
            if page_count is not None:
                document.page_count = page_count
            if word_count is not None:
                document.word_count = word_count
            if chunk_count is not None:
                document.chunk_count = chunk_count
            
            self.session.add(document)
            self.session.commit()
            return True
        return False
    
    def search_documents(self, user_id: int, query: str, limit: int = 10) -> List[Document]:
        """搜索用户的文档"""
        statement = (
            select(Document)
            .where(Document.user_id == user_id)
            .where(
                (Document.title.ilike(f"%{query}%")) |
                (Document.description.ilike(f"%{query}%"))
            )
            .limit(limit)
        )
        return self.session.exec(statement).all()
    
    def get_recent_documents(self, user_id: int, days: int = 7, limit: int = 10) -> List[Document]:
        """获取用户最近的文档"""
        from datetime import timedelta
        
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        statement = (
            select(Document)
            .where(Document.user_id == user_id)
            .where(Document.created_at >= cutoff_date)
            .order_by(Document.created_at.desc())
            .limit(limit)
        )
        return self.session.exec(statement).all()

class DocumentChunkRepository(BaseRepository[DocumentChunk]):
    """文档块仓库类"""
    
    def __init__(self, session: Session):
        super().__init__(DocumentChunk, session)
    
    def get_by_document(
        self, 
        document_id: int,
        skip: int = 0,
        limit: int = 100
    ) -> List[DocumentChunk]:
        """获取文档的所有块"""
        statement = (
            select(DocumentChunk)
            .where(DocumentChunk.document_id == document_id)
            .order_by(DocumentChunk.chunk_index.asc())
            .offset(skip)
            .limit(limit)
        )
        return self.session.exec(statement).all()
    
    def get_by_vector_id(self, vector_id: str) -> Optional[DocumentChunk]:
        """根据向量ID获取块"""
        statement = select(DocumentChunk).where(DocumentChunk.vector_id == vector_id)
        return self.session.exec(statement).first()
    
    def update_vector_info(
        self, 
        chunk_id: int, 
        vector_id: str, 
        embedding_model: str
    ) -> bool:
        """更新块的向量信息"""
        chunk = self.get(chunk_id)
        if chunk:
            chunk.vector_id = vector_id
            chunk.embedding_model = embedding_model
            self.session.add(chunk)
            self.session.commit()
            return True
        return False
    
    def bulk_create_chunks(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """批量创建文档块"""
        return self.bulk_create(chunks)
    
    def search_chunks(self, document_id: int, query: str, limit: int = 10) -> List[DocumentChunk]:
        """搜索文档块"""
        statement = (
            select(DocumentChunk)
            .where(DocumentChunk.document_id == document_id)
            .where(DocumentChunk.content.ilike(f"%{query}%"))
            .limit(limit)
        )
        return self.session.exec(statement).all()
```

### 3.2 测试仓库模式

**创建 `test_repositories.py`：**
```python
"""测试仓库模式"""

from src.database import get_db_session
from src.repositories import UserRepository, DocumentRepository, DocumentChunkRepository
from src.models import User, Document, DocumentChunk, UserRole, DocumentStatus
import hashlib

def test_user_repository():
    """测试用户仓库"""
    print("=== 测试用户仓库 ===")
    
    with get_db_session() as session:
        user_repo = UserRepository(session)
        
        # 创建用户
        user = User(
            username="repo_test_user",
            email="repo_test@example.com",
            password_hash="hashed_password",
            role=UserRole.USER
        )
        
        created_user = user_repo.create(user)
        print(f"✅ 用户创建成功，ID: {created_user.id}")
        
        # 根据用户名查询
        found_user = user_repo.get_by_username("repo_test_user")
        if found_user:
            print(f"✅ 根据用户名查询成功: {found_user.username}")
        
        # 根据邮箱查询
        found_user = user_repo.get_by_email("repo_test@example.com")
        if found_user:
            print(f"✅ 根据邮箱查询成功: {found_user.email}")
        
        # 更新用户
        updated_user = user_repo.update(found_user, {
            "preferred_language": "en",
            "max_query_length": 2000
        })
        print(f"✅ 用户更新成功，首选语言: {updated_user.preferred_language}")
        
        # 更新最后登录时间
        user_repo.update_last_login(found_user.id)
        print("✅ 最后登录时间更新成功")
        
        # 搜索用户
        search_results = user_repo.search_users("repo_test")
        print(f"✅ 用户搜索成功，找到 {len(search_results)} 个结果")
        
        # 清理测试数据
        user_repo.delete(found_user.id)
        print("✅ 测试用户删除成功")

def test_document_repository():
    """测试文档仓库"""
    print("\n=== 测试文档仓库 ===")
    
    with get_db_session() as session:
        user_repo = UserRepository(session)
        doc_repo = DocumentRepository(session)
        
        # 先创建一个用户
        user = User(
            username="doc_test_user",
            email="doc_test@example.com",
            password_hash="hashed_password"
        )
        created_user = user_repo.create(user)
        
        # 创建文档
        content = "这是测试文档内容"
        file_hash = hashlib.sha256(content.encode()).hexdigest()
        
        document = Document(
            title="测试文档",
            description="这是一个测试文档",
            original_filename="test.txt",
            file_path="/tmp/test.txt",
            file_size=len(content.encode()),
            mime_type="text/plain",
            file_hash=file_hash,
            user_id=created_user.id
        )
        
        created_doc = doc_repo.create(document)
        print(f"✅ 文档创建成功，ID: {created_doc.id}")
        
        # 根据用户获取文档
        user_docs = doc_repo.get_by_user(created_user.id)
        print(f"✅ 用户文档查询成功，找到 {len(user_docs)} 个文档")
        
        # 根据文件哈希查询
        found_doc = doc_repo.get_by_file_hash(file_hash)
        if found_doc:
            print(f"✅ 根据文件哈希查询成功: {found_doc.title}")
        
        # 更新文档状态
        doc_repo.update_status(created_doc.id, DocumentStatus.PROCESSING)
        print("✅ 文档状态更新成功")
        
        # 更新文档统计
        doc_repo.update_stats(created_doc.id, page_count=1, word_count=100)
        print("✅ 文档统计更新成功")
        
        # 搜索文档
        search_results = doc_repo.search_documents(created_user.id, "测试")
        print(f"✅ 文档搜索成功，找到 {len(search_results)} 个结果")
        
        # 清理测试数据
        doc_repo.delete(created_doc.id)
        user_repo.delete(created_user.id)
        print("✅ 测试数据清理成功")

def test_document_chunk_repository():
    """测试文档块仓库"""
    print("\n=== 测试文档块仓库 ===")
    
    with get_db_session() as session:
        user_repo = UserRepository(session)
        doc_repo = DocumentRepository(session)
        chunk_repo = DocumentChunkRepository(session)
        
        # 创建用户和文档
        user = User(
            username="chunk_test_user",
            email="chunk_test@example.com",
            password_hash="hashed_password"
        )
        created_user = user_repo.create(user)
        
        document = Document(
            title="块测试文档",
            original_filename="chunk_test.txt",
            file_path="/tmp/chunk_test.txt",
            file_size=1000,
            mime_type="text/plain",
            file_hash="chunk_test_hash",
            user_id=created_user.id
        )
        created_doc = doc_repo.create(document)
        
        # 创建文档块
        chunks = []
        for i in range(3):
            chunk = DocumentChunk(
                content=f"这是第 {i+1} 个文档块的内容",
                chunk_index=i,
                page_number=1,
                token_count=20,
                character_count=50,
                document_id=created_doc.id
            )
            chunks.append(chunk)
        
        # 批量创建块
        created_chunks = chunk_repo.bulk_create_chunks(chunks)
        print(f"✅ 批量创建文档块成功，创建了 {len(created_chunks)} 个块")
        
        # 获取文档的所有块
        doc_chunks = chunk_repo.get_by_document(created_doc.id)
        print(f"✅ 文档块查询成功，找到 {len(doc_chunks)} 个块")
        
        # 更新块的向量信息
        if doc_chunks:
            chunk_repo.update_vector_info(
                doc_chunks[0].id, 
                "vector_123", 
                "bge-m3"
            )
            print("✅ 块向量信息更新成功")
        
        # 搜索块
        search_results = chunk_repo.search_chunks(created_doc.id, "第 1 个")
        print(f"✅ 块搜索成功，找到 {len(search_results)} 个结果")
        
        # 清理测试数据
        for chunk in created_chunks:
            chunk_repo.delete(chunk.id)
        doc_repo.delete(created_doc.id)
        user_repo.delete(created_user.id)
        print("✅ 测试数据清理成功")

if __name__ == "__main__":
    print("开始仓库模式测试...")
    
    try:
        test_user_repository()
        test_document_repository()
        test_document_chunk_repository()
        print("\n✅ 所有仓库测试通过！")
    except Exception as e:
        print(f"\n❌ 测试失败: {e}")
```

**运行测试：**
```bash
python test_repositories.py
```

---

## 🔬 Exercise四：Alembic数据迁移

### 4.1 配置Alembic

**安装Alembic：**
```bash
uv add alembic
```

**初始化Alembic：**
```bash
# 在项目根目录下初始化
alembic init alembic
```

**配置 `alembic.ini`：**
```ini
# Alembic Config file

[alembic]
# path to migration scripts
script_location = alembic

# template used to generate migration file names
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
timezone = UTC

# max length of characters to apply to the "slug" field
truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
sourceless = false

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses
# os.pathsep. If this key is omitted entirely, it falls back to the legacy
# behavior of splitting on spaces and/or commas.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
output_encoding = utf-8

# 数据库连接URL（从环境变量读取）
sqlalchemy.url = postgresql://rag:ragpass@localhost:5432/rag_db

[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**配置 `alembic/env.py`：**
```python
"""Alembic环境配置"""

from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# 添加项目根目录到Python路径
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# 导入模型
from src.models import *
from sqlmodel import SQLModel

# Alembic配置对象
config = context.config

# 从环境变量读取数据库URL
if not config.get_main_option("sqlalchemy.url"):
    from src.database.config import db_config
    config.set_main_option("sqlalchemy.url", db_config.database_url)

# 配置日志
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# 设置目标元数据
target_metadata = SQLModel.metadata

def run_migrations_offline() -> None:
    """离线模式运行迁移"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    """在线模式运行迁移"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### 4.2 创建初始迁移

**生成初始迁移：**
```bash
# 生成初始迁移文件
alembic revision --autogenerate -m "Initial migration"
```

**应用迁移：**
```bash
# 应用迁移到数据库
alembic upgrade head
```

**查看迁移状态：**
```bash
# 查看当前迁移状态
alembic current

# 查看迁移历史
alembic history --verbose
```

### 4.3 创建数据迁移脚本

**创建 `scripts/migrate_data.py`：**
```python
"""数据迁移脚本"""

from src.database import get_db_session
from src.models import User, SystemConfig, UserRole
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_default_admin():
    """创建默认管理员用户"""
    with get_db_session() as session:
        # 检查是否已存在管理员
        from sqlmodel import select
        statement = select(User).where(User.role == UserRole.ADMIN)
        admin_exists = session.exec(statement).first()
        
        if not admin_exists:
            admin_user = User(
                username="admin",
                email="admin@example.com",
                password_hash="$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewdBPj3bp.gm.04i",  # password: admin123
                role=UserRole.ADMIN,
                is_active=True,
                preferred_language="zh",
                max_query_length=5000
            )
            
            session.add(admin_user)
            session.commit()
            logger.info("✅ 默认管理员用户创建成功")
        else:
            logger.info("ℹ️ 管理员用户已存在，跳过创建")

def create_system_configs():
    """创建系统配置"""
    with get_db_session() as session:
        configs = [
            {
                "key": "max_file_size",
                "value": "10485760",  # 10MB
                "description": "最大文件上传大小（字节）"
            },
            {
                "key": "supported_file_types",
                "value": "pdf,txt,docx,md",
                "description": "支持的文件类型"
            },
            {
                "key": "max_chunk_size",
                "value": "1000",
                "description": "最大块大小（字符数）"
            },
            {
                "key": "chunk_overlap",
                "value": "200",
                "description": "块重叠大小（字符数）"
            },
            {
                "key": "embedding_model",
                "value": "BAAI/bge-m3",
                "description": "默认嵌入模型"
            },
            {
                "key": "llm_model",
                "value": "gpt-3.5-turbo",
                "description": "默认LLM模型"
            },
            {
                "key": "max_query_length",
                "value": "2000",
                "description": "最大查询长度"
            },
            {
                "key": "retrieval_top_k",
                "value": "5",
                "description": "检索返回的最大结果数"
            }
        ]
        
        from sqlmodel import select
        for config_data in configs:
            # 检查配置是否已存在
            statement = select(SystemConfig).where(SystemConfig.key == config_data["key"])
            existing_config = session.exec(statement).first()
            
            if not existing_config:
                config = SystemConfig(**config_data)
                session.add(config)
                logger.info(f"✅ 系统配置创建成功: {config_data['key']}")
            else:
                logger.info(f"ℹ️ 系统配置已存在: {config_data['key']}")
        
        session.commit()

def migrate_existing_data():
    """迁移现有数据（示例）"""
    with get_db_session() as session:
        # 这里可以添加数据迁移逻辑
        # 例如：更新现有用户的默认设置
        from sqlmodel import select
        
        statement = select(User).where(User.preferred_language.is_(None))
        users_without_lang = session.exec(statement).all()
        
        for user in users_without_lang:
            user.preferred_language = "zh"
            session.add(user)
        
        if users_without_lang:
            session.commit()
            logger.info(f"✅ 更新了 {len(users_without_lang)} 个用户的默认语言设置")

def run_data_migration():
    """运行数据迁移"""
    logger.info("开始数据迁移...")
    
    try:
        create_default_admin()
        create_system_configs()
        migrate_existing_data()
        
        logger.info("✅ 数据迁移完成")
    except Exception as e:
        logger.error(f"❌ 数据迁移失败: {e}")
        raise

if __name__ == "__main__":
    run_data_migration()
```

**运行数据迁移：**
```bash
python scripts/migrate_data.py
```

---

## 🔬 Exercise五：数据库性能优化

### 5.1 创建索引优化脚本

**创建 `scripts/optimize_database.py`：**
```python
"""数据库性能优化脚本"""

from src.database import engine
from sqlalchemy import text
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_indexes():
    """创建性能优化索引"""
    indexes = [
        # 用户表索引
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email_active ON users(email) WHERE is_active = true;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_role_active ON users(role) WHERE is_active = true;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_last_login ON users(last_login_at DESC NULLS LAST);",
        
        # 文档表索引
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_user_status ON documents(user_id, status);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_file_hash ON documents(file_hash);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_created_desc ON documents(created_at DESC);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_title_gin ON documents USING gin(to_tsvector('english', title));",
        
        # 文档块表索引
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_document_index ON document_chunks(document_id, chunk_index);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_vector_id ON document_chunks(vector_id) WHERE vector_id IS NOT NULL;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_content_gin ON document_chunks USING gin(to_tsvector('english', content));",
        
        # 查询历史表索引
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_user_created ON query_history(user_id, created_at DESC);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_hash ON query_history(query_hash);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_status ON query_history(status);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_query_history_rating ON query_history(user_rating) WHERE user_rating IS NOT NULL;",
        
        # 系统配置表索引
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_system_config_active ON system_config(key) WHERE is_active = true;"
    ]
    
    with engine.connect() as conn:
        for index_sql in indexes:
            try:
                logger.info(f"创建索引: {index_sql.split('IF NOT EXISTS')[1].split('ON')[0].strip()}")
                conn.execute(text(index_sql))
                conn.commit()
                logger.info("✅ 索引创建成功")
            except Exception as e:
                logger.error(f"❌ 索引创建失败: {e}")

def analyze_tables():
    """分析表统计信息"""
    tables = [
        "users",
        "documents", 
        "document_chunks",
        "query_history",
        "system_config"
    ]
    
    with engine.connect() as conn:
        for table in tables:
            try:
                logger.info(f"分析表: {table}")
                conn.execute(text(f"ANALYZE {table};"))
                conn.commit()
                logger.info(f"✅ 表 {table} 分析完成")
            except Exception as e:
                logger.error(f"❌ 表 {table} 分析失败: {e}")

def vacuum_tables():
    """清理表空间"""
    tables = [
        "users",
        "documents", 
        "document_chunks",
        "query_history",
        "system_config"
    ]
    
    with engine.connect() as conn:
        for table in tables:
            try:
                logger.info(f"清理表: {table}")
                conn.execute(text(f"VACUUM ANALYZE {table};"))
                conn.commit()
                logger.info(f"✅ 表 {table} 清理完成")
            except Exception as e:
                logger.error(f"❌ 表 {table} 清理失败: {e}")

def get_table_stats():
    """获取表统计信息"""
    stats_query = """
    SELECT 
        schemaname,
        tablename,
        attname,
        n_distinct,
        correlation
    FROM pg_stats 
    WHERE schemaname = 'public'
    ORDER BY tablename, attname;
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(stats_query))
        rows = result.fetchall()
        
        logger.info("\n=== 表统计信息 ===")
        current_table = None
        for row in rows:
            if row.tablename != current_table:
                current_table = row.tablename
                logger.info(f"\n表: {current_table}")
            
            logger.info(f"  {row.attname}: distinct={row.n_distinct}, correlation={row.correlation}")

def optimize_database():
    """执行数据库优化"""
    logger.info("开始数据库性能优化...")
    
    try:
        create_indexes()
        analyze_tables()
        vacuum_tables()
        get_table_stats()
        
        logger.info("✅ 数据库优化完成")
    except Exception as e:
        logger.error(f"❌ 数据库优化失败: {e}")
        raise

if __name__ == "__main__":
    optimize_database()
```

### 5.2 创建性能监控脚本

**创建 `scripts/monitor_performance.py`：**
```python
"""数据库性能监控脚本"""

from src.database import engine, get_pool_status
from sqlalchemy import text
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def monitor_connections():
    """监控数据库连接"""
    query = """
    SELECT 
        count(*) as total_connections,
        count(*) FILTER (WHERE state = 'active') as active_connections,
        count(*) FILTER (WHERE state = 'idle') as idle_connections,
        count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction
    FROM pg_stat_activity 
    WHERE datname = current_database();
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query)).fetchone()
        
        logger.info("=== 数据库连接状态 ===")
        logger.info(f"总连接数: {result.total_connections}")
        logger.info(f"活跃连接: {result.active_connections}")
        logger.info(f"空闲连接: {result.idle_connections}")
        logger.info(f"事务中空闲: {result.idle_in_transaction}")
        
        # 连接池状态
        pool_status = get_pool_status()
        logger.info("\n=== 连接池状态 ===")
        logger.info(f"池大小: {pool_status['pool_size']}")
        logger.info(f"已检出: {pool_status['checked_out']}")
        logger.info(f"溢出连接: {pool_status['overflow']}")
        logger.info(f"无效连接: {pool_status['invalidated']}")

def monitor_slow_queries():
    """监控慢查询"""
    query = """
    SELECT 
        query,
        calls,
        total_time,
        mean_time,
        rows,
        100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
    FROM pg_stat_statements 
    WHERE mean_time > 100  -- 平均执行时间超过100ms
    ORDER BY mean_time DESC 
    LIMIT 10;
    """
    
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query)).fetchall()
            
            if result:
                logger.info("\n=== 慢查询监控 ===")
                for row in result:
                    logger.info(f"查询: {row.query[:100]}...")
                    logger.info(f"  调用次数: {row.calls}")
                    logger.info(f"  总时间: {row.total_time:.2f}ms")
                    logger.info(f"  平均时间: {row.mean_time:.2f}ms")
                    logger.info(f"  缓存命中率: {row.hit_percent:.2f}%")
                    logger.info("-" * 50)
            else:
                logger.info("\n=== 慢查询监控 ===\n暂无慢查询")
    except Exception as e:
        logger.warning(f"慢查询监控失败（可能需要安装pg_stat_statements扩展）: {e}")

def monitor_table_sizes():
    """监控表大小"""
    query = """
    SELECT 
        schemaname,
        tablename,
        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
        pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
    FROM pg_tables 
    WHERE schemaname = 'public'
    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query)).fetchall()
        
        logger.info("\n=== 表大小监控 ===")
        for row in result:
            logger.info(f"{row.tablename}: {row.size}")

def monitor_index_usage():
    """监控索引使用情况"""
    query = """
    SELECT 
        schemaname,
        tablename,
        indexname,
        idx_tup_read,
        idx_tup_fetch,
        idx_scan
    FROM pg_stat_user_indexes 
    WHERE schemaname = 'public'
    ORDER BY idx_scan DESC;
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query)).fetchall()
        
        logger.info("\n=== 索引使用监控 ===")
        for row in result:
            logger.info(f"{row.tablename}.{row.indexname}:")
            logger.info(f"  扫描次数: {row.idx_scan}")
            logger.info(f"  读取元组: {row.idx_tup_read}")
            logger.info(f"  获取元组: {row.idx_tup_fetch}")

def run_performance_monitor():
    """运行性能监控"""
    logger.info("开始数据库性能监控...")
    
    try:
        monitor_connections()
        monitor_slow_queries()
        monitor_table_sizes()
        monitor_index_usage()
        
        logger.info("\n✅ 性能监控完成")
    except Exception as e:
        logger.error(f"❌ 性能监控失败: {e}")
        raise

if __name__ == "__main__":
    run_performance_monitor()
```

**运行性能优化和监控：**
```bash
# 运行数据库优化
python scripts/optimize_database.py

# 运行性能监控
python scripts/monitor_performance.py
```

---

## 🤔 思考题

1. **数据模型设计**：
   - 为什么要使用UUID作为全局唯一标识符？
   - 如何设计文档和块之间的关系以支持高效查询？
   - 软删除和硬删除各有什么优缺点？

2. **数据库连接管理**：
   - 连接池的大小应该如何设置？
   - 什么情况下需要使用异步数据库连接？
   - 如何处理数据库连接超时和重连？

3. **仓库模式**：
   - 仓库模式相比直接使用ORM有什么优势？
   - 如何在仓库层实现缓存机制？
   - 批量操作如何优化性能？

4. **数据迁移**：
   - 如何设计向后兼容的数据库迁移？
   - 大表迁移时如何避免锁表？
   - 如何回滚失败的迁移？

5. **性能优化**：
   - 什么情况下应该创建复合索引？
   - 如何监控和优化慢查询？
   - 分区表在什么场景下有用？

---

## ✅ Exercise检查清单

### 基础功能检查
- [ ] SQLModel模型定义正确
- [ ] 数据库连接配置正常
- [ ] 基础CRUD操作正常
- [ ] 关系映射正确

### 高级功能检查
- [ ] 仓库模式实现完整
- [ ] 批量操作性能良好
- [ ] 事务处理正确
- [ ] 异步操作支持

### 迁移和优化检查
- [ ] Alembic配置正确
- [ ] 迁移脚本可执行
- [ ] 索引创建成功
- [ ] 性能监控正常

### 代码质量检查
- [ ] 类型注解完整
- [ ] 错误处理完善
- [ ] 日志记录详细
- [ ] 测试覆盖充分

---

## 🔧 常见问题解决

### 1. 数据库连接失败
```bash
# 检查PostgreSQL服务状态
docker ps | grep postgres

# 检查连接参数
psql -h localhost -p 5432 -U rag -d rag_db
```

### 2. 迁移失败
```bash
# 查看迁移状态
alembic current

# 强制标记迁移状态
alembic stamp head

# 重新生成迁移
alembic revision --autogenerate -m "Fix migration"
```

### 3. 性能问题
```bash
# 检查慢查询
psql -d rag_db -c "SELECT * FROM pg_stat_statements ORDER BY mean_time DESC LIMIT 5;"

# 分析表统计
psql -d rag_db -c "ANALYZE;"
```

---

## 📚 参考资料

- [SQLModel官方文档](https://sqlmodel.tiangolo.com/)
- [PostgreSQL性能优化](https://www.postgresql.org/docs/current/performance-tips.html)
- [Alembic迁移指南](https://alembic.sqlalchemy.org/en/latest/tutorial.html)
- [SQLAlchemy连接池](https://docs.sqlalchemy.org/en/14/core/pooling.html)
- [数据库设计最佳实践](https://www.postgresql.org/docs/current/ddl-best-practices.html)

---

## 🎯 Exercise完成标志

完成本Exercise后，你应该能够：
- ✅ 熟练使用SQLModel定义数据模型
- ✅ 配置和管理数据库连接池
- ✅ 实现完整的仓库模式
- ✅ 使用Alembic进行数据迁移
- ✅ 进行数据库性能优化和监控

## Exercise完成后的Git操作

### 为什么要进行Git提交？

完成lesson03 Exercise后，进行Git提交非常重要：

- **数据模型保存**: 保存SQLModel数据模型定义和关系映射
- **迁移脚本管理**: 版本控制数据库迁移脚本，确保数据库结构变更可追踪
- **仓库模式实现**: 保存完整的数据访问层代码
- **配置管理**: 保存数据库连接和性能优化配置
- **团队协作**: 确保团队成员使用相同的数据模型和迁移版本

### Git提交操作步骤

完成本课Exercise后，请按照以下步骤提交你的代码：

#### 步骤1：检查当前状态

```bash
# 查看当前Git状态
git status

# 查看具体的文件变更
git diff
```

**预期看到的文件变更**:
- `models/` - 数据模型定义文件
- `database/` - 数据库连接和配置
- `repositories/` - 仓库模式实现
- `alembic/` - 数据库迁移相关文件
- `scripts/` - 数据库管理和优化脚本
- `alembic.ini` - Alembic配置文件

#### 步骤2：添加文件到暂存区

```bash
# 添加所有更改的文件
git add .

# 或者选择性添加重要文件
git add models/
git add database/
git add repositories/
git add alembic/
git add scripts/
git add alembic.ini

# 再次检查状态
git status
```

**重要提醒**: 确保不要提交包含敏感数据库密码的配置文件，只提交模板文件。

#### 步骤3：提交更改

```bash
# 提交更改并添加描述信息
git commit -m "完成lesson03 Exercise：实现数据模型与迁移功能"

# 或者使用更详细的提交信息
git commit -m "完成lesson03 Exercise：数据模型与迁移

- 使用SQLModel定义Document和Chunk数据模型
- 实现数据库连接池和配置管理
- 创建完整的仓库模式数据访问层
- 配置Alembic数据库迁移工具
- 实现批量操作和事务处理
- 添加数据库性能优化和监控功能
- 创建数据库管理和测试脚本"
```

#### 步骤4：验证提交结果

```bash
# 查看提交历史
git log --oneline -5

# 查看最近一次提交的详细信息
git show HEAD

# 查看提交的文件列表
git show --name-only HEAD
```

### 数据库项目的特殊注意事项

#### 1. 迁移文件管理
```bash
# 确保迁移文件被正确提交
git add alembic/versions/*.py

# 检查迁移文件的完整性
ls -la alembic/versions/
```

#### 2. 敏感信息保护
```bash
# 确保数据库密码不被提交
echo "*.env" >> .gitignore
echo "database_url_with_password.txt" >> .gitignore

# 只提交配置模板
git add .env.example
```

#### 3. 数据库状态文件
```bash
# 通常不需要提交数据库文件
echo "*.db" >> .gitignore
echo "*.sqlite" >> .gitignore
echo "data/" >> .gitignore
```

### 验证数据模型和迁移

提交前，建议进行最后一次完整验证：

```bash
# 重置数据库（在测试环境中）
docker-compose down -v
docker-compose up -d postgres

# 等待数据库启动
sleep 10

# 运行迁移
alembic upgrade head

# 验证数据模型
python scripts/test_models.py

# 测试仓库功能
python scripts/test_repositories.py

# 如果一切正常，再进行提交
git add .
git commit -m "验证通过：完成lesson03数据模型和迁移"
```

### 可选：推送到远程仓库

```bash
# 推送到远程仓库
git push origin lesson03

# 如果是第一次推送该分支
git push -u origin lesson03
```

### 常见问题和解决方案

#### Q1: 迁移文件冲突
**问题**: 多人开发时迁移文件版本冲突
**解决方案**:
```bash
# 查看迁移历史
alembic history

# 合并迁移文件
alembic merge -m "合并迁移" head1 head2

# 重新生成迁移
alembic revision --autogenerate -m "修复迁移冲突"
```

#### Q2: 模型定义错误
**问题**: SQLModel模型定义导致迁移失败
**解决方案**:
```bash
# 检查模型语法
python -c "from models.document import Document; print('模型定义正确')"

# 重新生成迁移
alembic revision --autogenerate -m "修复模型定义"

# 检查生成的迁移脚本
cat alembic/versions/最新迁移文件.py
```

#### Q3: 数据库连接问题
**问题**: 提交后其他人无法连接数据库
**解决方案**:
```bash
# 确保环境变量模板完整
echo "DATABASE_URL=postgresql://user:password@localhost:5432/dbname" >> .env.example
echo "DB_POOL_SIZE=10" >> .env.example
echo "DB_MAX_OVERFLOW=20" >> .env.example

# 提供数据库初始化脚本
git add scripts/init_database.py
```

#### Q4: 性能优化配置
**问题**: 索引和优化配置在不同环境下表现不一致
**解决方案**:
```bash
# 创建环境特定的配置
echo "# 开发环境配置" >> config/development.py
echo "# 生产环境配置" >> config/production.py

# 提交配置文件
git add config/
```

### 数据库项目Git最佳实践

1. **迁移文件管理**: 每个迁移都应该有清晰的描述和版本号
   ```bash
   alembic revision --autogenerate -m "添加文档表索引优化"
   ```

2. **模型版本控制**: 重要的模型变更应该打标签
   ```bash
   git tag -a v0.3.0 -m "完成数据模型设计"
   git push origin v0.3.0
   ```

3. **测试数据管理**: 提供测试数据的生成脚本
   ```bash
   git add scripts/generate_test_data.py
   ```

4. **文档同步**: 确保数据模型文档与代码保持同步
   ```bash
   git add docs/database_schema.md
   ```

5. **备份策略**: 提供数据库备份和恢复脚本
   ```bash
   git add scripts/backup_database.py
   git add scripts/restore_database.py
   ```

### 数据库迁移的Git工作流

```bash
# 1. 创建新的迁移
alembic revision --autogenerate -m "添加新功能"

# 2. 检查生成的迁移文件
cat alembic/versions/最新文件.py

# 3. 测试迁移
alembic upgrade head

# 4. 测试回滚
alembic downgrade -1
alembic upgrade head

# 5. 提交迁移文件
git add alembic/versions/最新文件.py
git commit -m "添加数据库迁移：新功能"
```

### 下一步

完成Git提交后，你的数据模型和迁移功能已经安全保存。在lesson04中，我们将：
- 切换到 `lesson04` 分支
- 学习PDF文档解析技术
- 实现文档内容的智能分块
- 将解析结果存储到数据库中

数据模型是RAG系统的基础，良好的Git管理习惯将帮助你在复杂的数据库变更中保持代码的稳定性和可追溯性。记住，每次数据库结构变更都应该通过迁移脚本进行，并及时提交到版本控制系统中。

恭喜你完成了数据模型与迁移的学习！🎉