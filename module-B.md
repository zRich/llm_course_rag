å¥½çš„ï¼Œæˆ‘ä»¬æŒ‰ç…§æ¨¡å—Açš„è¯¦ç»†é£æ ¼ï¼Œå°† **æ¨¡å—Bï¼ˆæ£€ç´¢ä¼˜åŒ–ï¼ŒLesson 07â€“14ï¼‰** å±•å¼€æˆæ•™å¸ˆè®²ä¹‰ + å­¦ç”Ÿå®éªŒæŒ‡å¯¼ + æ€è€ƒé¢˜ç‰ˆæœ¬ã€‚

---

# æ¨¡å—Bï¼šæ£€ç´¢å¼ºåŒ–ä¸æ··åˆæœç´¢ï¼ˆLesson 07â€“14ï¼‰

---

## Lesson 07 â€“ å…³é”®è¯æ£€ç´¢

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šå®ç°åŸºäºå…³é”®è¯çš„å…¨æ–‡æ£€ç´¢ï¼ŒæŒæ¡æ–‡æœ¬ç´¢å¼•å’ŒæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯ã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * PostgreSQL 17 å…¨æ–‡æ£€ç´¢åŠŸèƒ½ï¼ˆtsvector/tsqueryï¼‰
  * ä¸­æ–‡åˆ†è¯ï¼ˆjieba æˆ– THULACï¼‰
  * æ£€ç´¢ç»“æœæ’åºä¸è¯„åˆ†
  * æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–ï¼ˆç´¢å¼•ã€æŸ¥è¯¢è®¡åˆ’ï¼‰

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
# å®‰è£…ä¸­æ–‡åˆ†è¯
uv pip install jieba

# å»ºç«‹å…¨æ–‡ç´¢å¼•
from sqlalchemy import text
from app.core.db import engine

with engine.connect() as conn:
    conn.execute(text("""
        CREATE INDEX idx_document_content
        ON document
        USING gin(to_tsvector('chinese', content));
    """))

# æŸ¥è¯¢ç¤ºä¾‹
from sqlalchemy import text
query = "äººå·¥æ™ºèƒ½"
with engine.connect() as conn:
    result = conn.execute(text("""
        SELECT * FROM document
        WHERE to_tsvector('chinese', content) @@ plainto_tsquery('chinese', :query)
        ORDER BY ts_rank(to_tsvector('chinese', content), plainto_tsquery('chinese', :query)) DESC
        LIMIT 5;
    """), {"query": query})
    for row in result:
        print(row)
```

### ğŸ¤” æ€è€ƒé¢˜

1. ä¸­æ–‡æ–‡æœ¬æ£€ç´¢ä¸è‹±æ–‡æ–‡æœ¬æ£€ç´¢æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
2. å¦‚ä½•æé«˜é•¿æ–‡æ¡£çš„æ£€ç´¢ç²¾åº¦å’Œæ•ˆç‡ï¼Ÿ

---

## Lesson 08 â€“ æ··åˆæ£€ç´¢èåˆç­–ç•¥

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šèåˆå‘é‡æ£€ç´¢ä¸å…³é”®è¯æ£€ç´¢ï¼Œæ„å»ºæ··åˆæ£€ç´¢ç³»ç»Ÿã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * å¤šè·¯æ£€ç´¢ç»“æœèåˆï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
  * æƒé‡è°ƒèŠ‚ç®—æ³•
  * å»é‡å’Œæ’åºç­–ç•¥
  * ç®€å•A/Bæµ‹è¯•æ¡†æ¶

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
# å‡è®¾å·²ç»æœ‰å‘é‡æ£€ç´¢ç»“æœ vector_hits å’Œå…³é”®è¯æ£€ç´¢ç»“æœ keyword_hits
# ç®€å•èåˆç¤ºä¾‹ï¼šåŠ æƒå¾—åˆ†
alpha = 0.6  # å‘é‡æƒé‡
beta = 0.4   # å…³é”®è¯æƒé‡

combined_results = {}
for hit in vector_hits:
    combined_results[hit["id"]] = combined_results.get(hit["id"], 0) + alpha * hit["score"]
for hit in keyword_hits:
    combined_results[hit["id"]] = combined_results.get(hit["id"], 0) + beta * hit["score"]

# æ’åºè¾“å‡º
final_results = sorted(combined_results.items(), key=lambda x: x[1], reverse=True)
print(final_results[:5])
```

### ğŸ¤” æ€è€ƒé¢˜

1. å¦‚ä½•ç¡®å®šèåˆæƒé‡ alpha/betaï¼Ÿ
2. å½“ä¸¤ä¸ªæ£€ç´¢ç»“æœå†²çªæ—¶ï¼Œå¦‚ä½•å¤„ç†ä¼˜å…ˆçº§ï¼Ÿ

---

## Lesson 09 â€“ å…ƒæ•°æ®è¿‡æ»¤

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šåŸºäºæ–‡æ¡£å…ƒæ•°æ®è¿›è¡Œæ£€ç´¢è¿‡æ»¤ï¼Œæé«˜ç²¾åº¦ã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * æ”¯æŒåŠ¨æ€å¤åˆæŸ¥è¯¢æ¡ä»¶
  * æ ‡ç­¾ã€ä½œè€…ã€æ—¥æœŸç­‰å…ƒæ•°æ®ç´¢å¼•
  * ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
  * æŸ¥è¯¢æ€§èƒ½ç›‘æ§

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
# å‡è®¾æ–‡æ¡£æœ‰å­—æ®µï¼šcategory, author, created_at
query = "äººå·¥æ™ºèƒ½"
filters = {"category": "AI", "author": "å¼ ä¸‰"}

with engine.connect() as conn:
    result = conn.execute(text("""
        SELECT * FROM document
        WHERE to_tsvector('chinese', content) @@ plainto_tsquery('chinese', :query)
          AND category = :category
          AND author = :author
        ORDER BY ts_rank(to_tsvector('chinese', content), plainto_tsquery('chinese', :query)) DESC
        LIMIT 5;
    """), {**filters, "query": query})
    for row in result:
        print(row)
```

### ğŸ¤” æ€è€ƒé¢˜

1. å…ƒæ•°æ®è¿‡æ»¤å¦‚ä½•å½±å“æ£€ç´¢å¬å›ç‡ï¼Ÿ
2. å¦‚ä½•åœ¨å¤§è§„æ¨¡æ•°æ®ä¸­ä¿æŒè¿‡æ»¤æŸ¥è¯¢æ€§èƒ½ï¼Ÿ

---

## Lesson 10 â€“ é‡æ’åº(Rerank)æ¥å…¥

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šé›†æˆé‡æ’åºæ¨¡å‹æå‡æ£€ç´¢ç²¾åº¦ã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * bge-reranker-v2-m3 æ¨¡å‹è°ƒç”¨
  * å‰å‘/æ‰¹é‡é‡æ’åº
  * ç¼“å­˜ç­–ç•¥ä¸æ€§èƒ½å¹³è¡¡
  * ç»“æœå¯è§£é‡Šæ€§

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder("bge-reranker-v2-m3")

# å¯¹ top-10 æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
pairs = [(query, doc["content"]) for doc in final_results[:10]]
scores = reranker.predict(pairs)

reranked = sorted(zip(final_results[:10], scores), key=lambda x: x[1], reverse=True)
for doc, score in reranked:
    print(doc["id"], score)
```

### ğŸ¤” æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆ Rerank å¯ä»¥æ˜¾è‘—æå‡ç»“æœç²¾åº¦ï¼Ÿ
2. å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æ£€ç´¢ç»“æœçš„ rerank æ€§èƒ½é—®é¢˜ï¼Ÿ

---

## Lesson 11 â€“ Chunkå°ºå¯¸ä¸é‡å å®éªŒ

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šä¼˜åŒ–æ–‡æœ¬åˆ†å—ç­–ç•¥ï¼Œæé«˜æ£€ç´¢ä¸ç”Ÿæˆæ•ˆæœã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * åŠ¨æ€åˆ†å—ç®—æ³•
  * é‡å ç­–ç•¥ä¼˜åŒ–
  * åˆ†å—è´¨é‡è¯„ä¼°
  * å‚æ•°è°ƒä¼˜å®éªŒ

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
chunk_sizes = [200, 400, 600]
overlaps = [50, 100, 150]

for size in chunk_sizes:
    for overlap in overlaps:
        temp_chunks = []
        for page in doc:
            text = page.get_text()
            for i in range(0, len(text), size - overlap):
                temp_chunks.append(text[i:i+size])
        print(f"Chunk size {size}, overlap {overlap}, total chunks: {len(temp_chunks)}")
```

### ğŸ¤” æ€è€ƒé¢˜

1. Chunkå¤§å°ä¸é‡å æ¯”ä¾‹å¯¹æ£€ç´¢ç²¾åº¦å’Œå¬å›ç‡çš„å½±å“ï¼Ÿ
2. å¦‚ä½•è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åˆ†å—ç­–ç•¥ï¼Ÿ

---

## Lesson 12 â€“ å¤šæ–‡æ¡£æºå¤„ç†

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šæ”¯æŒå¤šæ–‡æ¡£æ ¼å¼å’Œæ¥æºï¼Œç»Ÿä¸€å¤„ç†æ¥å£ã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * PDFã€Wordã€TXTã€HTML æ–‡æ¡£è§£æ
  * æ‰¹é‡å¤„ç†ä¼˜åŒ–
  * é”™è¯¯å¤„ç†æœºåˆ¶ä¸æ—¥å¿—
  * æ¥å£ç»Ÿä¸€åŒ–

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
from pathlib import Path
import fitz  # PDF
import docx  # Word

for file in Path("docs/").glob("*.*"):
    if file.suffix == ".pdf":
        doc = fitz.open(file)
        text = "".join([page.get_text() for page in doc])
    elif file.suffix == ".docx":
        doc = docx.Document(file)
        text = "\n".join([p.text for p in doc.paragraphs])
    else:
        text = file.read_text()
    # æ–‡æœ¬åˆ†å—å…¥åº“
```

### ğŸ¤” æ€è€ƒé¢˜

1. å¦‚ä½•ä¿è¯å¤šç§æ–‡æ¡£æ ¼å¼å¤„ç†ä¸€è‡´æ€§ï¼Ÿ
2. æ‰¹é‡å¤„ç†å¤§è§„æ¨¡æ–‡ä»¶æ—¶çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ï¼Ÿ

---

## Lesson 13 â€“ å¼•ç”¨ä¸å¯æº¯æºè¾“å‡º

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šåœ¨å›ç­”ä¸­æä¾›å¼•ç”¨å’Œæ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦ã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * å¼•ç”¨é“¾è¿½è¸ª
  * ç½®ä¿¡åº¦è®¡ç®—
  * å¯æº¯æºä¿¡æ¯å±•ç¤º
  * ä¸ç”Ÿæˆç»“æœç»“åˆ

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
# å‡è®¾æ¯ä¸ª chunk æœ‰ metadata: source_file, page_no
answer_chunks = reranked[:3]
for idx, (doc, score) in enumerate(answer_chunks):
    print(f"Answer {idx+1}:")
    print(doc["payload"]["text"])
    print(f"Source: {doc['payload']['source_file']}, Page: {doc['payload']['page_no']}, Score: {score}")
```

### ğŸ¤” æ€è€ƒé¢˜

1. å¦‚ä½•è®¾è®¡å¯æº¯æºç»“æ„ä»¥ä¾¿äºç”Ÿæˆç«¯ä½¿ç”¨ï¼Ÿ
2. å¤šæ¥æºæ–‡æ¡£ä¸­ï¼Œå¦‚ä½•åˆå¹¶å†²çªä¿¡æ¯ï¼Ÿ

---

## Lesson 14 â€“ ç¼“å­˜ç­–ç•¥

### ğŸ“ æ•™å¸ˆè®²ä¹‰è¦ç‚¹

* **è¯¾ç¨‹ç›®æ ‡**ï¼šé€šè¿‡å¤šå±‚ç¼“å­˜æå‡ç³»ç»Ÿæ€§èƒ½ã€‚
* **æŠ€æœ¯é‡ç‚¹**ï¼š

  * Redis å¤šçº§ç¼“å­˜
  * ç¼“å­˜å¤±æ•ˆä¸æ›´æ–°ç­–ç•¥
  * ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–
  * åˆ†å¸ƒå¼ç¼“å­˜è®¾è®¡

### ğŸ§ª å­¦ç”Ÿå®éªŒæŒ‡å¯¼

```python
import redis

cache = redis.Redis(host='localhost', port=6379, db=0)

# æ£€æŸ¥ç¼“å­˜
key = f"query:{query}"
if cache.exists(key):
    result = cache.get(key)
else:
    result = compute_results(query)
    cache.set(key, str(result), ex=3600)  # 1å°æ—¶è¿‡æœŸ
```

### ğŸ¤” æ€è€ƒé¢˜

1. ç¼“å­˜ç­–ç•¥å¦‚ä½•å¹³è¡¡å®æ—¶æ€§ä¸æ€§èƒ½ï¼Ÿ
2. åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•ä¿è¯ç¼“å­˜ä¸€è‡´æ€§ï¼Ÿ

---

æˆ‘å·²ç»å®Œæˆ **æ¨¡å—Bï¼ˆLesson 07â€“14ï¼‰è¯¦ç»†è®²ä¹‰ + å®éªŒæŒ‡å¯¼**ã€‚

å¦‚æœä½ ç¡®è®¤æ ¼å¼å’Œæ·±åº¦ï¼Œå¯ä»¥è®©æˆ‘ç»§ç»­æ•´ç† **æ¨¡å—Cï¼ˆLesson 15â€“20ï¼‰**ï¼Œç„¶åé€æ¨¡å—å®Œæˆç›´åˆ° Lesson 40ã€‚

ä½ å¸Œæœ›æˆ‘ç»§ç»­æ¨¡å—Cå—ï¼Ÿ
