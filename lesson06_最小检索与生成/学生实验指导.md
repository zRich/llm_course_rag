import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from src.llm.llm_client import LLMClient, ChatMessage, create_llm_client
from src.rag.retriever import RAGRetriever, RetrievalConfig
from src.rag.prompt_templates import PromptManager, RetrievedDocument
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore

@dataclass
class RAGConfig:
    """RAG系统配置"""
    collection_name: str
    prompt_template: str = "basic"
    retrieval_config: Optional[RetrievalConfig] = None
    enable_conversation: bool = False
    max_history_turns: int = 3

@dataclass
class RAGResponse:
    """RAG响应"""
    answer: str
    retrieved_documents: List[RetrievedDocument]
    prompt_used: str
    llm_usage: Dict[str, int]
    retrieval_time: float
    generation_time: float

class RAGSystem:
    """完整的RAG系统"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.llm_client = create_llm_client()
        self.embedder = TextEmbedder()
        self.vector_store = QdrantVectorStore()
        
        # 初始化检索配置
        if config.retrieval_config is None:
            config.retrieval_config = RetrievalConfig(
                collection_name=config.collection_name
            )
        
        self.retriever = RAGRetriever(
            qdrant_client=self.vector_store.client,
            embedder=self.embedder,
            config=config.retrieval_config
        )
        
        self.prompt_manager = PromptManager()
        self.conversation_history = []
    
    def query(self, question: str, filters: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """同步查询"""
        import time
        
        # 检索阶段
        start_time = time.time()
        retrieved_docs = self.retriever.search(question, filters)
        retrieval_time = time.time() - start_time
        
        # 生成阶段
        start_time = time.time()
        
        # 构建Prompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # 调用LLM
        messages = [ChatMessage(role="user", content=prompt)]
        llm_response = self.llm_client.chat(messages)
        
        generation_time = time.time() - start_time
        
        # 更新对话历史
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": llm_response.content
            })
        
        return RAGResponse(
            answer=llm_response.content,
            retrieved_documents=retrieved_docs,
            prompt_used=prompt,
            llm_usage=llm_response.usage,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )
    
    async def aquery(self, question: str, filters: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """异步查询"""
        import time
        
        # 检索阶段
        start_time = time.time()
        retrieved_docs = await self.retriever.asearch(question, filters)
        retrieval_time = time.time() - start_time
        
        # 生成阶段
        start_time = time.time()
        
        # 构建Prompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # 调用LLM
        messages = [ChatMessage(role="user", content=prompt)]
        llm_response = await self.llm_client.achat(messages)
        
        generation_time = time.time() - start_time
        
        # 更新对话历史
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": llm_response.content
            })
        
        return RAGResponse(
            answer=llm_response.content,
            retrieved_documents=retrieved_docs,
            prompt_used=prompt,
            llm_usage=llm_response.usage,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )
    
    async def stream_query(self, question: str, filters: Optional[Dict[str, Any]] = None) -> AsyncGenerator[str, None]:
        """流式查询"""
        # 检索阶段
        retrieved_docs = await self.retriever.asearch(question, filters)
        
        # 构建Prompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # 流式生成
        messages = [ChatMessage(role="user", content=prompt)]
        
        full_response = ""
        async for chunk in self.llm_client.stream_chat(messages):
            full_response += chunk
            yield chunk
        
        # 更新对话历史
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": full_response
            })
    
    def clear_conversation(self):
        """清空对话历史"""
        self.conversation_history.clear()
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """获取对话历史"""
        return self.conversation_history.copy()
    
    def get_system_info(self) -> Dict[str, Any]:
        """获取系统信息"""
        return {
            "collection_name": self.config.collection_name,
            "prompt_template": self.config.prompt_template,
            "retrieval_config": {
                "top_k": self.config.retrieval_config.top_k,
                "score_threshold": self.config.retrieval_config.score_threshold,
                "enable_reranking": self.config.retrieval_config.enable_reranking
            },
            "conversation_enabled": self.config.enable_conversation,
            "conversation_turns": len(self.conversation_history),
            "collection_info": self.retriever.get_collection_info()
        }

def create_rag_system(collection_name: str, **kwargs) -> RAGSystem:
    """创建RAG系统实例"""
    config = RAGConfig(collection_name=collection_name, **kwargs)
    return RAGSystem(config)