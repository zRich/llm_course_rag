# ç¬¬å…­è¯¾ï¼šæœ€å°æ£€ç´¢ä¸ç”Ÿæˆï¼ˆMVP RAGï¼‰- å­¦ç”Ÿå®éªŒæŒ‡å¯¼

## ä»£ç åŸºç¡€å‡†å¤‡

åœ¨å¼€å§‹æœ¬èŠ‚è¯¾çš„å®éªŒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åŸºäºä¸Šä¸€èŠ‚è¯¾çš„ä»£ç ç»§ç»­å¼€å‘ã€‚ç°åœ¨æˆ‘ä»¬ä½¿ç”¨Gitåˆ†æ”¯ç®¡ç†æ¥è·å–ä»£ç ã€‚

### æ­¥éª¤1ï¼šè¿›å…¥é¡¹ç›®ç›®å½•å¹¶åˆ‡æ¢åˆ†æ”¯

```bash
# è¿›å…¥rag-systemé¡¹ç›®ç›®å½•
cd rag-system

# åˆ‡æ¢åˆ°lesson06åˆ†æ”¯
git checkout lesson06

# éªŒè¯å½“å‰åˆ†æ”¯
git branch
# åº”è¯¥æ˜¾ç¤º * lesson06
```

### æ­¥éª¤2ï¼šéªŒè¯ä»£ç çŠ¶æ€

```bash
# æ£€æŸ¥é¡¹ç›®ç»“æ„
ls -la
# åº”è¯¥çœ‹åˆ°ï¼šsrc/ scripts/ test_documents/ docker-compose.yml ç­‰æ–‡ä»¶å’Œç›®å½•

# æ£€æŸ¥RAGç›¸å…³æ–‡ä»¶
ls -la src/llm/
# åº”è¯¥çœ‹åˆ°ï¼šllm_client.py rag_engine.py ç­‰æ–‡ä»¶
```

### æ­¥éª¤3ï¼šéªŒè¯å‘é‡åŒ–ç¯å¢ƒ

```bash
# å¯åŠ¨ä¾èµ–æœåŠ¡
docker-compose up -d

# æµ‹è¯•å‘é‡åŒ–åŠŸèƒ½
python -c "from src.embedding.embedder import TextEmbedder; print('å‘é‡åŒ–æ¨¡å—å¯¼å…¥æˆåŠŸ')"

# éªŒè¯Qdrantè¿æ¥
python -c "from qdrant_client import QdrantClient; client = QdrantClient('localhost', 6333); print('Qdrantè¿æ¥æˆåŠŸ')"
```

**è¯´æ˜**ï¼šlesson06åˆ†æ”¯åŒ…å«äº†lesson05çš„æ‰€æœ‰ä»£ç ï¼Œå¹¶æ–°å¢äº†LLMå®¢æˆ·ç«¯å’ŒRAGå¼•æ“ç›¸å…³çš„æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿã€‚

---

## ğŸ¯ å®éªŒç›®æ ‡

é€šè¿‡æœ¬å®éªŒï¼Œä½ å°†å­¦ä¼šï¼š
1. å®ç°å®Œæ•´çš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿ
2. é›†æˆå‘é‡æ£€ç´¢ä¸å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆ
3. è®¾è®¡æœ‰æ•ˆçš„Promptæ¨¡æ¿
4. æ„å»ºRAG APIæ¥å£
5. æµ‹è¯•å’Œä¼˜åŒ–RAGç³»ç»Ÿæ€§èƒ½

## ğŸ“‹ å®éªŒç¯å¢ƒå‡†å¤‡

### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- å·²å®Œæˆå‰5è¯¾çš„å®éªŒå†…å®¹
- Qdrantå‘é‡æ•°æ®åº“è¿è¡Œä¸­
- ç½‘ç»œè¿æ¥ï¼ˆç”¨äºè°ƒç”¨LLM APIï¼‰

### ä¾èµ–å®‰è£…

```bash
# å®‰è£…LLMç›¸å…³ä¾èµ–
pip install openai==1.3.0
pip install anthropic==0.7.0
pip install requests==2.31.0
pip install aiohttp==3.9.0
pip install tiktoken==0.5.0

# å®‰è£…å…¶ä»–å·¥å…·ä¾èµ–
pip install python-dotenv==1.0.0
pip install pydantic==2.5.0
pip install fastapi==0.104.0
pip install uvicorn==0.24.0
```

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# LLM APIé…ç½®
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# Qdranté…ç½®
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=

# åº”ç”¨é…ç½®
APP_HOST=0.0.0.0
APP_PORT=8000
DEBUG=true
```

### ç¯å¢ƒæµ‹è¯•

åˆ›å»º `test_environment.py` æµ‹è¯•è„šæœ¬ï¼š

```python
import os
import asyncio
from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient

def test_environment():
    """æµ‹è¯•å®éªŒç¯å¢ƒæ˜¯å¦å‡†å¤‡å°±ç»ª"""
    load_dotenv()
    
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ['OPENAI_API_KEY', 'QDRANT_HOST', 'QDRANT_PORT']
    missing_vars = []
    
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        return False
    
    # æµ‹è¯•OpenAIè¿æ¥
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=10
        )
        print("âœ… OpenAI APIè¿æ¥æ­£å¸¸")
    except Exception as e:
        print(f"âŒ OpenAI APIè¿æ¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•Qdrantè¿æ¥
    try:
        qdrant_client = QdrantClient(
            host=os.getenv('QDRANT_HOST'),
            port=int(os.getenv('QDRANT_PORT'))
        )
        collections = qdrant_client.get_collections()
        print("âœ… Qdrantè¿æ¥æ­£å¸¸")
    except Exception as e:
        print(f"âŒ Qdrantè¿æ¥å¤±è´¥: {e}")
        return False
    
    print("ğŸ‰ ç¯å¢ƒå‡†å¤‡å®Œæˆï¼")
    return True

if __name__ == "__main__":
    test_environment()
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
python test_environment.py
```

---

## ğŸ”§ å®éªŒä¸€ï¼šLLMå®¢æˆ·ç«¯å°è£…

### åˆ›å»ºLLMå®¢æˆ·ç«¯æ¨¡å—

åˆ›å»º `src/llm/llm_client.py`ï¼š

```python
import os
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import tiktoken
from openai import OpenAI, AsyncOpenAI
from pydantic import BaseModel

class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    LOCAL = "local"

@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    max_tokens: int = 1000
    temperature: float = 0.7
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str  # system, user, assistant
    content: str

class LLMResponse(BaseModel):
    """LLMå“åº”"""
    content: str
    usage: Dict[str, int]
    model: str
    finish_reason: str

class LLMClient:
    """LLMå®¢æˆ·ç«¯å°è£…"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.client = None
        self.async_client = None
        self._setup_client()
    
    def _setup_client(self):
        """è®¾ç½®å®¢æˆ·ç«¯"""
        if self.config.provider == LLMProvider.OPENAI:
            self.client = OpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url
            )
            self.async_client = AsyncOpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.config.provider}")
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        try:
            encoding = tiktoken.encoding_for_model(self.config.model)
            return len(encoding.encode(text))
        except:
            # ç®€å•ä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
            return len(text) // 4
    
    def chat(self, messages: List[ChatMessage]) -> LLMResponse:
        """åŒæ­¥èŠå¤©"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            openai_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in messages
            ]
            
            response = self.client.chat.completions.create(
                model=self.config.model,
                messages=openai_messages,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                frequency_penalty=self.config.frequency_penalty,
                presence_penalty=self.config.presence_penalty
            )
            
            return LLMResponse(
                content=response.choices[0].message.content,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                model=response.model,
                finish_reason=response.choices[0].finish_reason
            )
            
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {e}")
    
    async def stream_chat(self, messages: List[ChatMessage]) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            openai_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in messages
            ]
            
            stream = await self.async_client.chat.completions.create(
                model=self.config.model,
                messages=openai_messages,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            raise Exception(f"æµå¼LLMè°ƒç”¨å¤±è´¥: {e}")

def create_llm_client() -> LLMClient:
    """åˆ›å»ºLLMå®¢æˆ·ç«¯"""
    from dotenv import load_dotenv
    load_dotenv()
    
    config = LLMConfig(
        provider=LLMProvider.OPENAI,
        model=os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
        api_key=os.getenv('OPENAI_API_KEY'),
        base_url=os.getenv('OPENAI_BASE_URL'),
        max_tokens=int(os.getenv('MAX_TOKENS', '1000')),
        temperature=float(os.getenv('TEMPERATURE', '0.7'))
    )
    
    return LLMClient(config)
```

### LLMå®¢æˆ·ç«¯æµ‹è¯•

åˆ›å»º `test_llm.py` æµ‹è¯•è„šæœ¬ï¼š

```python
import asyncio
from src.llm.llm_client import create_llm_client, ChatMessage

def test_basic_chat():
    """æµ‹è¯•åŸºç¡€èŠå¤©åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•åŸºç¡€èŠå¤©åŠŸèƒ½...")
    
    client = create_llm_client()
    
    messages = [
        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"),
        ChatMessage(role="user", content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€ã€‚")
    ]
    
    try:
        response = client.chat(messages)
        print(f"âœ… èŠå¤©æˆåŠŸ")
        print(f"ğŸ“ å›ç­”: {response.content[:100]}...")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        return True
    except Exception as e:
        print(f"âŒ èŠå¤©å¤±è´¥: {e}")
        return False

async def test_async_chat():
    """æµ‹è¯•å¼‚æ­¥èŠå¤©åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å¼‚æ­¥èŠå¤©åŠŸèƒ½...")
    
    client = create_llm_client()
    
    messages = [
        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ã€‚"),
        ChatMessage(role="user", content="ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿè¯·ç®€è¦è¯´æ˜ã€‚")
    ]
    
    try:
        response = await client.achat(messages)
        print(f"âœ… å¼‚æ­¥èŠå¤©æˆåŠŸ")
        print(f"ğŸ“ å›ç­”: {response.content[:100]}...")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        return True
    except Exception as e:
        print(f"âŒ å¼‚æ­¥èŠå¤©å¤±è´¥: {e}")
        return False

async def test_stream_chat():
    """æµ‹è¯•æµå¼èŠå¤©åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æµå¼èŠå¤©åŠŸèƒ½...")
    
    client = create_llm_client()
    
    messages = [
        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªåˆ›æ„å†™ä½œåŠ©æ‰‹ã€‚"),
        ChatMessage(role="user", content="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ã€‚")
    ]
    
    try:
        print("ğŸ“ æµå¼è¾“å‡º:")
        async for chunk in client.stream_chat(messages):
            print(chunk, end="", flush=True)
        print("\nâœ… æµå¼èŠå¤©æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ æµå¼èŠå¤©å¤±è´¥: {e}")
        return False

def test_token_counting():
    """æµ‹è¯•Tokenè®¡æ•°åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•Tokenè®¡æ•°åŠŸèƒ½...")
    
    client = create_llm_client()
    
    test_texts = [
        "Hello, world!",
        "è¿™æ˜¯ä¸€æ®µä¸­æ–‡æµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯Tokenè®¡æ•°åŠŸèƒ½ã€‚",
        "This is a longer text that contains multiple sentences. It should have more tokens than the previous examples."
    ]
    
    for text in test_texts:
        token_count = client.count_tokens(text)
        print(f"ğŸ“ æ–‡æœ¬: {text[:50]}...")
        print(f"ğŸ”¢ Tokenæ•°é‡: {token_count}")
        print()
    
    return True

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹LLMå®¢æˆ·ç«¯æµ‹è¯•\n")
    
    tests = [
        ("åŸºç¡€èŠå¤©", test_basic_chat()),
        ("å¼‚æ­¥èŠå¤©", test_async_chat()),
        ("æµå¼èŠå¤©", test_stream_chat()),
        ("Tokenè®¡æ•°", test_token_counting())
    ]
    
    results = []
    for name, test in tests:
        if asyncio.iscoroutine(test):
            result = await test
        else:
            result = test
        results.append((name, result))
    
    print("\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {success_count}/{len(results)} æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    asyncio.run(main())
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
python test_llm.py
```

---

## ğŸ”§ å®éªŒäºŒï¼šPromptæ¨¡æ¿è®¾è®¡

### åˆ›å»ºPromptæ¨¡æ¿æ¨¡å—

åˆ›å»º `src/rag/prompt_templates.py`ï¼š

```python
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod

@dataclass
class RetrievedDocument:
    """æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    content: str
    metadata: Dict[str, Any]
    score: float
    source: Optional[str] = None

class PromptTemplate(ABC):
    """Promptæ¨¡æ¿åŸºç±»"""
    
    @abstractmethod
    def format(self, query: str, documents: List[RetrievedDocument], **kwargs) -> str:
        """æ ¼å¼åŒ–Prompt"""
        pass

class BasicRAGPrompt(PromptTemplate):
    """åŸºç¡€RAG Promptæ¨¡æ¿"""
    
    def __init__(self, system_message: Optional[str] = None):
        self.system_message = system_message or self._default_system_message()
    
    def _default_system_message(self) -> str:
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. åªåŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µæ”¯æŒä½ çš„å›ç­”
5. ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦"""
    
    def format(self, query: str, documents: List[RetrievedDocument], **kwargs) -> str:
        """æ ¼å¼åŒ–åŸºç¡€RAG Prompt"""
        # æ„å»ºå‚è€ƒæ–‡æ¡£éƒ¨åˆ†
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            source_info = f" (æ¥æº: {doc.source})" if doc.source else ""
            doc_sections.append(f"æ–‡æ¡£{i}{source_info}:\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        prompt = f"""{self.system_message}

å‚è€ƒæ–‡æ¡£ï¼š
{documents_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼š"""
        
        return prompt

class DetailedRAGPrompt(PromptTemplate):
    """è¯¦ç»†RAG Promptæ¨¡æ¿"""
    
    def format(self, query: str, documents: List[RetrievedDocument], **kwargs) -> str:
        """æ ¼å¼åŒ–è¯¦ç»†RAG Prompt"""
        # æ„å»ºå‚è€ƒæ–‡æ¡£éƒ¨åˆ†
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            metadata_info = []
            if doc.metadata:
                for key, value in doc.metadata.items():
                    if key not in ['chunk_id', 'vector'] and value:
                        metadata_info.append(f"{key}: {value}")
            
            meta_text = f" ({', '.join(metadata_info)})" if metadata_info else ""
            score_text = f" [ç›¸ä¼¼åº¦: {doc.score:.3f}]"
            
            doc_sections.append(f"æ–‡æ¡£{i}{meta_text}{score_text}:\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹å‚è€ƒæ–‡æ¡£ï¼Œå¹¶åŸºäºè¿™äº›æ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

## å‚è€ƒæ–‡æ¡£
{documents_text}

## ç”¨æˆ·é—®é¢˜
{query}

## å›ç­”è¦æ±‚
1. è¯·ä»”ç»†åˆ†æå‚è€ƒæ–‡æ¡£çš„å†…å®¹
2. åŸºäºæ–‡æ¡£å†…å®¹æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”
3. å¦‚æœéœ€è¦ï¼Œå¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µ
4. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
5. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œæ¡ç†æ€§

## å›ç­”
"""
        
        return prompt

class ConversationalRAGPrompt(PromptTemplate):
    """å¯¹è¯å¼RAG Promptæ¨¡æ¿"""
    
    def format(self, query: str, documents: List[RetrievedDocument], 
               conversation_history: Optional[List[Dict[str, str]]] = None, **kwargs) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å¼RAG Prompt"""
        # æ„å»ºå¯¹è¯å†å²
        history_text = ""
        if conversation_history:
            history_parts = []
            for turn in conversation_history[-3:]:  # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                history_parts.append(f"ç”¨æˆ·: {turn.get('user', '')}")
                history_parts.append(f"åŠ©æ‰‹: {turn.get('assistant', '')}")
            history_text = "\n".join(history_parts) + "\n\n"
        
        # æ„å»ºå‚è€ƒæ–‡æ¡£
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            source_info = f" (æ¥æº: {doc.source})" if doc.source else ""
            doc_sections.append(f"[æ–‡æ¡£{i}]{source_info}\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œæ­£åœ¨ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å’Œå¯¹è¯å†å²æ¥å›ç­”ç”¨æˆ·çš„æœ€æ–°é—®é¢˜ã€‚

## å¯¹è¯å†å²
{history_text}## å‚è€ƒæ–‡æ¡£
{documents_text}

## å½“å‰é—®é¢˜
{query}

è¯·ç»“åˆå¯¹è¯ä¸Šä¸‹æ–‡å’Œå‚è€ƒæ–‡æ¡£ï¼Œæä¾›æœ‰å¸®åŠ©çš„å›ç­”ï¼š"""
        
        return prompt

class StructuredRAGPrompt(PromptTemplate):
    """ç»“æ„åŒ–RAG Promptæ¨¡æ¿"""
    
    def format(self, query: str, documents: List[RetrievedDocument], 
               output_format: str = "markdown", **kwargs) -> str:
        """æ ¼å¼åŒ–ç»“æ„åŒ–RAG Prompt"""
        # æ„å»ºå‚è€ƒæ–‡æ¡£
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            doc_sections.append(f"### æ–‡æ¡£{i}\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        format_instructions = {
            "markdown": "è¯·ä½¿ç”¨Markdownæ ¼å¼å›ç­”ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€åˆ—è¡¨ã€ç²—ä½“ç­‰æ ¼å¼ã€‚",
            "json": "è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«answerã€sourcesã€confidenceç­‰å­—æ®µã€‚",
            "bullet_points": "è¯·ä»¥è¦ç‚¹å½¢å¼å›ç­”ï¼Œæ¯ä¸ªè¦ç‚¹ä¸€è¡Œï¼Œä½¿ç”¨-å¼€å¤´ã€‚"
        }
        
        format_instruction = format_instructions.get(output_format, format_instructions["markdown"])
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## å‚è€ƒæ–‡æ¡£
{documents_text}

## ç”¨æˆ·é—®é¢˜
{query}

## è¾“å‡ºè¦æ±‚
{format_instruction}

## å›ç­”
"""
        
        return prompt

class PromptManager:
    """Promptç®¡ç†å™¨"""
    
    def __init__(self):
        self.templates = {
            "basic": BasicRAGPrompt(),
            "detailed": DetailedRAGPrompt(),
            "conversational": ConversationalRAGPrompt(),
            "structured": StructuredRAGPrompt()
        }
    
    def get_template(self, template_name: str) -> PromptTemplate:
        """è·å–Promptæ¨¡æ¿"""
        if template_name not in self.templates:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡æ¿åç§°: {template_name}")
        return self.templates[template_name]
    
    def add_template(self, name: str, template: PromptTemplate):
        """æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿"""
        self.templates[name] = template
    
    def list_templates(self) -> List[str]:
         """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡æ¿"""
         return list(self.templates.keys())
```

---

## ğŸ”§ å®éªŒä¸‰ï¼šRAGæ£€ç´¢å™¨å®ç°

### åˆ›å»ºRAGæ£€ç´¢å™¨æ¨¡å—

åˆ›å»º `src/rag/retriever.py`ï¼š

```python
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from src.embedding.embedder import TextEmbedder
from src.rag.prompt_templates import RetrievedDocument

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    collection_name: str
    top_k: int = 5
    score_threshold: float = 0.7
    enable_reranking: bool = False
    diversity_threshold: float = 0.8
    max_tokens: int = 4000

class RAGRetriever:
    """RAGæ£€ç´¢å™¨"""
    
    def __init__(self, 
                 qdrant_client: QdrantClient,
                 embedder: TextEmbedder,
                 config: RetrievalConfig):
        self.qdrant_client = qdrant_client
        self.embedder = embedder
        self.config = config
    
    def search(self, query: str, filters: Optional[Dict[str, Any]] = None) -> List[RetrievedDocument]:
        """åŒæ­¥æ£€ç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embedder.embed_text(query)
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            search_filter = self._build_filter(filters) if filters else None
            
            # æ‰§è¡Œå‘é‡æœç´¢
            search_results = self.qdrant_client.search(
                collection_name=self.config.collection_name,
                query_vector=query_vector,
                limit=self.config.top_k * 2,  # è·å–æ›´å¤šç»“æœç”¨äºåå¤„ç†
                score_threshold=self.config.score_threshold,
                query_filter=search_filter
            )
            
            # è½¬æ¢ä¸ºRetrievedDocumentæ ¼å¼
            documents = []
            for result in search_results:
                doc = RetrievedDocument(
                    content=result.payload.get('content', ''),
                    metadata=result.payload,
                    score=result.score,
                    source=result.payload.get('source')
                )
                documents.append(doc)
            
            # åå¤„ç†
            documents = self._post_process(documents, query)
            
            return documents[:self.config.top_k]
            
        except Exception as e:
            raise Exception(f"æ£€ç´¢å¤±è´¥: {e}")
    
    async def asearch(self, query: str, filters: Optional[Dict[str, Any]] = None) -> List[RetrievedDocument]:
        """å¼‚æ­¥æ£€ç´¢"""
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ£€ç´¢
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search, query, filters)
    
    def _build_filter(self, filters: Dict[str, Any]) -> Filter:
        """æ„å»ºQdrantè¿‡æ»¤æ¡ä»¶"""
        conditions = []
        
        for key, value in filters.items():
            if isinstance(value, (str, int, float, bool)):
                conditions.append(
                    FieldCondition(key=key, match=MatchValue(value=value))
                )
            elif isinstance(value, list):
                for v in value:
                    conditions.append(
                        FieldCondition(key=key, match=MatchValue(value=v))
                    )
        
        return Filter(must=conditions) if conditions else None
    
    def _post_process(self, documents: List[RetrievedDocument], query: str) -> List[RetrievedDocument]:
        """åå¤„ç†æ£€ç´¢ç»“æœ"""
        # å»é‡
        documents = self._remove_duplicates(documents)
        
        # å¤šæ ·æ€§è¿‡æ»¤
        if self.config.diversity_threshold < 1.0:
            documents = self._apply_diversity_filter(documents)
        
        # é‡æ’åº
        if self.config.enable_reranking:
            documents = self._rerank(documents, query)
        
        # Tokené•¿åº¦æ§åˆ¶
        documents = self._control_token_length(documents)
        
        return documents
    
    def _remove_duplicates(self, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """å»é™¤é‡å¤æ–‡æ¡£"""
        seen_contents = set()
        unique_docs = []
        
        for doc in documents:
            content_hash = hash(doc.content)
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_docs.append(doc)
        
        return unique_docs
    
    def _apply_diversity_filter(self, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """åº”ç”¨å¤šæ ·æ€§è¿‡æ»¤"""
        if len(documents) <= 1:
            return documents
        
        # ç®€å•çš„å¤šæ ·æ€§è¿‡æ»¤ï¼šåŸºäºå†…å®¹ç›¸ä¼¼åº¦
        filtered_docs = [documents[0]]  # ä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆæœ€ç›¸å…³çš„ï¼‰
        
        for doc in documents[1:]:
            is_diverse = True
            for selected_doc in filtered_docs:
                # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºå…±åŒè¯æ±‡ï¼‰
                similarity = self._calculate_content_similarity(doc.content, selected_doc.content)
                if similarity > self.config.diversity_threshold:
                    is_diverse = False
                    break
            
            if is_diverse:
                filtered_docs.append(doc)
        
        return filtered_docs
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦"""
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _rerank(self, documents: List[RetrievedDocument], query: str) -> List[RetrievedDocument]:
        """é‡æ’åºæ–‡æ¡£"""
        # ç®€å•çš„é‡æ’åºï¼šç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…
        query_words = set(query.lower().split())
        
        for doc in documents:
            doc_words = set(doc.content.lower().split())
            keyword_score = len(query_words.intersection(doc_words)) / len(query_words)
            
            # ç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…åˆ†æ•°
            doc.score = 0.7 * doc.score + 0.3 * keyword_score
        
        # æŒ‰æ–°åˆ†æ•°æ’åº
        documents.sort(key=lambda x: x.score, reverse=True)
        return documents
    
    def _control_token_length(self, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """æ§åˆ¶æ€»Tokené•¿åº¦"""
        total_tokens = 0
        filtered_docs = []
        
        for doc in documents:
            # ç®€å•ä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
            doc_tokens = len(doc.content) // 4
            
            if total_tokens + doc_tokens <= self.config.max_tokens:
                filtered_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break
        
        return filtered_docs
    
    def get_collection_info(self) -> Dict[str, Any]:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            collection_info = self.qdrant_client.get_collection(self.config.collection_name)
            return {
                "name": self.config.collection_name,
                "vectors_count": collection_info.vectors_count,
                "points_count": collection_info.points_count,
                "status": collection_info.status
            }
        except Exception as e:
             return {"error": str(e)}
```

---

## ğŸ”§ å®éªŒå››ï¼šå®Œæ•´RAGç³»ç»Ÿå®ç°

### åˆ›å»ºRAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

åˆ›å»º `src/rag/rag_system.py`ï¼š

```python
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from src.llm.llm_client import LLMClient, ChatMessage, create_llm_client
from src.rag.retriever import RAGRetriever, RetrievalConfig
from src.rag.prompt_templates import PromptManager, RetrievedDocument
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore

@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®"""
    collection_name: str
    prompt_template: str = "basic"
    retrieval_config: Optional[RetrievalConfig] = None
    enable_conversation: bool = False
    max_history_turns: int = 3

@dataclass
class RAGResponse:
    """RAGå“åº”"""
    answer: str
    retrieved_documents: List[RetrievedDocument]
    prompt_used: str
    llm_usage: Dict[str, int]
    retrieval_time: float
    generation_time: float

class RAGSystem:
    """å®Œæ•´çš„RAGç³»ç»Ÿ"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.llm_client = create_llm_client()
        self.embedder = TextEmbedder()
        self.vector_store = QdrantVectorStore()
        
        # åˆå§‹åŒ–æ£€ç´¢é…ç½®
        if config.retrieval_config is None:
            config.retrieval_config = RetrievalConfig(
                collection_name=config.collection_name
            )
        
        self.retriever = RAGRetriever(
            qdrant_client=self.vector_store.client,
            embedder=self.embedder,
            config=config.retrieval_config
        )
        
        self.prompt_manager = PromptManager()
        self.conversation_history = []
    
    def query(self, question: str, filters: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """åŒæ­¥æŸ¥è¯¢"""
        import time
        
        # æ£€ç´¢é˜¶æ®µ
        start_time = time.time()
        retrieved_docs = self.retriever.search(question, filters)
        retrieval_time = time.time() - start_time
        
        # ç”Ÿæˆé˜¶æ®µ
        start_time = time.time()
        
        # æ„å»ºPrompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # è°ƒç”¨LLM
        messages = [ChatMessage(role="user", content=prompt)]
        llm_response = self.llm_client.chat(messages)
        
        generation_time = time.time() - start_time
        
        # æ›´æ–°å¯¹è¯å†å²
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": llm_response.content
            })
        
        return RAGResponse(
            answer=llm_response.content,
            retrieved_documents=retrieved_docs,
            prompt_used=prompt,
            llm_usage=llm_response.usage,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )
    
    async def aquery(self, question: str, filters: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """å¼‚æ­¥æŸ¥è¯¢"""
        import time
        
        # æ£€ç´¢é˜¶æ®µ
        start_time = time.time()
        retrieved_docs = await self.retriever.asearch(question, filters)
        retrieval_time = time.time() - start_time
        
        # ç”Ÿæˆé˜¶æ®µ
        start_time = time.time()
        
        # æ„å»ºPrompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # è°ƒç”¨LLM
        messages = [ChatMessage(role="user", content=prompt)]
        llm_response = await self.llm_client.achat(messages)
        
        generation_time = time.time() - start_time
        
        # æ›´æ–°å¯¹è¯å†å²
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": llm_response.content
            })
        
        return RAGResponse(
            answer=llm_response.content,
            retrieved_documents=retrieved_docs,
            prompt_used=prompt,
            llm_usage=llm_response.usage,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )
    
    async def stream_query(self, question: str, filters: Optional[Dict[str, Any]] = None) -> AsyncGenerator[str, None]:
        """æµå¼æŸ¥è¯¢"""
        # æ£€ç´¢é˜¶æ®µ
        retrieved_docs = await self.retriever.asearch(question, filters)
        
        # æ„å»ºPrompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # æµå¼ç”Ÿæˆ
        messages = [ChatMessage(role="user", content=prompt)]
        
        full_response = ""
        async for chunk in self.llm_client.stream_chat(messages):
            full_response += chunk
            yield chunk
        
        # æ›´æ–°å¯¹è¯å†å²
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": full_response
            })
    
    def clear_conversation(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.copy()
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "collection_name": self.config.collection_name,
            "prompt_template": self.config.prompt_template,
            "retrieval_config": {
                "top_k": self.config.retrieval_config.top_k,
                "score_threshold": self.config.retrieval_config.score_threshold,
                "enable_reranking": self.config.retrieval_config.enable_reranking
            },
            "conversation_enabled": self.config.enable_conversation,
            "conversation_turns": len(self.conversation_history),
            "collection_info": self.retriever.get_collection_info()
        }

def create_rag_system(collection_name: str, **kwargs) -> RAGSystem:
    """åˆ›å»ºRAGç³»ç»Ÿå®ä¾‹"""
    config = RAGConfig(collection_name=collection_name, **kwargs)
    return RAGSystem(config)
```

### RAGç³»ç»Ÿæµ‹è¯•

åˆ›å»º `test_rag_system.py` æµ‹è¯•è„šæœ¬ï¼š

```python
import asyncio
import time
from src.rag.rag_system import create_rag_system, RAGConfig, RetrievalConfig

def test_basic_rag():
    """æµ‹è¯•åŸºç¡€RAGåŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•åŸºç¡€RAGåŠŸèƒ½...")
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="basic"
    )
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "å¦‚ä½•å®‰è£…Pythonï¼Ÿ",
        "Pythonæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ"
    ]
    
    for question in questions:
        try:
            print(f"\nâ“ é—®é¢˜: {question}")
            response = rag_system.query(question)
            
            print(f"âœ… å›ç­”: {response.answer[:200]}...")
            print(f"ğŸ“Š æ£€ç´¢æ—¶é—´: {response.retrieval_time:.3f}s")
            print(f"ğŸ“Š ç”Ÿæˆæ—¶é—´: {response.generation_time:.3f}s")
            print(f"ğŸ“Š æ£€ç´¢åˆ° {len(response.retrieved_documents)} ä¸ªæ–‡æ¡£")
            print(f"ğŸ“Š Tokenä½¿ç”¨: {response.llm_usage}")
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return False
    
    return True

async def test_async_rag():
    """æµ‹è¯•å¼‚æ­¥RAGåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å¼‚æ­¥RAGåŠŸèƒ½...")
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="detailed"
    )
    
    question = "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚"
    
    try:
        print(f"â“ é—®é¢˜: {question}")
        response = await rag_system.aquery(question)
        
        print(f"âœ… å›ç­”: {response.answer[:300]}...")
        print(f"ğŸ“Š æ£€ç´¢æ—¶é—´: {response.retrieval_time:.3f}s")
        print(f"ğŸ“Š ç”Ÿæˆæ—¶é—´: {response.generation_time:.3f}s")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯
        print(f"\nğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£:")
        for i, doc in enumerate(response.retrieved_documents, 1):
            print(f"  æ–‡æ¡£{i}: ç›¸ä¼¼åº¦={doc.score:.3f}, é•¿åº¦={len(doc.content)}å­—ç¬¦")
            if doc.source:
                print(f"    æ¥æº: {doc.source}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¼‚æ­¥æŸ¥è¯¢å¤±è´¥: {e}")
        return False

async def test_stream_rag():
    """æµ‹è¯•æµå¼RAGåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æµå¼RAGåŠŸèƒ½...")
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="basic"
    )
    
    question = "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½å‘å±•å†ç¨‹çš„ç®€çŸ­æ–‡ç« ã€‚"
    
    try:
        print(f"â“ é—®é¢˜: {question}")
        print("ğŸ“ æµå¼å›ç­”:")
        
        async for chunk in rag_system.stream_query(question):
            print(chunk, end="", flush=True)
        
        print("\nâœ… æµå¼æŸ¥è¯¢æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {e}")
        return False

def test_conversation_rag():
    """æµ‹è¯•å¯¹è¯å¼RAGåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å¯¹è¯å¼RAGåŠŸèƒ½...")
    
    # åˆ›å»ºæ”¯æŒå¯¹è¯çš„RAGç³»ç»Ÿ
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="conversational",
        enable_conversation=True,
        max_history_turns=3
    )
    
    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    conversation = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "å®ƒå’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "èƒ½ä¸¾ä¸ªå…·ä½“çš„åº”ç”¨ä¾‹å­å—ï¼Ÿ"
    ]
    
    try:
        for i, question in enumerate(conversation, 1):
            print(f"\nğŸ—£ï¸ ç¬¬{i}è½®å¯¹è¯")
            print(f"â“ ç”¨æˆ·: {question}")
            
            response = rag_system.query(question)
            print(f"ğŸ¤– åŠ©æ‰‹: {response.answer[:200]}...")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        history = rag_system.get_conversation_history()
        print(f"\nğŸ“š å¯¹è¯å†å² ({len(history)} è½®):")
        for i, turn in enumerate(history, 1):
            print(f"  ç¬¬{i}è½® - ç”¨æˆ·: {turn['user'][:50]}...")
            print(f"        åŠ©æ‰‹: {turn['assistant'][:50]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_advanced_retrieval():
    """æµ‹è¯•é«˜çº§æ£€ç´¢åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•é«˜çº§æ£€ç´¢åŠŸèƒ½...")
    
    # åˆ›å»ºå¸¦é«˜çº§æ£€ç´¢é…ç½®çš„RAGç³»ç»Ÿ
    retrieval_config = RetrievalConfig(
        collection_name="documents",
        top_k=10,
        score_threshold=0.6,
        enable_reranking=True,
        diversity_threshold=0.7,
        max_tokens=3000
    )
    
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="structured",
        retrieval_config=retrieval_config
    )
    
    question = "æ¯”è¾ƒä¸åŒç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯"
    
    try:
        print(f"â“ é—®é¢˜: {question}")
        
        # ä½¿ç”¨è¿‡æ»¤æ¡ä»¶
        filters = {"document_type": "tutorial"}
        response = rag_system.query(question, filters=filters)
        
        print(f"âœ… å›ç­”: {response.answer[:300]}...")
        print(f"ğŸ“Š æ£€ç´¢é…ç½®: top_k={retrieval_config.top_k}, é˜ˆå€¼={retrieval_config.score_threshold}")
        print(f"ğŸ“Š å®é™…æ£€ç´¢åˆ° {len(response.retrieved_documents)} ä¸ªæ–‡æ¡£")
        
        # åˆ†ææ£€ç´¢ç»“æœçš„å¤šæ ·æ€§
        scores = [doc.score for doc in response.retrieved_documents]
        print(f"ğŸ“Š ç›¸ä¼¼åº¦åˆ†å¸ƒ: æœ€é«˜={max(scores):.3f}, æœ€ä½={min(scores):.3f}, å¹³å‡={sum(scores)/len(scores):.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é«˜çº§æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_system_info():
    """æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–"""
    print("\nğŸ” æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–...")
    
    rag_system = create_rag_system(
        collection_name="documents",
        enable_conversation=True
    )
    
    try:
        system_info = rag_system.get_system_info()
        
        print("ğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
        print(f"  é›†åˆåç§°: {system_info['collection_name']}")
        print(f"  Promptæ¨¡æ¿: {system_info['prompt_template']}")
        print(f"  å¯¹è¯åŠŸèƒ½: {system_info['conversation_enabled']}")
        print(f"  å¯¹è¯è½®æ•°: {system_info['conversation_turns']}")
        
        retrieval_config = system_info['retrieval_config']
        print(f"  æ£€ç´¢é…ç½®:")
        print(f"    top_k: {retrieval_config['top_k']}")
        print(f"    é˜ˆå€¼: {retrieval_config['score_threshold']}")
        print(f"    é‡æ’åº: {retrieval_config['enable_reranking']}")
        
        collection_info = system_info['collection_info']
        if 'error' not in collection_info:
            print(f"  é›†åˆä¿¡æ¯:")
            print(f"    å‘é‡æ•°é‡: {collection_info['vectors_count']}")
            print(f"    ç‚¹æ•°é‡: {collection_info['points_count']}")
            print(f"    çŠ¶æ€: {collection_info['status']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿä¿¡æ¯è·å–å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹RAGç³»ç»Ÿæµ‹è¯•\n")
    
    tests = [
        ("åŸºç¡€RAG", test_basic_rag()),
        ("å¼‚æ­¥RAG", test_async_rag()),
        ("æµå¼RAG", test_stream_rag()),
        ("å¯¹è¯RAG", test_conversation_rag()),
        ("é«˜çº§æ£€ç´¢", test_advanced_retrieval()),
        ("ç³»ç»Ÿä¿¡æ¯", test_system_info())
    ]
    
    results = []
    for name, test in tests:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•: {name}")
        print(f"{'='*50}")
        
        if asyncio.iscoroutine(test):
            result = await test
        else:
            result = test
        results.append((name, result))
    
    print(f"\n{'='*50}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*50}")
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {success_count}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if success_count == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RAGç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
python test_rag_system.py
```

## å®éªŒäº”ï¼šAPIæ¥å£å®ç°

### FastAPIåº”ç”¨åˆ›å»º

åˆ›å»º `src/api/rag_api.py` æ–‡ä»¶ï¼š

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import json
from datetime import datetime

from src.rag.rag_system import create_rag_system, RAGConfig
from src.rag.retriever import RetrievalConfig

app = FastAPI(
    title="RAG API",
    description="æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»ŸAPI",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€RAGç³»ç»Ÿå®ä¾‹
rag_system = None

# è¯·æ±‚æ¨¡å‹
class QueryRequest(BaseModel):
    question: str = Field(..., description="ç”¨æˆ·é—®é¢˜")
    collection_name: str = Field(default="documents", description="é›†åˆåç§°")
    prompt_template: str = Field(default="basic", description="Promptæ¨¡æ¿ç±»å‹")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="è¿‡æ»¤æ¡ä»¶")
    top_k: int = Field(default=5, description="æ£€ç´¢æ•°é‡")
    score_threshold: float = Field(default=0.5, description="ç›¸ä¼¼åº¦é˜ˆå€¼")
    enable_reranking: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨é‡æ’åº")
    enable_conversation: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨å¯¹è¯æ¨¡å¼")

class ConversationRequest(BaseModel):
    question: str = Field(..., description="ç”¨æˆ·é—®é¢˜")
    session_id: str = Field(..., description="ä¼šè¯ID")
    collection_name: str = Field(default="documents", description="é›†åˆåç§°")
    prompt_template: str = Field(default="conversational", description="Promptæ¨¡æ¿ç±»å‹")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="è¿‡æ»¤æ¡ä»¶")

class DocumentInfo(BaseModel):
    content: str
    score: float
    source: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class QueryResponse(BaseModel):
    answer: str
    retrieved_documents: List[DocumentInfo]
    retrieval_time: float
    generation_time: float
    total_time: float
    llm_usage: Optional[Dict[str, Any]] = None
    timestamp: datetime

class SystemInfoResponse(BaseModel):
    collection_name: str
    prompt_template: str
    conversation_enabled: bool
    retrieval_config: Dict[str, Any]
    collection_info: Dict[str, Any]
    system_status: str

# ä¼šè¯ç®¡ç†
conversation_sessions: Dict[str, Any] = {}

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    global rag_system
    try:
        rag_system = create_rag_system(
            collection_name="documents",
            prompt_template="basic"
        )
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "RAG APIæœåŠ¡",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥RAGç³»ç»ŸçŠ¶æ€
        if rag_system is None:
            return {"status": "error", "message": "RAGç³»ç»Ÿæœªåˆå§‹åŒ–"}
        
        system_info = rag_system.get_system_info()
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "rag_system": "initialized",
            "collection": system_info.get("collection_name", "unknown")
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """æŸ¥è¯¢æ–‡æ¡£"""
    try:
        # åˆ›å»ºæˆ–æ›´æ–°RAGç³»ç»Ÿé…ç½®
        retrieval_config = RetrievalConfig(
            collection_name=request.collection_name,
            top_k=request.top_k,
            score_threshold=request.score_threshold,
            enable_reranking=request.enable_reranking
        )
        
        current_rag = create_rag_system(
            collection_name=request.collection_name,
            prompt_template=request.prompt_template,
            retrieval_config=retrieval_config,
            enable_conversation=request.enable_conversation
        )
        
        # æ‰§è¡ŒæŸ¥è¯¢
        response = await current_rag.aquery(
            question=request.question,
            filters=request.filters
        )
        
        # è½¬æ¢å“åº”æ ¼å¼
        documents = [
            DocumentInfo(
                content=doc.content,
                score=doc.score,
                source=doc.source,
                metadata=doc.metadata
            )
            for doc in response.retrieved_documents
        ]
        
        return QueryResponse(
            answer=response.answer,
            retrieved_documents=documents,
            retrieval_time=response.retrieval_time,
            generation_time=response.generation_time,
            total_time=response.retrieval_time + response.generation_time,
            llm_usage=response.llm_usage,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/stream-query")
async def stream_query_documents(request: QueryRequest):
    """æµå¼æŸ¥è¯¢æ–‡æ¡£"""
    try:
        # åˆ›å»ºRAGç³»ç»Ÿ
        retrieval_config = RetrievalConfig(
            collection_name=request.collection_name,
            top_k=request.top_k,
            score_threshold=request.score_threshold,
            enable_reranking=request.enable_reranking
        )
        
        current_rag = create_rag_system(
            collection_name=request.collection_name,
            prompt_template=request.prompt_template,
            retrieval_config=retrieval_config
        )
        
        async def generate_stream():
            """ç”Ÿæˆæµå¼å“åº”"""
            try:
                async for chunk in current_rag.stream_query(
                    question=request.question,
                    filters=request.filters
                ):
                    # å‘é€æ•°æ®å—
                    yield f"data: {json.dumps({'chunk': chunk, 'type': 'content'})}\n\n"
                
                # å‘é€ç»“æŸæ ‡è®°
                yield f"data: {json.dumps({'type': 'done'})}\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'error': str(e), 'type': 'error'})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/conversation", response_model=QueryResponse)
async def conversation_query(request: ConversationRequest):
    """å¯¹è¯å¼æŸ¥è¯¢"""
    try:
        # è·å–æˆ–åˆ›å»ºä¼šè¯
        if request.session_id not in conversation_sessions:
            conversation_sessions[request.session_id] = create_rag_system(
                collection_name=request.collection_name,
                prompt_template=request.prompt_template,
                enable_conversation=True,
                max_history_turns=5
            )
        
        session_rag = conversation_sessions[request.session_id]
        
        # æ‰§è¡Œå¯¹è¯æŸ¥è¯¢
        response = await session_rag.aquery(
            question=request.question,
            filters=request.filters
        )
        
        # è½¬æ¢å“åº”æ ¼å¼
        documents = [
            DocumentInfo(
                content=doc.content,
                score=doc.score,
                source=doc.source,
                metadata=doc.metadata
            )
            for doc in response.retrieved_documents
        ]
        
        return QueryResponse(
            answer=response.answer,
            retrieved_documents=documents,
            retrieval_time=response.retrieval_time,
            generation_time=response.generation_time,
            total_time=response.retrieval_time + response.generation_time,
            llm_usage=response.llm_usage,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversation/{session_id}/history")
async def get_conversation_history(session_id: str):
    """è·å–å¯¹è¯å†å²"""
    try:
        if session_id not in conversation_sessions:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_rag = conversation_sessions[session_id]
        history = session_rag.get_conversation_history()
        
        return {
            "session_id": session_id,
            "history": history,
            "turn_count": len(history)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/conversation/{session_id}")
async def clear_conversation(session_id: str):
    """æ¸…é™¤å¯¹è¯ä¼šè¯"""
    try:
        if session_id in conversation_sessions:
            del conversation_sessions[session_id]
            return {"message": f"ä¼šè¯ {session_id} å·²æ¸…é™¤"}
        else:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/info", response_model=SystemInfoResponse)
async def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    try:
        if rag_system is None:
            raise HTTPException(status_code=500, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        system_info = rag_system.get_system_info()
        
        return SystemInfoResponse(
            collection_name=system_info["collection_name"],
            prompt_template=system_info["prompt_template"],
            conversation_enabled=system_info["conversation_enabled"],
            retrieval_config=system_info["retrieval_config"],
            collection_info=system_info["collection_info"],
            system_status="running"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/sessions")
async def get_active_sessions():
    """è·å–æ´»è·ƒä¼šè¯åˆ—è¡¨"""
    return {
        "active_sessions": list(conversation_sessions.keys()),
        "session_count": len(conversation_sessions)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### APIæµ‹è¯•è„šæœ¬

åˆ›å»º `test_api.py` æµ‹è¯•è„šæœ¬ï¼š

```python
import requests
import json
import asyncio
import aiohttp
import time
from typing import Dict, Any

# APIåŸºç¡€URL
BASE_URL = "http://localhost:8000"

def test_health_check():
    """æµ‹è¯•å¥åº·æ£€æŸ¥"""
    print("ğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥...")
    
    try:
        response = requests.get(f"{BASE_URL}/health")
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… å¥åº·æ£€æŸ¥é€šè¿‡: {data['status']}")
            print(f"ğŸ“Š RAGç³»ç»ŸçŠ¶æ€: {data.get('rag_system', 'unknown')}")
            return True
        else:
            print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        return False

def test_basic_query():
    """æµ‹è¯•åŸºç¡€æŸ¥è¯¢"""
    print("\nğŸ” æµ‹è¯•åŸºç¡€æŸ¥è¯¢...")
    
    query_data = {
        "question": "ä»€ä¹ˆæ˜¯Pythonç¼–ç¨‹è¯­è¨€ï¼Ÿ",
        "collection_name": "documents",
        "prompt_template": "basic",
        "top_k": 3,
        "score_threshold": 0.5
    }
    
    try:
        response = requests.post(
            f"{BASE_URL}/query",
            json=query_data,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… æŸ¥è¯¢æˆåŠŸ")
            print(f"ğŸ“ å›ç­”: {data['answer'][:200]}...")
            print(f"ğŸ“Š æ£€ç´¢æ—¶é—´: {data['retrieval_time']:.3f}s")
            print(f"ğŸ“Š ç”Ÿæˆæ—¶é—´: {data['generation_time']:.3f}s")
            print(f"ğŸ“Š æ£€ç´¢æ–‡æ¡£æ•°: {len(data['retrieved_documents'])}")
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£
            for i, doc in enumerate(data['retrieved_documents'], 1):
                print(f"  æ–‡æ¡£{i}: ç›¸ä¼¼åº¦={doc['score']:.3f}, é•¿åº¦={len(doc['content'])}å­—ç¬¦")
            
            return True
        else:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¼‚å¸¸: {e}")
        return False

def test_stream_query():
    """æµ‹è¯•æµå¼æŸ¥è¯¢"""
    print("\nğŸ” æµ‹è¯•æµå¼æŸ¥è¯¢...")
    
    query_data = {
        "question": "è¯·è¯¦ç»†ä»‹ç»æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨ã€‚",
        "collection_name": "documents",
        "prompt_template": "detailed",
        "top_k": 5
    }
    
    try:
        response = requests.post(
            f"{BASE_URL}/stream-query",
            json=query_data,
            headers={"Content-Type": "application/json"},
            stream=True
        )
        
        if response.status_code == 200:
            print("âœ… æµå¼æŸ¥è¯¢å¼€å§‹")
            print("ğŸ“ æµå¼å›ç­”:")
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        try:
                            data = json.loads(line_str[6:])  # å»æ‰ 'data: ' å‰ç¼€
                            if data.get('type') == 'content':
                                print(data['chunk'], end='', flush=True)
                            elif data.get('type') == 'done':
                                print("\nâœ… æµå¼æŸ¥è¯¢å®Œæˆ")
                                break
                            elif data.get('type') == 'error':
                                print(f"\nâŒ æµå¼æŸ¥è¯¢é”™è¯¯: {data['error']}")
                                return False
                        except json.JSONDecodeError:
                            continue
            
            return True
        else:
            print(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ æµå¼æŸ¥è¯¢å¼‚å¸¸: {e}")
        return False

def test_conversation():
    """æµ‹è¯•å¯¹è¯åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å¯¹è¯åŠŸèƒ½...")
    
    session_id = "test_session_001"
    
    # å¤šè½®å¯¹è¯
    conversations = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "å®ƒå’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "èƒ½ä¸¾ä¸ªæ·±åº¦å­¦ä¹ çš„å®é™…åº”ç”¨ä¾‹å­å—ï¼Ÿ"
    ]
    
    try:
        for i, question in enumerate(conversations, 1):
            print(f"\nğŸ—£ï¸ ç¬¬{i}è½®å¯¹è¯")
            print(f"â“ ç”¨æˆ·: {question}")
            
            conversation_data = {
                "question": question,
                "session_id": session_id,
                "collection_name": "documents",
                "prompt_template": "conversational"
            }
            
            response = requests.post(
                f"{BASE_URL}/conversation",
                json=conversation_data,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"ğŸ¤– åŠ©æ‰‹: {data['answer'][:200]}...")
            else:
                print(f"âŒ å¯¹è¯å¤±è´¥: {response.status_code}")
                return False
        
        # è·å–å¯¹è¯å†å²
        history_response = requests.get(f"{BASE_URL}/conversation/{session_id}/history")
        if history_response.status_code == 200:
            history_data = history_response.json()
            print(f"\nğŸ“š å¯¹è¯å†å² ({history_data['turn_count']} è½®):")
            for i, turn in enumerate(history_data['history'], 1):
                print(f"  ç¬¬{i}è½® - ç”¨æˆ·: {turn['user'][:50]}...")
                print(f"        åŠ©æ‰‹: {turn['assistant'][:50]}...")
        
        # æ¸…é™¤ä¼šè¯
        clear_response = requests.delete(f"{BASE_URL}/conversation/{session_id}")
        if clear_response.status_code == 200:
            print(f"\nğŸ—‘ï¸ ä¼šè¯å·²æ¸…é™¤")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¹è¯æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_system_info():
    """æµ‹è¯•ç³»ç»Ÿä¿¡æ¯"""
    print("\nğŸ” æµ‹è¯•ç³»ç»Ÿä¿¡æ¯...")
    
    try:
        response = requests.get(f"{BASE_URL}/system/info")
        
        if response.status_code == 200:
            data = response.json()
            print("âœ… ç³»ç»Ÿä¿¡æ¯è·å–æˆåŠŸ")
            print(f"ğŸ“Š é›†åˆåç§°: {data['collection_name']}")
            print(f"ğŸ“Š Promptæ¨¡æ¿: {data['prompt_template']}")
            print(f"ğŸ“Š å¯¹è¯åŠŸèƒ½: {data['conversation_enabled']}")
            print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {data['system_status']}")
            
            retrieval_config = data['retrieval_config']
            print(f"ğŸ“Š æ£€ç´¢é…ç½®:")
            print(f"    top_k: {retrieval_config['top_k']}")
            print(f"    é˜ˆå€¼: {retrieval_config['score_threshold']}")
            
            collection_info = data['collection_info']
            if 'error' not in collection_info:
                print(f"ğŸ“Š é›†åˆä¿¡æ¯:")
                print(f"    å‘é‡æ•°é‡: {collection_info.get('vectors_count', 'N/A')}")
                print(f"    çŠ¶æ€: {collection_info.get('status', 'N/A')}")
            
            return True
        else:
            print(f"âŒ ç³»ç»Ÿä¿¡æ¯è·å–å¤±è´¥: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿä¿¡æ¯è·å–å¼‚å¸¸: {e}")
        return False

def test_advanced_query():
    """æµ‹è¯•é«˜çº§æŸ¥è¯¢åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•é«˜çº§æŸ¥è¯¢åŠŸèƒ½...")
    
    # æµ‹è¯•ä¸åŒçš„Promptæ¨¡æ¿
    templates = ["basic", "detailed", "structured"]
    
    for template in templates:
        print(f"\nğŸ“ æµ‹è¯• {template} æ¨¡æ¿")
        
        query_data = {
            "question": "æ¯”è¾ƒPythonå’ŒJavaç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹",
            "collection_name": "documents",
            "prompt_template": template,
            "top_k": 5,
            "score_threshold": 0.6,
            "enable_reranking": True
        }
        
        try:
            response = requests.post(
                f"{BASE_URL}/query",
                json=query_data,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"  âœ… {template} æ¨¡æ¿æŸ¥è¯¢æˆåŠŸ")
                print(f"  ğŸ“Š å›ç­”é•¿åº¦: {len(data['answer'])} å­—ç¬¦")
                print(f"  ğŸ“Š æ£€ç´¢æ–‡æ¡£: {len(data['retrieved_documents'])} ä¸ª")
            else:
                print(f"  âŒ {template} æ¨¡æ¿æŸ¥è¯¢å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"  âŒ {template} æ¨¡æ¿æŸ¥è¯¢å¼‚å¸¸: {e}")
            return False
    
    return True

def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\nğŸ” æµ‹è¯•APIæ€§èƒ½...")
    
    query_data = {
        "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "collection_name": "documents",
        "top_k": 3
    }
    
    # å¹¶å‘æµ‹è¯•
    concurrent_requests = 5
    total_time = 0
    success_count = 0
    
    try:
        start_time = time.time()
        
        for i in range(concurrent_requests):
            response = requests.post(
                f"{BASE_URL}/query",
                json=query_data,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                success_count += 1
                data = response.json()
                total_time += data['total_time']
        
        end_time = time.time()
        wall_time = end_time - start_time
        
        print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"ğŸ“Š æˆåŠŸè¯·æ±‚: {success_count}/{concurrent_requests}")
        print(f"ğŸ“Š å¹³å‡å“åº”æ—¶é—´: {total_time/success_count:.3f}s")
        print(f"ğŸ“Š æ€»è€—æ—¶: {wall_time:.3f}s")
        print(f"ğŸ“Š QPS: {success_count/wall_time:.2f}")
        
        return success_count == concurrent_requests
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹APIæµ‹è¯•\n")
    print("è¯·ç¡®ä¿APIæœåŠ¡å·²å¯åŠ¨: uvicorn src.api.rag_api:app --reload")
    print("="*60)
    
    tests = [
        ("å¥åº·æ£€æŸ¥", test_health_check),
        ("åŸºç¡€æŸ¥è¯¢", test_basic_query),
        ("æµå¼æŸ¥è¯¢", test_stream_query),
        ("å¯¹è¯åŠŸèƒ½", test_conversation),
        ("ç³»ç»Ÿä¿¡æ¯", test_system_info),
        ("é«˜çº§æŸ¥è¯¢", test_advanced_query),
        ("æ€§èƒ½æµ‹è¯•", test_performance)
    ]
    
    results = []
    for name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•: {name}")
        print(f"{'='*50}")
        
        result = test_func()
        results.append((name, result))
    
    print(f"\n{'='*50}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*50}")
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {success_count}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if success_count == len(results):
        print("ğŸ‰ æ‰€æœ‰APIæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†APIæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€ã€‚")

if __name__ == "__main__":
    main()
```

### å¯åŠ¨å’Œæµ‹è¯•API

1. **å¯åŠ¨APIæœåŠ¡**ï¼š
```bash
# å¼€å‘æ¨¡å¼å¯åŠ¨
uvicorn src.api.rag_api:app --reload --host 0.0.0.0 --port 8000

# æˆ–è€…ç›´æ¥è¿è¡Œ
python src/api/rag_api.py
```

2. **è®¿é—®APIæ–‡æ¡£**ï¼š
```bash
# æ‰“å¼€æµè§ˆå™¨è®¿é—®
http://localhost:8000/docs
```

3. **è¿è¡ŒAPIæµ‹è¯•**ï¼š
```bash
python test_api.py
```

4. **ä½¿ç”¨curlæµ‹è¯•**ï¼š
```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# åŸºç¡€æŸ¥è¯¢
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{
       "question": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
       "collection_name": "documents",
       "top_k": 3
     }'

# ç³»ç»Ÿä¿¡æ¯
curl http://localhost:8000/system/info
```

## æ€è€ƒé¢˜

### 1. ç³»ç»Ÿæ¶æ„æ€è€ƒ
- ä¸ºä»€ä¹ˆè¦å°†æ£€ç´¢å™¨ã€LLMå®¢æˆ·ç«¯å’ŒPromptæ¨¡æ¿åˆ†åˆ«å°è£…ï¼Ÿè¿™ç§è®¾è®¡æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
- å¦‚ä½•è®¾è®¡RAGç³»ç»Ÿçš„ç¼“å­˜ç­–ç•¥æ¥æé«˜å“åº”é€Ÿåº¦ï¼Ÿ
- åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼ŒRAGç³»ç»Ÿå¯èƒ½é‡åˆ°å“ªäº›ç“¶é¢ˆï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

### 2. æ£€ç´¢ä¼˜åŒ–æ€è€ƒ
- é‡æ’åºï¼ˆrerankingï¼‰åœ¨ä»€ä¹ˆæƒ…å†µä¸‹èƒ½æ˜¾è‘—æå‡æ£€ç´¢æ•ˆæœï¼Ÿ
- å¦‚ä½•å¹³è¡¡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ï¼Ÿ
- å‘é‡æ£€ç´¢çš„score_thresholdåº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿè¿‡é«˜æˆ–è¿‡ä½ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

### 3. Promptå·¥ç¨‹æ€è€ƒ
- ä¸åŒç±»å‹çš„é—®é¢˜ï¼ˆäº‹å®æ€§ã€åˆ†ææ€§ã€åˆ›é€ æ€§ï¼‰åº”è¯¥ä½¿ç”¨ä»€ä¹ˆæ ·çš„Promptç­–ç•¥ï¼Ÿ
- å¦‚ä½•è®¾è®¡Promptæ¥å‡å°‘LLMçš„å¹»è§‰é—®é¢˜ï¼Ÿ
- åœ¨å¯¹è¯å¼RAGä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å†å²å¯¹è¯ä¿¡æ¯ï¼Ÿ

### 4. æ€§èƒ½ä¼˜åŒ–æ€è€ƒ
- å¼‚æ­¥å¤„ç†åœ¨RAGç³»ç»Ÿä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå“ªäº›ç¯èŠ‚é€‚åˆå¼‚æ­¥åŒ–ï¼Ÿ
- æµå¼å“åº”å¯¹ç”¨æˆ·ä½“éªŒæœ‰ä»€ä¹ˆæ”¹å–„ï¼Ÿå®ç°æ—¶éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•ç›‘æ§å’Œè¯„ä¼°RAGç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡ï¼Ÿ

### 5. å®é™…åº”ç”¨æ€è€ƒ
- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²RAGç³»ç»Ÿéœ€è¦è€ƒè™‘å“ªäº›å®‰å…¨æ€§é—®é¢˜ï¼Ÿ
- å¦‚ä½•å¤„ç†ä¸åŒé¢†åŸŸæˆ–è¯­è¨€çš„æ–‡æ¡£æ£€ç´¢ï¼Ÿ
- RAGç³»ç»Ÿå¦‚ä½•ä¸ç°æœ‰çš„ä¸šåŠ¡ç³»ç»Ÿé›†æˆï¼Ÿ

## å®éªŒæ£€æŸ¥æ¸…å•

### âœ… ç¯å¢ƒå‡†å¤‡
- [ ] LLM APIé…ç½®æ­£ç¡®ï¼ˆOpenAI/Ollamaç­‰ï¼‰
- [ ] QdrantæœåŠ¡æ­£å¸¸è¿è¡Œ
- [ ] æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆ
- [ ] ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®

### âœ… LLMå®¢æˆ·ç«¯
- [ ] LLMClientç±»å®ç°å®Œæ•´
- [ ] æ”¯æŒåŒæ­¥ã€å¼‚æ­¥å’Œæµå¼èŠå¤©
- [ ] Tokenè®¡æ•°åŠŸèƒ½æ­£å¸¸
- [ ] é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„
- [ ] æµ‹è¯•è„šæœ¬è¿è¡Œé€šè¿‡

### âœ… Promptæ¨¡æ¿
- [ ] å®ç°äº†4ç§ä¸åŒç±»å‹çš„Promptæ¨¡æ¿
- [ ] PromptManagerç®¡ç†åŠŸèƒ½æ­£å¸¸
- [ ] æ¨¡æ¿æ¸²æŸ“ç»“æœç¬¦åˆé¢„æœŸ
- [ ] æ”¯æŒè‡ªå®šä¹‰æ¨¡æ¿æ‰©å±•

### âœ… RAGæ£€ç´¢å™¨
- [ ] RAGRetrieverç±»åŠŸèƒ½å®Œæ•´
- [ ] æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥æ£€ç´¢
- [ ] é‡æ’åºå’Œå¤šæ ·æ€§è¿‡æ»¤æ­£å¸¸å·¥ä½œ
- [ ] Tokené•¿åº¦æ§åˆ¶æœ‰æ•ˆ
- [ ] è¿‡æ»¤æ¡ä»¶æ”¯æŒå®Œå–„

### âœ… RAGç³»ç»Ÿ
- [ ] RAGSystemç±»é›†æˆæ‰€æœ‰ç»„ä»¶
- [ ] æ”¯æŒä¸‰ç§æŸ¥è¯¢æ¨¡å¼ï¼ˆåŒæ­¥ã€å¼‚æ­¥ã€æµå¼ï¼‰
- [ ] å¯¹è¯å†å²ç®¡ç†åŠŸèƒ½æ­£å¸¸
- [ ] ç³»ç»Ÿä¿¡æ¯è·å–å®Œæ•´
- [ ] é…ç½®ç®¡ç†çµæ´»

### âœ… APIæ¥å£
- [ ] FastAPIåº”ç”¨å¯åŠ¨æ­£å¸¸
- [ ] æ‰€æœ‰APIç«¯ç‚¹å“åº”æ­£ç¡®
- [ ] è¯·æ±‚éªŒè¯å’Œé”™è¯¯å¤„ç†å®Œå–„
- [ ] æµå¼å“åº”åŠŸèƒ½æ­£å¸¸
- [ ] å¯¹è¯ä¼šè¯ç®¡ç†æœ‰æ•ˆ
- [ ] APIæ–‡æ¡£ç”Ÿæˆæ­£ç¡®

### âœ… æµ‹è¯•éªŒè¯
- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] APIæµ‹è¯•è„šæœ¬è¿è¡ŒæˆåŠŸ
- [ ] æ€§èƒ½æµ‹è¯•ç»“æœåˆç†
- [ ] é”™è¯¯åœºæ™¯å¤„ç†æ­£ç¡®

## å¸¸è§é—®é¢˜è§£å†³

### 1. LLMè¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥APIå¯†é’¥
echo $OPENAI_API_KEY

# æµ‹è¯•ç½‘ç»œè¿æ¥
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰
ollama serve
ollama pull llama2
```

### 2. Qdrantè¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥QdrantæœåŠ¡çŠ¶æ€
docker ps | grep qdrant

# é‡å¯QdrantæœåŠ¡
docker restart qdrant

# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :6333
```

### 3. å‘é‡æ£€ç´¢æ— ç»“æœ
- æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨å‘é‡æ•°æ®
- è°ƒæ•´score_thresholdé˜ˆå€¼
- éªŒè¯æŸ¥è¯¢å‘é‡ç”Ÿæˆæ˜¯å¦æ­£ç¡®
- æ£€æŸ¥è¿‡æ»¤æ¡ä»¶æ˜¯å¦è¿‡äºä¸¥æ ¼

### 4. APIå“åº”æ…¢
- å¯ç”¨å¼‚æ­¥å¤„ç†
- ä¼˜åŒ–æ£€ç´¢å‚æ•°ï¼ˆå‡å°‘top_kï¼‰
- ä½¿ç”¨æ›´å¿«çš„embeddingæ¨¡å‹
- æ·»åŠ ç»“æœç¼“å­˜

### 5. å†…å­˜ä½¿ç”¨è¿‡é«˜
- å‡å°‘æ‰¹å¤„ç†å¤§å°
- åŠæ—¶æ¸…ç†å¯¹è¯å†å²
- ä¼˜åŒ–å‘é‡å­˜å‚¨é…ç½®
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

## å‚è€ƒèµ„æ–™

### æŠ€æœ¯æ–‡æ¡£
- [FastAPIå®˜æ–¹æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Qdrantå‘é‡æ•°æ®åº“](https://qdrant.tech/documentation/)
- [sentence-transformers](https://www.sbert.net/)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs/)

### å­¦æœ¯è®ºæ–‡
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- "Dense Passage Retrieval for Open-Domain Question Answering"
- "FiD: Leveraging Passage Retrieval with Generative Models"

### å¼€æºé¡¹ç›®
- [LangChain](https://github.com/langchain-ai/langchain)
- [LlamaIndex](https://github.com/run-llama/llama_index)
- [Haystack](https://github.com/deepset-ai/haystack)

## ğŸ“ å®éªŒå®Œæˆåçš„Gitæ“ä½œ

### ä¸ºä»€ä¹ˆè¦è¿›è¡ŒGitæäº¤ï¼Ÿ

å®ŒæˆRAGç³»ç»Ÿå®Œæ•´å®ç°åï¼Œè¿›è¡ŒGitæäº¤è‡³å…³é‡è¦ï¼š

1. **å®Œæ•´ç³»ç»Ÿä¿å­˜**ï¼šä¿å­˜RAGSystemã€LLMClientã€RAGRetrieverç­‰æ ¸å¿ƒç»„ä»¶çš„å®Œæ•´å®ç°
2. **APIæ¥å£ç®¡ç†**ï¼šè®°å½•FastAPIæ¥å£è®¾è®¡å’ŒRESTfulæœåŠ¡å®ç°
3. **Promptå·¥ç¨‹è®°å½•**ï¼šä¿å­˜å„ç§Promptæ¨¡æ¿å’Œå·¥ç¨‹ä¼˜åŒ–ç­–ç•¥
4. **ç³»ç»Ÿé›†æˆæ–¹æ¡ˆ**ï¼šè®°å½•æ£€ç´¢ã€ç”Ÿæˆã€APIç­‰ç»„ä»¶çš„é›†æˆæ¶æ„
5. **æµ‹è¯•ç”¨ä¾‹ä¿å­˜**ï¼šä¿å­˜å®Œæ•´çš„æµ‹è¯•å¥—ä»¶å’ŒéªŒè¯è„šæœ¬
6. **é…ç½®ç®¡ç†**ï¼šè®°å½•LLMé…ç½®ã€å‘é‡æ•°æ®åº“é…ç½®ç­‰å…³é”®å‚æ•°
7. **é¡¹ç›®é‡Œç¨‹ç¢‘**ï¼šæ ‡è®°RAGç³»ç»Ÿä»æ¦‚å¿µåˆ°å®Œæ•´å®ç°çš„é‡è¦èŠ‚ç‚¹
8. **å›¢é˜Ÿåä½œ**ï¼šè®©å›¢é˜Ÿæˆå‘˜äº†è§£å®Œæ•´RAGç³»ç»Ÿçš„å®ç°ç»†èŠ‚

### Gitæ“ä½œæ­¥éª¤

#### 1. æ£€æŸ¥å½“å‰ä¿®æ”¹çŠ¶æ€
```bash
git status
```

**é¢„æœŸçœ‹åˆ°çš„æ–‡ä»¶å˜æ›´**ï¼š
- `src/llm/llm_client.py` - LLMå®¢æˆ·ç«¯å®ç°
- `src/llm/prompt_templates.py` - Promptæ¨¡æ¿ç³»ç»Ÿ
- `src/rag/rag_retriever.py` - RAGæ£€ç´¢å™¨
- `src/rag/rag_system.py` - RAGç³»ç»Ÿæ ¸å¿ƒ
- `src/api/rag_api.py` - FastAPIæ¥å£å®ç°
- `src/config/rag_config.py` - RAGç³»ç»Ÿé…ç½®
- `tests/test_llm_client.py` - LLMå®¢æˆ·ç«¯æµ‹è¯•
- `tests/test_rag_system.py` - RAGç³»ç»Ÿæµ‹è¯•
- `tests/test_api.py` - APIæ¥å£æµ‹è¯•
- `requirements.txt` - æ–°å¢FastAPIã€OpenAIç­‰ä¾èµ–
- `.env.example` - ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
- å¯èƒ½çš„å…¶ä»–é…ç½®å’Œæ–‡æ¡£æ–‡ä»¶

#### 2. æ·»åŠ ä¿®æ”¹çš„æ–‡ä»¶
```bash
# æ·»åŠ æ‰€æœ‰ä¿®æ”¹çš„æ–‡ä»¶
git add .

# æˆ–è€…é€‰æ‹©æ€§æ·»åŠ ç‰¹å®šæ–‡ä»¶
git add src/llm/
git add src/rag/
git add src/api/
git add src/config/
git add tests/
git add requirements.txt
git add .env.example
```

#### 3. æäº¤æ›´æ”¹
```bash
git commit -m "å®Œæˆlesson06å®éªŒï¼šå®ç°å®Œæ•´RAGç³»ç»Ÿ

- å®ç°LLMClientç±»ï¼Œæ”¯æŒåŒæ­¥ã€å¼‚æ­¥å’Œæµå¼èŠå¤©
- å®ç°å¤šç§Promptæ¨¡æ¿ï¼ˆåŸºç¡€ã€è¯¦ç»†ã€å¯¹è¯ã€ç»“æ„åŒ–ï¼‰
- å®ç°RAGRetrieverï¼Œæ”¯æŒé‡æ’åºå’Œå¤šæ ·æ€§è¿‡æ»¤
- å®ç°RAGSystemæ ¸å¿ƒç±»ï¼Œé›†æˆæ‰€æœ‰ç»„ä»¶
- å®ç°FastAPIæ¥å£ï¼Œæä¾›RESTfulæœåŠ¡
- æ·»åŠ å®Œæ•´çš„æµ‹è¯•å¥—ä»¶å’ŒAPIæµ‹è¯•
- æ”¯æŒå¯¹è¯å†å²ç®¡ç†å’Œæµå¼å“åº”
- å®Œæˆç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œé”™è¯¯å¤„ç†
- å®ç°é…ç½®ç®¡ç†å’Œç¯å¢ƒå˜é‡æ”¯æŒ"
```

#### 4. æŸ¥çœ‹æäº¤å†å²
```bash
git log --oneline -5
```

#### 5. æ¨é€åˆ°è¿œç¨‹ä»“åº“ï¼ˆå¯é€‰ï¼‰
```bash
# å¦‚æœéœ€è¦æ¨é€åˆ°è¿œç¨‹ä»“åº“
git push origin lesson06-rag-system

# æˆ–æ¨é€åˆ°ä¸»åˆ†æ”¯ï¼ˆæ ¹æ®ä½ çš„åˆ†æ”¯ç­–ç•¥ï¼‰
git push origin main
```

### æäº¤å‰éªŒè¯æ¸…å•

åœ¨æäº¤ä¹‹å‰ï¼Œè¯·ç¡®ä¿ï¼š

- [ ] **LLMå®¢æˆ·ç«¯éªŒè¯**ï¼šLLMClientèƒ½æ­£å¸¸è°ƒç”¨å„ç§LLM API
- [ ] **Promptæ¨¡æ¿éªŒè¯**ï¼šæ‰€æœ‰Promptæ¨¡æ¿æ¸²æŸ“ç»“æœç¬¦åˆé¢„æœŸ
- [ ] **æ£€ç´¢åŠŸèƒ½éªŒè¯**ï¼šRAGRetrieverèƒ½æ­£å¸¸æ£€ç´¢å’Œé‡æ’åº
- [ ] **RAGç³»ç»ŸéªŒè¯**ï¼šRAGSystemé›†æˆåŠŸèƒ½å®Œæ•´ä¸”ç¨³å®š
- [ ] **APIæ¥å£éªŒè¯**ï¼šFastAPIæœåŠ¡èƒ½æ­£å¸¸å¯åŠ¨å’Œå“åº”
- [ ] **æµå¼å“åº”éªŒè¯**ï¼šæµå¼èŠå¤©åŠŸèƒ½æ­£å¸¸å·¥ä½œ
- [ ] **å¯¹è¯ç®¡ç†éªŒè¯**ï¼šå¯¹è¯å†å²ç®¡ç†åŠŸèƒ½æœ‰æ•ˆ
- [ ] **æµ‹è¯•é€šè¿‡**ï¼šæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹è¿è¡ŒæˆåŠŸ
- [ ] **é…ç½®å®Œæ•´**ï¼šç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶è®¾ç½®æ­£ç¡®
- [ ] **æ–‡æ¡£æ›´æ–°**ï¼šAPIæ–‡æ¡£å’Œä»£ç æ³¨é‡Šå·²æ›´æ–°

### RAGç³»ç»Ÿé¡¹ç›®ç‰¹æ®Šæ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥ä¿æŠ¤**ï¼š
   ```bash
   # ç¡®ä¿.envæ–‡ä»¶è¢«.gitignoreæ’é™¤
   echo ".env" >> .gitignore
   echo "*.key" >> .gitignore
   
   # æ£€æŸ¥æ˜¯å¦æœ‰æ•æ„Ÿä¿¡æ¯æ³„éœ²
   grep -r "sk-\|api_key\|secret" src/ --exclude-dir=__pycache__ || echo "æœªå‘ç°æ•æ„Ÿä¿¡æ¯"
   ```

2. **å¤§æ–‡ä»¶æ’é™¤**ï¼š
   ```bash
   # æ’é™¤å¯èƒ½çš„å¤§å‹æ¨¡å‹ç¼“å­˜æ–‡ä»¶
   echo "models/" >> .gitignore
   echo "*.bin" >> .gitignore
   echo "cache/" >> .gitignore
   ```

3. **æ—¥å¿—æ–‡ä»¶ç®¡ç†**ï¼š
   ```bash
   # æ’é™¤æ—¥å¿—æ–‡ä»¶
   echo "logs/" >> .gitignore
   echo "*.log" >> .gitignore
   ```

4. **æµ‹è¯•æ•°æ®ç®¡ç†**ï¼š
   ```bash
   # æ’é™¤æµ‹è¯•ç”Ÿæˆçš„ä¸´æ—¶æ•°æ®
   echo "test_data/" >> .gitignore
   echo "temp/" >> .gitignore
   ```

### å¸¸è§é—®é¢˜è§£å†³

1. **å¤§æ–‡ä»¶æäº¤é—®é¢˜**ï¼š
   ```bash
   # å¦‚æœæ„å¤–æ·»åŠ äº†å¤§æ–‡ä»¶
   git reset HEAD large_file.bin
   git rm --cached large_file.bin
   ```

2. **æ•æ„Ÿä¿¡æ¯æ³„éœ²**ï¼š
   ```bash
   # å¦‚æœæ„å¤–æäº¤äº†APIå¯†é’¥
   git reset --soft HEAD~1
   # ç¼–è¾‘æ–‡ä»¶ç§»é™¤æ•æ„Ÿä¿¡æ¯åé‡æ–°æäº¤
   ```

3. **æäº¤ä¿¡æ¯ä¿®æ”¹**ï¼š
   ```bash
   # ä¿®æ”¹æœ€åä¸€æ¬¡æäº¤ä¿¡æ¯
   git commit --amend -m "æ–°çš„æäº¤ä¿¡æ¯"
   ```

4. **æ’¤é”€æ–‡ä»¶æ·»åŠ **ï¼š
   ```bash
   # æ’¤é”€git addæ“ä½œ
   git reset HEAD <æ–‡ä»¶å>
   ```

### RAGç³»ç»ŸGitæœ€ä½³å®è·µ

1. **åˆ†å±‚æäº¤ç­–ç•¥**ï¼š
   - å…ˆæäº¤æ ¸å¿ƒç»„ä»¶ï¼ˆLLMClientã€RAGRetrieverï¼‰
   - å†æäº¤ç³»ç»Ÿé›†æˆï¼ˆRAGSystemï¼‰
   - ç„¶åæäº¤APIæ¥å£ï¼ˆFastAPIï¼‰
   - æœ€åæäº¤æµ‹è¯•å’Œæ–‡æ¡£

2. **åŠŸèƒ½åˆ†æ”¯ç®¡ç†**ï¼š
   ```bash
   # ä¸ºé‡è¦åŠŸèƒ½åˆ›å»ºä¸“é—¨åˆ†æ”¯
   git checkout -b feature/streaming-response
   git checkout -b feature/conversation-history
   ```

3. **ç‰ˆæœ¬æ ‡è®°**ï¼š
   ```bash
   # ä¸ºé‡è¦ç‰ˆæœ¬æ‰“æ ‡ç­¾
   git tag -a v1.0-rag-mvp -m "RAGç³»ç»ŸMVPç‰ˆæœ¬"
   git tag -a v1.1-api-complete -m "å®Œæ•´APIæ¥å£ç‰ˆæœ¬"
   ```

4. **é…ç½®ç‰ˆæœ¬ç®¡ç†**ï¼š
   - ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†ç³»ç»Ÿå‚æ•°
   - ç‰ˆæœ¬æ§åˆ¶é…ç½®å˜æ›´å†å²
   - è®°å½•ä¸åŒç¯å¢ƒçš„é…ç½®å·®å¼‚

### ç³»ç»Ÿéƒ¨ç½²å‡†å¤‡

å®ŒæˆGitæäº¤åï¼Œä¸ºç”Ÿäº§éƒ¨ç½²åšå‡†å¤‡ï¼š

1. **ç¯å¢ƒé…ç½®æ£€æŸ¥**ï¼š
   ```bash
   # éªŒè¯ç¯å¢ƒå˜é‡é…ç½®
   python -c "from src.config.rag_config import RAGConfig; print('é…ç½®åŠ è½½æˆåŠŸ')"
   ```

2. **ä¾èµ–ç‰ˆæœ¬é”å®š**ï¼š
   ```bash
   # ç”Ÿæˆç²¾ç¡®çš„ä¾èµ–ç‰ˆæœ¬æ–‡ä»¶
   pip freeze > requirements-lock.txt
   ```

3. **DockeråŒ–å‡†å¤‡**ï¼š
   ```bash
   # åˆ›å»ºDockerfileï¼ˆå¦‚æœéœ€è¦ï¼‰
   # å‡†å¤‡docker-compose.yml
   ```

### ä¸‹ä¸€æ­¥å­¦ä¹ æŒ‡å¯¼

å®ŒæˆGitæäº¤åï¼Œä½ å¯ä»¥ï¼š

1. **ç³»ç»Ÿä¼˜åŒ–**ï¼šç ”ç©¶RAGç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
2. **é«˜çº§åŠŸèƒ½**ï¼šæ¢ç´¢å¤šæ¨¡æ€RAGã€å›¾RAGç­‰é«˜çº§æŠ€æœ¯
3. **ç”Ÿäº§éƒ¨ç½²**ï¼šå­¦ä¹ RAGç³»ç»Ÿçš„ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
4. **ç›‘æ§è¿ç»´**ï¼šäº†è§£RAGç³»ç»Ÿçš„ç›‘æ§å’Œè¿ç»´æœ€ä½³å®è·µ
5. **ä¸šåŠ¡é›†æˆ**ï¼šç ”ç©¶å¦‚ä½•å°†RAGç³»ç»Ÿé›†æˆåˆ°å®é™…ä¸šåŠ¡åœºæ™¯

è®°ä½ï¼Œå®Œæ•´çš„RAGç³»ç»Ÿæ˜¯ä¸€ä¸ªå¤æ‚çš„å·¥ç¨‹é¡¹ç›®ï¼Œè‰¯å¥½çš„ç‰ˆæœ¬æ§åˆ¶æ˜¯ç¡®ä¿é¡¹ç›®æˆåŠŸçš„å…³é”®ï¼

---

## å®éªŒå®Œæˆæ ‡å¿—

å®Œæˆæœ¬å®éªŒåï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

1. **ç†è§£RAGç³»ç»Ÿæ¶æ„**ï¼šæŒæ¡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ ¸å¿ƒåŸç†å’Œç»„ä»¶è®¾è®¡
2. **ç†Ÿç»ƒä½¿ç”¨LLM API**ï¼šèƒ½å¤Ÿå°è£…å’Œè°ƒç”¨å„ç§å¤§è¯­è¨€æ¨¡å‹æ¥å£
3. **è®¾è®¡æœ‰æ•ˆçš„Prompt**ï¼šæŒæ¡ä¸åŒåœºæ™¯ä¸‹çš„Promptå·¥ç¨‹æŠ€å·§
4. **å®ç°é«˜æ•ˆæ£€ç´¢**ï¼šèƒ½å¤Ÿä¼˜åŒ–å‘é‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ€§èƒ½
5. **æ„å»ºå®Œæ•´ç³»ç»Ÿ**ï¼šèƒ½å¤Ÿé›†æˆå„ä¸ªç»„ä»¶æ„å»ºå¯ç”¨çš„RAGåº”ç”¨
6. **å¼€å‘APIæ¥å£**ï¼šèƒ½å¤Ÿè®¾è®¡å’Œå®ç°RESTful APIæœåŠ¡
7. **è¿›è¡Œç³»ç»Ÿæµ‹è¯•**ï¼šèƒ½å¤Ÿç¼–å†™æµ‹è¯•ç”¨ä¾‹éªŒè¯ç³»ç»ŸåŠŸèƒ½
8. **è§£å†³å®é™…é—®é¢˜**ï¼šèƒ½å¤Ÿè¯Šæ–­å’Œè§£å†³RAGç³»ç»Ÿçš„å¸¸è§é—®é¢˜
9. âœ… **å®ŒæˆGitç‰ˆæœ¬æ§åˆ¶**ï¼š
   - æ‰€æœ‰RAGç³»ç»Ÿä»£ç å·²æäº¤åˆ°Gitä»“åº“
   - æäº¤ä¿¡æ¯æ¸…æ™°æè¿°äº†ç³»ç»Ÿæ¶æ„å’ŒåŠŸèƒ½
   - APIæ¥å£ã€æµ‹è¯•ç”¨ä¾‹å’Œé…ç½®æ–‡ä»¶å·²ä¿å­˜
   - ä¸ºåç»­çš„ç³»ç»Ÿä¼˜åŒ–å’Œéƒ¨ç½²åšå¥½å‡†å¤‡

ğŸ‰ **æ­å–œå®Œæˆç¬¬å…­è¯¾å®éªŒï¼ä½ å·²ç»æŒæ¡äº†æ„å»ºå®Œæ•´RAGç³»ç»Ÿçš„æ ¸å¿ƒæŠ€èƒ½ï¼Œå¹¶å»ºç«‹äº†è‰¯å¥½çš„ç‰ˆæœ¬æ§åˆ¶ä¹ æƒ¯ã€‚**