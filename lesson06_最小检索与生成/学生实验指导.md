# 第六课：最小检索与生成（MVP RAG）- 学生实验指导

## 代码基础准备

在开始本节课的实验之前，我们需要基于上一节课的代码继续开发。现在我们使用Git分支管理来获取代码。

### 步骤1：进入项目目录并切换分支

```bash
# 进入rag-system项目目录
cd rag-system

# 切换到lesson06分支
git checkout lesson06

# 验证当前分支
git branch
# 应该显示 * lesson06
```

### 步骤2：验证代码状态

```bash
# 检查项目结构
ls -la
# 应该看到：src/ scripts/ test_documents/ docker-compose.yml 等文件和目录

# 检查RAG相关文件
ls -la src/llm/
# 应该看到：llm_client.py rag_engine.py 等文件
```

### 步骤3：验证向量化环境

```bash
# 启动依赖服务
docker-compose up -d

# 测试向量化功能
python -c "from src.embedding.embedder import TextEmbedder; print('向量化模块导入成功')"

# 验证Qdrant连接
python -c "from qdrant_client import QdrantClient; client = QdrantClient('localhost', 6333); print('Qdrant连接成功')"
```

**说明**：lesson06分支包含了lesson05的所有代码，并新增了LLM客户端和RAG引擎相关的模块，实现完整的检索增强生成系统。

---

## 🎯 实验目标

通过本实验，你将学会：
1. 实现完整的RAG（检索增强生成）系统
2. 集成向量检索与大语言模型生成
3. 设计有效的Prompt模板
4. 构建RAG API接口
5. 测试和优化RAG系统性能

## 📋 实验环境准备

### 环境要求
- Python 3.8+
- 已完成前5课的实验内容
- Qdrant向量数据库运行中
- 网络连接（用于调用LLM API）

### 依赖安装

```bash
# 安装LLM相关依赖
pip install openai==1.3.0
pip install anthropic==0.7.0
pip install requests==2.31.0
pip install aiohttp==3.9.0
pip install tiktoken==0.5.0

# 安装其他工具依赖
pip install python-dotenv==1.0.0
pip install pydantic==2.5.0
pip install fastapi==0.104.0
pip install uvicorn==0.24.0
```

### 环境变量配置

在项目根目录创建 `.env` 文件：

```bash
# LLM API配置
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# Qdrant配置
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=

# 应用配置
APP_HOST=0.0.0.0
APP_PORT=8000
DEBUG=true
```

### 环境测试

创建 `test_environment.py` 测试脚本：

```python
import os
import asyncio
from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient

def test_environment():
    """测试实验环境是否准备就绪"""
    load_dotenv()
    
    print("🔍 检查环境配置...")
    
    # 检查环境变量
    required_vars = ['OPENAI_API_KEY', 'QDRANT_HOST', 'QDRANT_PORT']
    missing_vars = []
    
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"❌ 缺少环境变量: {', '.join(missing_vars)}")
        return False
    
    # 测试OpenAI连接
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=10
        )
        print("✅ OpenAI API连接正常")
    except Exception as e:
        print(f"❌ OpenAI API连接失败: {e}")
        return False
    
    # 测试Qdrant连接
    try:
        qdrant_client = QdrantClient(
            host=os.getenv('QDRANT_HOST'),
            port=int(os.getenv('QDRANT_PORT'))
        )
        collections = qdrant_client.get_collections()
        print("✅ Qdrant连接正常")
    except Exception as e:
        print(f"❌ Qdrant连接失败: {e}")
        return False
    
    print("🎉 环境准备完成！")
    return True

if __name__ == "__main__":
    test_environment()
```

运行测试：
```bash
python test_environment.py
```

---

## 🔧 实验一：LLM客户端封装

### 创建LLM客户端模块

创建 `src/llm/llm_client.py`：

```python
import os
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import tiktoken
from openai import OpenAI, AsyncOpenAI
from pydantic import BaseModel

class LLMProvider(Enum):
    """LLM提供商枚举"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    LOCAL = "local"

@dataclass
class LLMConfig:
    """LLM配置"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    max_tokens: int = 1000
    temperature: float = 0.7
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0

class ChatMessage(BaseModel):
    """聊天消息"""
    role: str  # system, user, assistant
    content: str

class LLMResponse(BaseModel):
    """LLM响应"""
    content: str
    usage: Dict[str, int]
    model: str
    finish_reason: str

class LLMClient:
    """LLM客户端封装"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.client = None
        self.async_client = None
        self._setup_client()
    
    def _setup_client(self):
        """设置客户端"""
        if self.config.provider == LLMProvider.OPENAI:
            self.client = OpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url
            )
            self.async_client = AsyncOpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url
            )
        else:
            raise ValueError(f"不支持的LLM提供商: {self.config.provider}")
    
    def count_tokens(self, text: str) -> int:
        """计算token数量"""
        try:
            encoding = tiktoken.encoding_for_model(self.config.model)
            return len(encoding.encode(text))
        except:
            # 简单估算：1个token约等于4个字符
            return len(text) // 4
    
    def chat(self, messages: List[ChatMessage]) -> LLMResponse:
        """同步聊天"""
        try:
            # 转换消息格式
            openai_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in messages
            ]
            
            response = self.client.chat.completions.create(
                model=self.config.model,
                messages=openai_messages,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                frequency_penalty=self.config.frequency_penalty,
                presence_penalty=self.config.presence_penalty
            )
            
            return LLMResponse(
                content=response.choices[0].message.content,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                model=response.model,
                finish_reason=response.choices[0].finish_reason
            )
            
        except Exception as e:
            raise Exception(f"LLM调用失败: {e}")
    
    async def stream_chat(self, messages: List[ChatMessage]) -> AsyncGenerator[str, None]:
        """流式聊天"""
        try:
            # 转换消息格式
            openai_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in messages
            ]
            
            stream = await self.async_client.chat.completions.create(
                model=self.config.model,
                messages=openai_messages,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            raise Exception(f"流式LLM调用失败: {e}")

def create_llm_client() -> LLMClient:
    """创建LLM客户端"""
    from dotenv import load_dotenv
    load_dotenv()
    
    config = LLMConfig(
        provider=LLMProvider.OPENAI,
        model=os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
        api_key=os.getenv('OPENAI_API_KEY'),
        base_url=os.getenv('OPENAI_BASE_URL'),
        max_tokens=int(os.getenv('MAX_TOKENS', '1000')),
        temperature=float(os.getenv('TEMPERATURE', '0.7'))
    )
    
    return LLMClient(config)
```

### LLM客户端测试

创建 `test_llm.py` 测试脚本：

```python
import asyncio
from src.llm.llm_client import create_llm_client, ChatMessage

def test_basic_chat():
    """测试基础聊天功能"""
    print("🔍 测试基础聊天功能...")
    
    client = create_llm_client()
    
    messages = [
        ChatMessage(role="system", content="你是一个有用的AI助手。"),
        ChatMessage(role="user", content="请简单介绍一下Python编程语言。")
    ]
    
    try:
        response = client.chat(messages)
        print(f"✅ 聊天成功")
        print(f"📝 回答: {response.content[:100]}...")
        print(f"📊 Token使用: {response.usage}")
        return True
    except Exception as e:
        print(f"❌ 聊天失败: {e}")
        return False

async def test_async_chat():
    """测试异步聊天功能"""
    print("\n🔍 测试异步聊天功能...")
    
    client = create_llm_client()
    
    messages = [
        ChatMessage(role="system", content="你是一个专业的技术顾问。"),
        ChatMessage(role="user", content="什么是RAG技术？请简要说明。")
    ]
    
    try:
        response = await client.achat(messages)
        print(f"✅ 异步聊天成功")
        print(f"📝 回答: {response.content[:100]}...")
        print(f"📊 Token使用: {response.usage}")
        return True
    except Exception as e:
        print(f"❌ 异步聊天失败: {e}")
        return False

async def test_stream_chat():
    """测试流式聊天功能"""
    print("\n🔍 测试流式聊天功能...")
    
    client = create_llm_client()
    
    messages = [
        ChatMessage(role="system", content="你是一个创意写作助手。"),
        ChatMessage(role="user", content="写一首关于春天的短诗。")
    ]
    
    try:
        print("📝 流式输出:")
        async for chunk in client.stream_chat(messages):
            print(chunk, end="", flush=True)
        print("\n✅ 流式聊天成功")
        return True
    except Exception as e:
        print(f"❌ 流式聊天失败: {e}")
        return False

def test_token_counting():
    """测试Token计数功能"""
    print("\n🔍 测试Token计数功能...")
    
    client = create_llm_client()
    
    test_texts = [
        "Hello, world!",
        "这是一段中文测试文本，用于验证Token计数功能。",
        "This is a longer text that contains multiple sentences. It should have more tokens than the previous examples."
    ]
    
    for text in test_texts:
        token_count = client.count_tokens(text)
        print(f"📝 文本: {text[:50]}...")
        print(f"🔢 Token数量: {token_count}")
        print()
    
    return True

async def main():
    """主测试函数"""
    print("🚀 开始LLM客户端测试\n")
    
    tests = [
        ("基础聊天", test_basic_chat()),
        ("异步聊天", test_async_chat()),
        ("流式聊天", test_stream_chat()),
        ("Token计数", test_token_counting())
    ]
    
    results = []
    for name, test in tests:
        if asyncio.iscoroutine(test):
            result = await test
        else:
            result = test
        results.append((name, result))
    
    print("\n📊 测试结果汇总:")
    for name, result in results:
        status = "✅ 通过" if result else "❌ 失败"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    print(f"\n🎯 总体结果: {success_count}/{len(results)} 测试通过")

if __name__ == "__main__":
    asyncio.run(main())
```

运行测试：
```bash
python test_llm.py
```

---

## 🔧 实验二：Prompt模板设计

### 创建Prompt模板模块

创建 `src/rag/prompt_templates.py`：

```python
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod

@dataclass
class RetrievedDocument:
    """检索到的文档"""
    content: str
    metadata: Dict[str, Any]
    score: float
    source: Optional[str] = None

class PromptTemplate(ABC):
    """Prompt模板基类"""
    
    @abstractmethod
    def format(self, query: str, documents: List[RetrievedDocument], **kwargs) -> str:
        """格式化Prompt"""
        pass

class BasicRAGPrompt(PromptTemplate):
    """基础RAG Prompt模板"""
    
    def __init__(self, system_message: Optional[str] = None):
        self.system_message = system_message or self._default_system_message()
    
    def _default_system_message(self) -> str:
        return """你是一个专业的AI助手，请根据提供的参考文档回答用户问题。

请遵循以下原则：
1. 只基于提供的参考文档回答问题
2. 如果文档中没有相关信息，请明确说明
3. 回答要准确、简洁、有条理
4. 可以引用具体的文档片段支持你的回答
5. 保持客观中立的态度"""
    
    def format(self, query: str, documents: List[RetrievedDocument], **kwargs) -> str:
        """格式化基础RAG Prompt"""
        # 构建参考文档部分
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            source_info = f" (来源: {doc.source})" if doc.source else ""
            doc_sections.append(f"文档{i}{source_info}:\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        prompt = f"""{self.system_message}

参考文档：
{documents_text}

用户问题：{query}

请基于上述参考文档回答问题："""
        
        return prompt

class DetailedRAGPrompt(PromptTemplate):
    """详细RAG Prompt模板"""
    
    def format(self, query: str, documents: List[RetrievedDocument], **kwargs) -> str:
        """格式化详细RAG Prompt"""
        # 构建参考文档部分
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            metadata_info = []
            if doc.metadata:
                for key, value in doc.metadata.items():
                    if key not in ['chunk_id', 'vector'] and value:
                        metadata_info.append(f"{key}: {value}")
            
            meta_text = f" ({', '.join(metadata_info)})" if metadata_info else ""
            score_text = f" [相似度: {doc.score:.3f}]"
            
            doc_sections.append(f"文档{i}{meta_text}{score_text}:\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        prompt = f"""你是一个专业的知识问答助手。请仔细阅读以下参考文档，并基于这些文档回答用户的问题。

## 参考文档
{documents_text}

## 用户问题
{query}

## 回答要求
1. 请仔细分析参考文档的内容
2. 基于文档内容提供准确、详细的回答
3. 如果需要，可以引用具体的文档片段
4. 如果文档中没有足够信息回答问题，请明确指出
5. 保持回答的逻辑性和条理性

## 回答
"""
        
        return prompt

class ConversationalRAGPrompt(PromptTemplate):
    """对话式RAG Prompt模板"""
    
    def format(self, query: str, documents: List[RetrievedDocument], 
               conversation_history: Optional[List[Dict[str, str]]] = None, **kwargs) -> str:
        """格式化对话式RAG Prompt"""
        # 构建对话历史
        history_text = ""
        if conversation_history:
            history_parts = []
            for turn in conversation_history[-3:]:  # 只保留最近3轮对话
                history_parts.append(f"用户: {turn.get('user', '')}")
                history_parts.append(f"助手: {turn.get('assistant', '')}")
            history_text = "\n".join(history_parts) + "\n\n"
        
        # 构建参考文档
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            source_info = f" (来源: {doc.source})" if doc.source else ""
            doc_sections.append(f"[文档{i}]{source_info}\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        prompt = f"""你是一个友好的AI助手，正在与用户进行对话。请基于提供的参考文档和对话历史来回答用户的最新问题。

## 对话历史
{history_text}## 参考文档
{documents_text}

## 当前问题
{query}

请结合对话上下文和参考文档，提供有帮助的回答："""
        
        return prompt

class StructuredRAGPrompt(PromptTemplate):
    """结构化RAG Prompt模板"""
    
    def format(self, query: str, documents: List[RetrievedDocument], 
               output_format: str = "markdown", **kwargs) -> str:
        """格式化结构化RAG Prompt"""
        # 构建参考文档
        doc_sections = []
        for i, doc in enumerate(documents, 1):
            doc_sections.append(f"### 文档{i}\n{doc.content}")
        
        documents_text = "\n\n".join(doc_sections)
        
        format_instructions = {
            "markdown": "请使用Markdown格式回答，包括标题、列表、粗体等格式。",
            "json": "请以JSON格式回答，包含answer、sources、confidence等字段。",
            "bullet_points": "请以要点形式回答，每个要点一行，使用-开头。"
        }
        
        format_instruction = format_instructions.get(output_format, format_instructions["markdown"])
        
        prompt = f"""请基于以下参考文档回答用户问题。

## 参考文档
{documents_text}

## 用户问题
{query}

## 输出要求
{format_instruction}

## 回答
"""
        
        return prompt

class PromptManager:
    """Prompt管理器"""
    
    def __init__(self):
        self.templates = {
            "basic": BasicRAGPrompt(),
            "detailed": DetailedRAGPrompt(),
            "conversational": ConversationalRAGPrompt(),
            "structured": StructuredRAGPrompt()
        }
    
    def get_template(self, template_name: str) -> PromptTemplate:
        """获取Prompt模板"""
        if template_name not in self.templates:
            raise ValueError(f"未知的模板名称: {template_name}")
        return self.templates[template_name]
    
    def add_template(self, name: str, template: PromptTemplate):
        """添加自定义模板"""
        self.templates[name] = template
    
    def list_templates(self) -> List[str]:
         """列出所有可用模板"""
         return list(self.templates.keys())
```

---

## 🔧 实验三：RAG检索器实现

### 创建RAG检索器模块

创建 `src/rag/retriever.py`：

```python
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from src.embedding.embedder import TextEmbedder
from src.rag.prompt_templates import RetrievedDocument

@dataclass
class RetrievalConfig:
    """检索配置"""
    collection_name: str
    top_k: int = 5
    score_threshold: float = 0.7
    enable_reranking: bool = False
    diversity_threshold: float = 0.8
    max_tokens: int = 4000

class RAGRetriever:
    """RAG检索器"""
    
    def __init__(self, 
                 qdrant_client: QdrantClient,
                 embedder: TextEmbedder,
                 config: RetrievalConfig):
        self.qdrant_client = qdrant_client
        self.embedder = embedder
        self.config = config
    
    def search(self, query: str, filters: Optional[Dict[str, Any]] = None) -> List[RetrievedDocument]:
        """同步检索"""
        try:
            # 生成查询向量
            query_vector = self.embedder.embed_text(query)
            
            # 构建过滤条件
            search_filter = self._build_filter(filters) if filters else None
            
            # 执行向量搜索
            search_results = self.qdrant_client.search(
                collection_name=self.config.collection_name,
                query_vector=query_vector,
                limit=self.config.top_k * 2,  # 获取更多结果用于后处理
                score_threshold=self.config.score_threshold,
                query_filter=search_filter
            )
            
            # 转换为RetrievedDocument格式
            documents = []
            for result in search_results:
                doc = RetrievedDocument(
                    content=result.payload.get('content', ''),
                    metadata=result.payload,
                    score=result.score,
                    source=result.payload.get('source')
                )
                documents.append(doc)
            
            # 后处理
            documents = self._post_process(documents, query)
            
            return documents[:self.config.top_k]
            
        except Exception as e:
            raise Exception(f"检索失败: {e}")
    
    async def asearch(self, query: str, filters: Optional[Dict[str, Any]] = None) -> List[RetrievedDocument]:
        """异步检索"""
        # 在线程池中执行同步检索
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search, query, filters)
    
    def _build_filter(self, filters: Dict[str, Any]) -> Filter:
        """构建Qdrant过滤条件"""
        conditions = []
        
        for key, value in filters.items():
            if isinstance(value, (str, int, float, bool)):
                conditions.append(
                    FieldCondition(key=key, match=MatchValue(value=value))
                )
            elif isinstance(value, list):
                for v in value:
                    conditions.append(
                        FieldCondition(key=key, match=MatchValue(value=v))
                    )
        
        return Filter(must=conditions) if conditions else None
    
    def _post_process(self, documents: List[RetrievedDocument], query: str) -> List[RetrievedDocument]:
        """后处理检索结果"""
        # 去重
        documents = self._remove_duplicates(documents)
        
        # 多样性过滤
        if self.config.diversity_threshold < 1.0:
            documents = self._apply_diversity_filter(documents)
        
        # 重排序
        if self.config.enable_reranking:
            documents = self._rerank(documents, query)
        
        # Token长度控制
        documents = self._control_token_length(documents)
        
        return documents
    
    def _remove_duplicates(self, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """去除重复文档"""
        seen_contents = set()
        unique_docs = []
        
        for doc in documents:
            content_hash = hash(doc.content)
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_docs.append(doc)
        
        return unique_docs
    
    def _apply_diversity_filter(self, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """应用多样性过滤"""
        if len(documents) <= 1:
            return documents
        
        # 简单的多样性过滤：基于内容相似度
        filtered_docs = [documents[0]]  # 保留第一个（最相关的）
        
        for doc in documents[1:]:
            is_diverse = True
            for selected_doc in filtered_docs:
                # 简单的相似度计算（基于共同词汇）
                similarity = self._calculate_content_similarity(doc.content, selected_doc.content)
                if similarity > self.config.diversity_threshold:
                    is_diverse = False
                    break
            
            if is_diverse:
                filtered_docs.append(doc)
        
        return filtered_docs
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """计算内容相似度"""
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _rerank(self, documents: List[RetrievedDocument], query: str) -> List[RetrievedDocument]:
        """重排序文档"""
        # 简单的重排序：结合向量相似度和关键词匹配
        query_words = set(query.lower().split())
        
        for doc in documents:
            doc_words = set(doc.content.lower().split())
            keyword_score = len(query_words.intersection(doc_words)) / len(query_words)
            
            # 结合向量相似度和关键词匹配分数
            doc.score = 0.7 * doc.score + 0.3 * keyword_score
        
        # 按新分数排序
        documents.sort(key=lambda x: x.score, reverse=True)
        return documents
    
    def _control_token_length(self, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """控制总Token长度"""
        total_tokens = 0
        filtered_docs = []
        
        for doc in documents:
            # 简单估算：1个token约等于4个字符
            doc_tokens = len(doc.content) // 4
            
            if total_tokens + doc_tokens <= self.config.max_tokens:
                filtered_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break
        
        return filtered_docs
    
    def get_collection_info(self) -> Dict[str, Any]:
        """获取集合信息"""
        try:
            collection_info = self.qdrant_client.get_collection(self.config.collection_name)
            return {
                "name": self.config.collection_name,
                "vectors_count": collection_info.vectors_count,
                "points_count": collection_info.points_count,
                "status": collection_info.status
            }
        except Exception as e:
             return {"error": str(e)}
```

---

## 🔧 实验四：完整RAG系统实现

### 创建RAG系统核心模块

创建 `src/rag/rag_system.py`：

```python
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from src.llm.llm_client import LLMClient, ChatMessage, create_llm_client
from src.rag.retriever import RAGRetriever, RetrievalConfig
from src.rag.prompt_templates import PromptManager, RetrievedDocument
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore

@dataclass
class RAGConfig:
    """RAG系统配置"""
    collection_name: str
    prompt_template: str = "basic"
    retrieval_config: Optional[RetrievalConfig] = None
    enable_conversation: bool = False
    max_history_turns: int = 3

@dataclass
class RAGResponse:
    """RAG响应"""
    answer: str
    retrieved_documents: List[RetrievedDocument]
    prompt_used: str
    llm_usage: Dict[str, int]
    retrieval_time: float
    generation_time: float

class RAGSystem:
    """完整的RAG系统"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.llm_client = create_llm_client()
        self.embedder = TextEmbedder()
        self.vector_store = QdrantVectorStore()
        
        # 初始化检索配置
        if config.retrieval_config is None:
            config.retrieval_config = RetrievalConfig(
                collection_name=config.collection_name
            )
        
        self.retriever = RAGRetriever(
            qdrant_client=self.vector_store.client,
            embedder=self.embedder,
            config=config.retrieval_config
        )
        
        self.prompt_manager = PromptManager()
        self.conversation_history = []
    
    def query(self, question: str, filters: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """同步查询"""
        import time
        
        # 检索阶段
        start_time = time.time()
        retrieved_docs = self.retriever.search(question, filters)
        retrieval_time = time.time() - start_time
        
        # 生成阶段
        start_time = time.time()
        
        # 构建Prompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # 调用LLM
        messages = [ChatMessage(role="user", content=prompt)]
        llm_response = self.llm_client.chat(messages)
        
        generation_time = time.time() - start_time
        
        # 更新对话历史
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": llm_response.content
            })
        
        return RAGResponse(
            answer=llm_response.content,
            retrieved_documents=retrieved_docs,
            prompt_used=prompt,
            llm_usage=llm_response.usage,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )
    
    async def aquery(self, question: str, filters: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """异步查询"""
        import time
        
        # 检索阶段
        start_time = time.time()
        retrieved_docs = await self.retriever.asearch(question, filters)
        retrieval_time = time.time() - start_time
        
        # 生成阶段
        start_time = time.time()
        
        # 构建Prompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # 调用LLM
        messages = [ChatMessage(role="user", content=prompt)]
        llm_response = await self.llm_client.achat(messages)
        
        generation_time = time.time() - start_time
        
        # 更新对话历史
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": llm_response.content
            })
        
        return RAGResponse(
            answer=llm_response.content,
            retrieved_documents=retrieved_docs,
            prompt_used=prompt,
            llm_usage=llm_response.usage,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )
    
    async def stream_query(self, question: str, filters: Optional[Dict[str, Any]] = None) -> AsyncGenerator[str, None]:
        """流式查询"""
        # 检索阶段
        retrieved_docs = await self.retriever.asearch(question, filters)
        
        # 构建Prompt
        prompt_template = self.prompt_manager.get_template(self.config.prompt_template)
        
        if self.config.enable_conversation and self.conversation_history:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs,
                conversation_history=self.conversation_history[-self.config.max_history_turns:]
            )
        else:
            prompt = prompt_template.format(
                query=question,
                documents=retrieved_docs
            )
        
        # 流式生成
        messages = [ChatMessage(role="user", content=prompt)]
        
        full_response = ""
        async for chunk in self.llm_client.stream_chat(messages):
            full_response += chunk
            yield chunk
        
        # 更新对话历史
        if self.config.enable_conversation:
            self.conversation_history.append({
                "user": question,
                "assistant": full_response
            })
    
    def clear_conversation(self):
        """清空对话历史"""
        self.conversation_history.clear()
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """获取对话历史"""
        return self.conversation_history.copy()
    
    def get_system_info(self) -> Dict[str, Any]:
        """获取系统信息"""
        return {
            "collection_name": self.config.collection_name,
            "prompt_template": self.config.prompt_template,
            "retrieval_config": {
                "top_k": self.config.retrieval_config.top_k,
                "score_threshold": self.config.retrieval_config.score_threshold,
                "enable_reranking": self.config.retrieval_config.enable_reranking
            },
            "conversation_enabled": self.config.enable_conversation,
            "conversation_turns": len(self.conversation_history),
            "collection_info": self.retriever.get_collection_info()
        }

def create_rag_system(collection_name: str, **kwargs) -> RAGSystem:
    """创建RAG系统实例"""
    config = RAGConfig(collection_name=collection_name, **kwargs)
    return RAGSystem(config)
```

### RAG系统测试

创建 `test_rag_system.py` 测试脚本：

```python
import asyncio
import time
from src.rag.rag_system import create_rag_system, RAGConfig, RetrievalConfig

def test_basic_rag():
    """测试基础RAG功能"""
    print("🔍 测试基础RAG功能...")
    
    # 创建RAG系统
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="basic"
    )
    
    # 测试问题
    questions = [
        "什么是Python？",
        "如何安装Python？",
        "Python有哪些特点？"
    ]
    
    for question in questions:
        try:
            print(f"\n❓ 问题: {question}")
            response = rag_system.query(question)
            
            print(f"✅ 回答: {response.answer[:200]}...")
            print(f"📊 检索时间: {response.retrieval_time:.3f}s")
            print(f"📊 生成时间: {response.generation_time:.3f}s")
            print(f"📊 检索到 {len(response.retrieved_documents)} 个文档")
            print(f"📊 Token使用: {response.llm_usage}")
            
        except Exception as e:
            print(f"❌ 查询失败: {e}")
            return False
    
    return True

async def test_async_rag():
    """测试异步RAG功能"""
    print("\n🔍 测试异步RAG功能...")
    
    # 创建RAG系统
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="detailed"
    )
    
    question = "请详细介绍一下机器学习的基本概念。"
    
    try:
        print(f"❓ 问题: {question}")
        response = await rag_system.aquery(question)
        
        print(f"✅ 回答: {response.answer[:300]}...")
        print(f"📊 检索时间: {response.retrieval_time:.3f}s")
        print(f"📊 生成时间: {response.generation_time:.3f}s")
        
        # 显示检索到的文档信息
        print(f"\n📚 检索到的文档:")
        for i, doc in enumerate(response.retrieved_documents, 1):
            print(f"  文档{i}: 相似度={doc.score:.3f}, 长度={len(doc.content)}字符")
            if doc.source:
                print(f"    来源: {doc.source}")
        
        return True
        
    except Exception as e:
        print(f"❌ 异步查询失败: {e}")
        return False

async def test_stream_rag():
    """测试流式RAG功能"""
    print("\n🔍 测试流式RAG功能...")
    
    # 创建RAG系统
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="basic"
    )
    
    question = "请写一篇关于人工智能发展历程的简短文章。"
    
    try:
        print(f"❓ 问题: {question}")
        print("📝 流式回答:")
        
        async for chunk in rag_system.stream_query(question):
            print(chunk, end="", flush=True)
        
        print("\n✅ 流式查询成功")
        return True
        
    except Exception as e:
        print(f"❌ 流式查询失败: {e}")
        return False

def test_conversation_rag():
    """测试对话式RAG功能"""
    print("\n🔍 测试对话式RAG功能...")
    
    # 创建支持对话的RAG系统
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="conversational",
        enable_conversation=True,
        max_history_turns=3
    )
    
    # 模拟多轮对话
    conversation = [
        "什么是深度学习？",
        "它和机器学习有什么区别？",
        "能举个具体的应用例子吗？"
    ]
    
    try:
        for i, question in enumerate(conversation, 1):
            print(f"\n🗣️ 第{i}轮对话")
            print(f"❓ 用户: {question}")
            
            response = rag_system.query(question)
            print(f"🤖 助手: {response.answer[:200]}...")
        
        # 显示对话历史
        history = rag_system.get_conversation_history()
        print(f"\n📚 对话历史 ({len(history)} 轮):")
        for i, turn in enumerate(history, 1):
            print(f"  第{i}轮 - 用户: {turn['user'][:50]}...")
            print(f"        助手: {turn['assistant'][:50]}...")
        
        return True
        
    except Exception as e:
        print(f"❌ 对话测试失败: {e}")
        return False

def test_advanced_retrieval():
    """测试高级检索功能"""
    print("\n🔍 测试高级检索功能...")
    
    # 创建带高级检索配置的RAG系统
    retrieval_config = RetrievalConfig(
        collection_name="documents",
        top_k=10,
        score_threshold=0.6,
        enable_reranking=True,
        diversity_threshold=0.7,
        max_tokens=3000
    )
    
    rag_system = create_rag_system(
        collection_name="documents",
        prompt_template="structured",
        retrieval_config=retrieval_config
    )
    
    question = "比较不同编程语言的特点和适用场景"
    
    try:
        print(f"❓ 问题: {question}")
        
        # 使用过滤条件
        filters = {"document_type": "tutorial"}
        response = rag_system.query(question, filters=filters)
        
        print(f"✅ 回答: {response.answer[:300]}...")
        print(f"📊 检索配置: top_k={retrieval_config.top_k}, 阈值={retrieval_config.score_threshold}")
        print(f"📊 实际检索到 {len(response.retrieved_documents)} 个文档")
        
        # 分析检索结果的多样性
        scores = [doc.score for doc in response.retrieved_documents]
        print(f"📊 相似度分布: 最高={max(scores):.3f}, 最低={min(scores):.3f}, 平均={sum(scores)/len(scores):.3f}")
        
        return True
        
    except Exception as e:
        print(f"❌ 高级检索测试失败: {e}")
        return False

def test_system_info():
    """测试系统信息获取"""
    print("\n🔍 测试系统信息获取...")
    
    rag_system = create_rag_system(
        collection_name="documents",
        enable_conversation=True
    )
    
    try:
        system_info = rag_system.get_system_info()
        
        print("📊 系统信息:")
        print(f"  集合名称: {system_info['collection_name']}")
        print(f"  Prompt模板: {system_info['prompt_template']}")
        print(f"  对话功能: {system_info['conversation_enabled']}")
        print(f"  对话轮数: {system_info['conversation_turns']}")
        
        retrieval_config = system_info['retrieval_config']
        print(f"  检索配置:")
        print(f"    top_k: {retrieval_config['top_k']}")
        print(f"    阈值: {retrieval_config['score_threshold']}")
        print(f"    重排序: {retrieval_config['enable_reranking']}")
        
        collection_info = system_info['collection_info']
        if 'error' not in collection_info:
            print(f"  集合信息:")
            print(f"    向量数量: {collection_info['vectors_count']}")
            print(f"    点数量: {collection_info['points_count']}")
            print(f"    状态: {collection_info['status']}")
        
        return True
        
    except Exception as e:
        print(f"❌ 系统信息获取失败: {e}")
        return False

async def main():
    """主测试函数"""
    print("🚀 开始RAG系统测试\n")
    
    tests = [
        ("基础RAG", test_basic_rag()),
        ("异步RAG", test_async_rag()),
        ("流式RAG", test_stream_rag()),
        ("对话RAG", test_conversation_rag()),
        ("高级检索", test_advanced_retrieval()),
        ("系统信息", test_system_info())
    ]
    
    results = []
    for name, test in tests:
        print(f"\n{'='*50}")
        print(f"测试: {name}")
        print(f"{'='*50}")
        
        if asyncio.iscoroutine(test):
            result = await test
        else:
            result = test
        results.append((name, result))
    
    print(f"\n{'='*50}")
    print("📊 测试结果汇总")
    print(f"{'='*50}")
    
    for name, result in results:
        status = "✅ 通过" if result else "❌ 失败"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    print(f"\n🎯 总体结果: {success_count}/{len(results)} 测试通过")
    
    if success_count == len(results):
        print("🎉 所有测试通过！RAG系统运行正常。")
    else:
        print("⚠️ 部分测试失败，请检查配置和依赖。")

if __name__ == "__main__":
    asyncio.run(main())
```

运行测试：
```bash
python test_rag_system.py
```

## 实验五：API接口实现

### FastAPI应用创建

创建 `src/api/rag_api.py` 文件：

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import json
from datetime import datetime

from src.rag.rag_system import create_rag_system, RAGConfig
from src.rag.retriever import RetrievalConfig

app = FastAPI(
    title="RAG API",
    description="检索增强生成系统API",
    version="1.0.0"
)

# 添加CORS中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 全局RAG系统实例
rag_system = None

# 请求模型
class QueryRequest(BaseModel):
    question: str = Field(..., description="用户问题")
    collection_name: str = Field(default="documents", description="集合名称")
    prompt_template: str = Field(default="basic", description="Prompt模板类型")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="过滤条件")
    top_k: int = Field(default=5, description="检索数量")
    score_threshold: float = Field(default=0.5, description="相似度阈值")
    enable_reranking: bool = Field(default=False, description="是否启用重排序")
    enable_conversation: bool = Field(default=False, description="是否启用对话模式")

class ConversationRequest(BaseModel):
    question: str = Field(..., description="用户问题")
    session_id: str = Field(..., description="会话ID")
    collection_name: str = Field(default="documents", description="集合名称")
    prompt_template: str = Field(default="conversational", description="Prompt模板类型")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="过滤条件")

class DocumentInfo(BaseModel):
    content: str
    score: float
    source: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class QueryResponse(BaseModel):
    answer: str
    retrieved_documents: List[DocumentInfo]
    retrieval_time: float
    generation_time: float
    total_time: float
    llm_usage: Optional[Dict[str, Any]] = None
    timestamp: datetime

class SystemInfoResponse(BaseModel):
    collection_name: str
    prompt_template: str
    conversation_enabled: bool
    retrieval_config: Dict[str, Any]
    collection_info: Dict[str, Any]
    system_status: str

# 会话管理
conversation_sessions: Dict[str, Any] = {}

@app.on_event("startup")
async def startup_event():
    """应用启动时初始化RAG系统"""
    global rag_system
    try:
        rag_system = create_rag_system(
            collection_name="documents",
            prompt_template="basic"
        )
        print("✅ RAG系统初始化成功")
    except Exception as e:
        print(f"❌ RAG系统初始化失败: {e}")

@app.get("/")
async def root():
    """根路径"""
    return {
        "message": "RAG API服务",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health_check():
    """健康检查"""
    try:
        # 检查RAG系统状态
        if rag_system is None:
            return {"status": "error", "message": "RAG系统未初始化"}
        
        system_info = rag_system.get_system_info()
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "rag_system": "initialized",
            "collection": system_info.get("collection_name", "unknown")
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """查询文档"""
    try:
        # 创建或更新RAG系统配置
        retrieval_config = RetrievalConfig(
            collection_name=request.collection_name,
            top_k=request.top_k,
            score_threshold=request.score_threshold,
            enable_reranking=request.enable_reranking
        )
        
        current_rag = create_rag_system(
            collection_name=request.collection_name,
            prompt_template=request.prompt_template,
            retrieval_config=retrieval_config,
            enable_conversation=request.enable_conversation
        )
        
        # 执行查询
        response = await current_rag.aquery(
            question=request.question,
            filters=request.filters
        )
        
        # 转换响应格式
        documents = [
            DocumentInfo(
                content=doc.content,
                score=doc.score,
                source=doc.source,
                metadata=doc.metadata
            )
            for doc in response.retrieved_documents
        ]
        
        return QueryResponse(
            answer=response.answer,
            retrieved_documents=documents,
            retrieval_time=response.retrieval_time,
            generation_time=response.generation_time,
            total_time=response.retrieval_time + response.generation_time,
            llm_usage=response.llm_usage,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/stream-query")
async def stream_query_documents(request: QueryRequest):
    """流式查询文档"""
    try:
        # 创建RAG系统
        retrieval_config = RetrievalConfig(
            collection_name=request.collection_name,
            top_k=request.top_k,
            score_threshold=request.score_threshold,
            enable_reranking=request.enable_reranking
        )
        
        current_rag = create_rag_system(
            collection_name=request.collection_name,
            prompt_template=request.prompt_template,
            retrieval_config=retrieval_config
        )
        
        async def generate_stream():
            """生成流式响应"""
            try:
                async for chunk in current_rag.stream_query(
                    question=request.question,
                    filters=request.filters
                ):
                    # 发送数据块
                    yield f"data: {json.dumps({'chunk': chunk, 'type': 'content'})}\n\n"
                
                # 发送结束标记
                yield f"data: {json.dumps({'type': 'done'})}\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'error': str(e), 'type': 'error'})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/conversation", response_model=QueryResponse)
async def conversation_query(request: ConversationRequest):
    """对话式查询"""
    try:
        # 获取或创建会话
        if request.session_id not in conversation_sessions:
            conversation_sessions[request.session_id] = create_rag_system(
                collection_name=request.collection_name,
                prompt_template=request.prompt_template,
                enable_conversation=True,
                max_history_turns=5
            )
        
        session_rag = conversation_sessions[request.session_id]
        
        # 执行对话查询
        response = await session_rag.aquery(
            question=request.question,
            filters=request.filters
        )
        
        # 转换响应格式
        documents = [
            DocumentInfo(
                content=doc.content,
                score=doc.score,
                source=doc.source,
                metadata=doc.metadata
            )
            for doc in response.retrieved_documents
        ]
        
        return QueryResponse(
            answer=response.answer,
            retrieved_documents=documents,
            retrieval_time=response.retrieval_time,
            generation_time=response.generation_time,
            total_time=response.retrieval_time + response.generation_time,
            llm_usage=response.llm_usage,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversation/{session_id}/history")
async def get_conversation_history(session_id: str):
    """获取对话历史"""
    try:
        if session_id not in conversation_sessions:
            raise HTTPException(status_code=404, detail="会话不存在")
        
        session_rag = conversation_sessions[session_id]
        history = session_rag.get_conversation_history()
        
        return {
            "session_id": session_id,
            "history": history,
            "turn_count": len(history)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/conversation/{session_id}")
async def clear_conversation(session_id: str):
    """清除对话会话"""
    try:
        if session_id in conversation_sessions:
            del conversation_sessions[session_id]
            return {"message": f"会话 {session_id} 已清除"}
        else:
            raise HTTPException(status_code=404, detail="会话不存在")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/info", response_model=SystemInfoResponse)
async def get_system_info():
    """获取系统信息"""
    try:
        if rag_system is None:
            raise HTTPException(status_code=500, detail="RAG系统未初始化")
        
        system_info = rag_system.get_system_info()
        
        return SystemInfoResponse(
            collection_name=system_info["collection_name"],
            prompt_template=system_info["prompt_template"],
            conversation_enabled=system_info["conversation_enabled"],
            retrieval_config=system_info["retrieval_config"],
            collection_info=system_info["collection_info"],
            system_status="running"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/sessions")
async def get_active_sessions():
    """获取活跃会话列表"""
    return {
        "active_sessions": list(conversation_sessions.keys()),
        "session_count": len(conversation_sessions)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### API测试脚本

创建 `test_api.py` 测试脚本：

```python
import requests
import json
import asyncio
import aiohttp
import time
from typing import Dict, Any

# API基础URL
BASE_URL = "http://localhost:8000"

def test_health_check():
    """测试健康检查"""
    print("🔍 测试健康检查...")
    
    try:
        response = requests.get(f"{BASE_URL}/health")
        if response.status_code == 200:
            data = response.json()
            print(f"✅ 健康检查通过: {data['status']}")
            print(f"📊 RAG系统状态: {data.get('rag_system', 'unknown')}")
            return True
        else:
            print(f"❌ 健康检查失败: {response.status_code}")
            return False
    except Exception as e:
        print(f"❌ 健康检查异常: {e}")
        return False

def test_basic_query():
    """测试基础查询"""
    print("\n🔍 测试基础查询...")
    
    query_data = {
        "question": "什么是Python编程语言？",
        "collection_name": "documents",
        "prompt_template": "basic",
        "top_k": 3,
        "score_threshold": 0.5
    }
    
    try:
        response = requests.post(
            f"{BASE_URL}/query",
            json=query_data,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"✅ 查询成功")
            print(f"📝 回答: {data['answer'][:200]}...")
            print(f"📊 检索时间: {data['retrieval_time']:.3f}s")
            print(f"📊 生成时间: {data['generation_time']:.3f}s")
            print(f"📊 检索文档数: {len(data['retrieved_documents'])}")
            
            # 显示检索到的文档
            for i, doc in enumerate(data['retrieved_documents'], 1):
                print(f"  文档{i}: 相似度={doc['score']:.3f}, 长度={len(doc['content'])}字符")
            
            return True
        else:
            print(f"❌ 查询失败: {response.status_code}")
            print(f"错误信息: {response.text}")
            return False
            
    except Exception as e:
        print(f"❌ 查询异常: {e}")
        return False

def test_stream_query():
    """测试流式查询"""
    print("\n🔍 测试流式查询...")
    
    query_data = {
        "question": "请详细介绍机器学习的基本概念和应用。",
        "collection_name": "documents",
        "prompt_template": "detailed",
        "top_k": 5
    }
    
    try:
        response = requests.post(
            f"{BASE_URL}/stream-query",
            json=query_data,
            headers={"Content-Type": "application/json"},
            stream=True
        )
        
        if response.status_code == 200:
            print("✅ 流式查询开始")
            print("📝 流式回答:")
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        try:
                            data = json.loads(line_str[6:])  # 去掉 'data: ' 前缀
                            if data.get('type') == 'content':
                                print(data['chunk'], end='', flush=True)
                            elif data.get('type') == 'done':
                                print("\n✅ 流式查询完成")
                                break
                            elif data.get('type') == 'error':
                                print(f"\n❌ 流式查询错误: {data['error']}")
                                return False
                        except json.JSONDecodeError:
                            continue
            
            return True
        else:
            print(f"❌ 流式查询失败: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"❌ 流式查询异常: {e}")
        return False

def test_conversation():
    """测试对话功能"""
    print("\n🔍 测试对话功能...")
    
    session_id = "test_session_001"
    
    # 多轮对话
    conversations = [
        "什么是深度学习？",
        "它和传统机器学习有什么区别？",
        "能举个深度学习的实际应用例子吗？"
    ]
    
    try:
        for i, question in enumerate(conversations, 1):
            print(f"\n🗣️ 第{i}轮对话")
            print(f"❓ 用户: {question}")
            
            conversation_data = {
                "question": question,
                "session_id": session_id,
                "collection_name": "documents",
                "prompt_template": "conversational"
            }
            
            response = requests.post(
                f"{BASE_URL}/conversation",
                json=conversation_data,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"🤖 助手: {data['answer'][:200]}...")
            else:
                print(f"❌ 对话失败: {response.status_code}")
                return False
        
        # 获取对话历史
        history_response = requests.get(f"{BASE_URL}/conversation/{session_id}/history")
        if history_response.status_code == 200:
            history_data = history_response.json()
            print(f"\n📚 对话历史 ({history_data['turn_count']} 轮):")
            for i, turn in enumerate(history_data['history'], 1):
                print(f"  第{i}轮 - 用户: {turn['user'][:50]}...")
                print(f"        助手: {turn['assistant'][:50]}...")
        
        # 清除会话
        clear_response = requests.delete(f"{BASE_URL}/conversation/{session_id}")
        if clear_response.status_code == 200:
            print(f"\n🗑️ 会话已清除")
        
        return True
        
    except Exception as e:
        print(f"❌ 对话测试异常: {e}")
        return False

def test_system_info():
    """测试系统信息"""
    print("\n🔍 测试系统信息...")
    
    try:
        response = requests.get(f"{BASE_URL}/system/info")
        
        if response.status_code == 200:
            data = response.json()
            print("✅ 系统信息获取成功")
            print(f"📊 集合名称: {data['collection_name']}")
            print(f"📊 Prompt模板: {data['prompt_template']}")
            print(f"📊 对话功能: {data['conversation_enabled']}")
            print(f"📊 系统状态: {data['system_status']}")
            
            retrieval_config = data['retrieval_config']
            print(f"📊 检索配置:")
            print(f"    top_k: {retrieval_config['top_k']}")
            print(f"    阈值: {retrieval_config['score_threshold']}")
            
            collection_info = data['collection_info']
            if 'error' not in collection_info:
                print(f"📊 集合信息:")
                print(f"    向量数量: {collection_info.get('vectors_count', 'N/A')}")
                print(f"    状态: {collection_info.get('status', 'N/A')}")
            
            return True
        else:
            print(f"❌ 系统信息获取失败: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"❌ 系统信息获取异常: {e}")
        return False

def test_advanced_query():
    """测试高级查询功能"""
    print("\n🔍 测试高级查询功能...")
    
    # 测试不同的Prompt模板
    templates = ["basic", "detailed", "structured"]
    
    for template in templates:
        print(f"\n📝 测试 {template} 模板")
        
        query_data = {
            "question": "比较Python和Java编程语言的特点",
            "collection_name": "documents",
            "prompt_template": template,
            "top_k": 5,
            "score_threshold": 0.6,
            "enable_reranking": True
        }
        
        try:
            response = requests.post(
                f"{BASE_URL}/query",
                json=query_data,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"  ✅ {template} 模板查询成功")
                print(f"  📊 回答长度: {len(data['answer'])} 字符")
                print(f"  📊 检索文档: {len(data['retrieved_documents'])} 个")
            else:
                print(f"  ❌ {template} 模板查询失败: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"  ❌ {template} 模板查询异常: {e}")
            return False
    
    return True

def test_performance():
    """测试性能"""
    print("\n🔍 测试API性能...")
    
    query_data = {
        "question": "什么是人工智能？",
        "collection_name": "documents",
        "top_k": 3
    }
    
    # 并发测试
    concurrent_requests = 5
    total_time = 0
    success_count = 0
    
    try:
        start_time = time.time()
        
        for i in range(concurrent_requests):
            response = requests.post(
                f"{BASE_URL}/query",
                json=query_data,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                success_count += 1
                data = response.json()
                total_time += data['total_time']
        
        end_time = time.time()
        wall_time = end_time - start_time
        
        print(f"✅ 性能测试完成")
        print(f"📊 成功请求: {success_count}/{concurrent_requests}")
        print(f"📊 平均响应时间: {total_time/success_count:.3f}s")
        print(f"📊 总耗时: {wall_time:.3f}s")
        print(f"📊 QPS: {success_count/wall_time:.2f}")
        
        return success_count == concurrent_requests
        
    except Exception as e:
        print(f"❌ 性能测试异常: {e}")
        return False

def main():
    """主测试函数"""
    print("🚀 开始API测试\n")
    print("请确保API服务已启动: uvicorn src.api.rag_api:app --reload")
    print("="*60)
    
    tests = [
        ("健康检查", test_health_check),
        ("基础查询", test_basic_query),
        ("流式查询", test_stream_query),
        ("对话功能", test_conversation),
        ("系统信息", test_system_info),
        ("高级查询", test_advanced_query),
        ("性能测试", test_performance)
    ]
    
    results = []
    for name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"测试: {name}")
        print(f"{'='*50}")
        
        result = test_func()
        results.append((name, result))
    
    print(f"\n{'='*50}")
    print("📊 测试结果汇总")
    print(f"{'='*50}")
    
    for name, result in results:
        status = "✅ 通过" if result else "❌ 失败"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    print(f"\n🎯 总体结果: {success_count}/{len(results)} 测试通过")
    
    if success_count == len(results):
        print("🎉 所有API测试通过！")
    else:
        print("⚠️ 部分API测试失败，请检查服务状态。")

if __name__ == "__main__":
    main()
```

### 启动和测试API

1. **启动API服务**：
```bash
# 开发模式启动
uvicorn src.api.rag_api:app --reload --host 0.0.0.0 --port 8000

# 或者直接运行
python src/api/rag_api.py
```

2. **访问API文档**：
```bash
# 打开浏览器访问
http://localhost:8000/docs
```

3. **运行API测试**：
```bash
python test_api.py
```

4. **使用curl测试**：
```bash
# 健康检查
curl http://localhost:8000/health

# 基础查询
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{
       "question": "什么是Python？",
       "collection_name": "documents",
       "top_k": 3
     }'

# 系统信息
curl http://localhost:8000/system/info
```

## 思考题

### 1. 系统架构思考
- 为什么要将检索器、LLM客户端和Prompt模板分别封装？这种设计有什么优势？
- 如何设计RAG系统的缓存策略来提高响应速度？
- 在高并发场景下，RAG系统可能遇到哪些瓶颈？如何优化？

### 2. 检索优化思考
- 重排序（reranking）在什么情况下能显著提升检索效果？
- 如何平衡检索的准确性和多样性？
- 向量检索的score_threshold应该如何设置？过高或过低会有什么影响？

### 3. Prompt工程思考
- 不同类型的问题（事实性、分析性、创造性）应该使用什么样的Prompt策略？
- 如何设计Prompt来减少LLM的幻觉问题？
- 在对话式RAG中，如何有效利用历史对话信息？

### 4. 性能优化思考
- 异步处理在RAG系统中的作用是什么？哪些环节适合异步化？
- 流式响应对用户体验有什么改善？实现时需要注意什么？
- 如何监控和评估RAG系统的性能指标？

### 5. 实际应用思考
- 在生产环境中部署RAG系统需要考虑哪些安全性问题？
- 如何处理不同领域或语言的文档检索？
- RAG系统如何与现有的业务系统集成？

## 实验检查清单

### ✅ 环境准备
- [ ] LLM API配置正确（OpenAI/Ollama等）
- [ ] Qdrant服务正常运行
- [ ] 所有依赖包安装完成
- [ ] 环境变量配置正确

### ✅ LLM客户端
- [ ] LLMClient类实现完整
- [ ] 支持同步、异步和流式聊天
- [ ] Token计数功能正常
- [ ] 错误处理机制完善
- [ ] 测试脚本运行通过

### ✅ Prompt模板
- [ ] 实现了4种不同类型的Prompt模板
- [ ] PromptManager管理功能正常
- [ ] 模板渲染结果符合预期
- [ ] 支持自定义模板扩展

### ✅ RAG检索器
- [ ] RAGRetriever类功能完整
- [ ] 支持同步和异步检索
- [ ] 重排序和多样性过滤正常工作
- [ ] Token长度控制有效
- [ ] 过滤条件支持完善

### ✅ RAG系统
- [ ] RAGSystem类集成所有组件
- [ ] 支持三种查询模式（同步、异步、流式）
- [ ] 对话历史管理功能正常
- [ ] 系统信息获取完整
- [ ] 配置管理灵活

### ✅ API接口
- [ ] FastAPI应用启动正常
- [ ] 所有API端点响应正确
- [ ] 请求验证和错误处理完善
- [ ] 流式响应功能正常
- [ ] 对话会话管理有效
- [ ] API文档生成正确

### ✅ 测试验证
- [ ] 所有单元测试通过
- [ ] API测试脚本运行成功
- [ ] 性能测试结果合理
- [ ] 错误场景处理正确

## 常见问题解决

### 1. LLM连接问题
```bash
# 检查API密钥
echo $OPENAI_API_KEY

# 测试网络连接
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models

# 使用本地模型（Ollama）
ollama serve
ollama pull llama2
```

### 2. Qdrant连接失败
```bash
# 检查Qdrant服务状态
docker ps | grep qdrant

# 重启Qdrant服务
docker restart qdrant

# 检查端口占用
lsof -i :6333
```

### 3. 向量检索无结果
- 检查集合是否存在向量数据
- 调整score_threshold阈值
- 验证查询向量生成是否正确
- 检查过滤条件是否过于严格

### 4. API响应慢
- 启用异步处理
- 优化检索参数（减少top_k）
- 使用更快的embedding模型
- 添加结果缓存

### 5. 内存使用过高
- 减少批处理大小
- 及时清理对话历史
- 优化向量存储配置
- 使用更小的模型

## 参考资料

### 技术文档
- [FastAPI官方文档](https://fastapi.tiangolo.com/)
- [Qdrant向量数据库](https://qdrant.tech/documentation/)
- [sentence-transformers](https://www.sbert.net/)
- [OpenAI API文档](https://platform.openai.com/docs/)

### 学术论文
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- "Dense Passage Retrieval for Open-Domain Question Answering"
- "FiD: Leveraging Passage Retrieval with Generative Models"

### 开源项目
- [LangChain](https://github.com/langchain-ai/langchain)
- [LlamaIndex](https://github.com/run-llama/llama_index)
- [Haystack](https://github.com/deepset-ai/haystack)

## 📝 实验完成后的Git操作

### 为什么要进行Git提交？

完成RAG系统完整实现后，进行Git提交至关重要：

1. **完整系统保存**：保存RAGSystem、LLMClient、RAGRetriever等核心组件的完整实现
2. **API接口管理**：记录FastAPI接口设计和RESTful服务实现
3. **Prompt工程记录**：保存各种Prompt模板和工程优化策略
4. **系统集成方案**：记录检索、生成、API等组件的集成架构
5. **测试用例保存**：保存完整的测试套件和验证脚本
6. **配置管理**：记录LLM配置、向量数据库配置等关键参数
7. **项目里程碑**：标记RAG系统从概念到完整实现的重要节点
8. **团队协作**：让团队成员了解完整RAG系统的实现细节

### Git操作步骤

#### 1. 检查当前修改状态
```bash
git status
```

**预期看到的文件变更**：
- `src/llm/llm_client.py` - LLM客户端实现
- `src/llm/prompt_templates.py` - Prompt模板系统
- `src/rag/rag_retriever.py` - RAG检索器
- `src/rag/rag_system.py` - RAG系统核心
- `src/api/rag_api.py` - FastAPI接口实现
- `src/config/rag_config.py` - RAG系统配置
- `tests/test_llm_client.py` - LLM客户端测试
- `tests/test_rag_system.py` - RAG系统测试
- `tests/test_api.py` - API接口测试
- `requirements.txt` - 新增FastAPI、OpenAI等依赖
- `.env.example` - 环境变量示例文件
- 可能的其他配置和文档文件

#### 2. 添加修改的文件
```bash
# 添加所有修改的文件
git add .

# 或者选择性添加特定文件
git add src/llm/
git add src/rag/
git add src/api/
git add src/config/
git add tests/
git add requirements.txt
git add .env.example
```

#### 3. 提交更改
```bash
git commit -m "完成lesson06实验：实现完整RAG系统

- 实现LLMClient类，支持同步、异步和流式聊天
- 实现多种Prompt模板（基础、详细、对话、结构化）
- 实现RAGRetriever，支持重排序和多样性过滤
- 实现RAGSystem核心类，集成所有组件
- 实现FastAPI接口，提供RESTful服务
- 添加完整的测试套件和API测试
- 支持对话历史管理和流式响应
- 完成系统性能监控和错误处理
- 实现配置管理和环境变量支持"
```

#### 4. 查看提交历史
```bash
git log --oneline -5
```

#### 5. 推送到远程仓库（可选）
```bash
# 如果需要推送到远程仓库
git push origin lesson06-rag-system

# 或推送到主分支（根据你的分支策略）
git push origin main
```

### 提交前验证清单

在提交之前，请确保：

- [ ] **LLM客户端验证**：LLMClient能正常调用各种LLM API
- [ ] **Prompt模板验证**：所有Prompt模板渲染结果符合预期
- [ ] **检索功能验证**：RAGRetriever能正常检索和重排序
- [ ] **RAG系统验证**：RAGSystem集成功能完整且稳定
- [ ] **API接口验证**：FastAPI服务能正常启动和响应
- [ ] **流式响应验证**：流式聊天功能正常工作
- [ ] **对话管理验证**：对话历史管理功能有效
- [ ] **测试通过**：所有测试用例运行成功
- [ ] **配置完整**：环境变量和配置文件设置正确
- [ ] **文档更新**：API文档和代码注释已更新

### RAG系统项目特殊注意事项

1. **API密钥保护**：
   ```bash
   # 确保.env文件被.gitignore排除
   echo ".env" >> .gitignore
   echo "*.key" >> .gitignore
   
   # 检查是否有敏感信息泄露
   grep -r "sk-\|api_key\|secret" src/ --exclude-dir=__pycache__ || echo "未发现敏感信息"
   ```

2. **大文件排除**：
   ```bash
   # 排除可能的大型模型缓存文件
   echo "models/" >> .gitignore
   echo "*.bin" >> .gitignore
   echo "cache/" >> .gitignore
   ```

3. **日志文件管理**：
   ```bash
   # 排除日志文件
   echo "logs/" >> .gitignore
   echo "*.log" >> .gitignore
   ```

4. **测试数据管理**：
   ```bash
   # 排除测试生成的临时数据
   echo "test_data/" >> .gitignore
   echo "temp/" >> .gitignore
   ```

### 常见问题解决

1. **大文件提交问题**：
   ```bash
   # 如果意外添加了大文件
   git reset HEAD large_file.bin
   git rm --cached large_file.bin
   ```

2. **敏感信息泄露**：
   ```bash
   # 如果意外提交了API密钥
   git reset --soft HEAD~1
   # 编辑文件移除敏感信息后重新提交
   ```

3. **提交信息修改**：
   ```bash
   # 修改最后一次提交信息
   git commit --amend -m "新的提交信息"
   ```

4. **撤销文件添加**：
   ```bash
   # 撤销git add操作
   git reset HEAD <文件名>
   ```

### RAG系统Git最佳实践

1. **分层提交策略**：
   - 先提交核心组件（LLMClient、RAGRetriever）
   - 再提交系统集成（RAGSystem）
   - 然后提交API接口（FastAPI）
   - 最后提交测试和文档

2. **功能分支管理**：
   ```bash
   # 为重要功能创建专门分支
   git checkout -b feature/streaming-response
   git checkout -b feature/conversation-history
   ```

3. **版本标记**：
   ```bash
   # 为重要版本打标签
   git tag -a v1.0-rag-mvp -m "RAG系统MVP版本"
   git tag -a v1.1-api-complete -m "完整API接口版本"
   ```

4. **配置版本管理**：
   - 使用配置文件管理系统参数
   - 版本控制配置变更历史
   - 记录不同环境的配置差异

### 系统部署准备

完成Git提交后，为生产部署做准备：

1. **环境配置检查**：
   ```bash
   # 验证环境变量配置
   python -c "from src.config.rag_config import RAGConfig; print('配置加载成功')"
   ```

2. **依赖版本锁定**：
   ```bash
   # 生成精确的依赖版本文件
   pip freeze > requirements-lock.txt
   ```

3. **Docker化准备**：
   ```bash
   # 创建Dockerfile（如果需要）
   # 准备docker-compose.yml
   ```

### 下一步学习指导

完成Git提交后，你可以：

1. **系统优化**：研究RAG系统的性能优化策略
2. **高级功能**：探索多模态RAG、图RAG等高级技术
3. **生产部署**：学习RAG系统的生产环境部署
4. **监控运维**：了解RAG系统的监控和运维最佳实践
5. **业务集成**：研究如何将RAG系统集成到实际业务场景

记住，完整的RAG系统是一个复杂的工程项目，良好的版本控制是确保项目成功的关键！

---

## 实验完成标志

完成本实验后，你应该能够：

1. **理解RAG系统架构**：掌握检索增强生成的核心原理和组件设计
2. **熟练使用LLM API**：能够封装和调用各种大语言模型接口
3. **设计有效的Prompt**：掌握不同场景下的Prompt工程技巧
4. **实现高效检索**：能够优化向量检索的准确性和性能
5. **构建完整系统**：能够集成各个组件构建可用的RAG应用
6. **开发API接口**：能够设计和实现RESTful API服务
7. **进行系统测试**：能够编写测试用例验证系统功能
8. **解决实际问题**：能够诊断和解决RAG系统的常见问题
9. ✅ **完成Git版本控制**：
   - 所有RAG系统代码已提交到Git仓库
   - 提交信息清晰描述了系统架构和功能
   - API接口、测试用例和配置文件已保存
   - 为后续的系统优化和部署做好准备

🎉 **恭喜完成第六课实验！你已经掌握了构建完整RAG系统的核心技能，并建立了良好的版本控制习惯。**