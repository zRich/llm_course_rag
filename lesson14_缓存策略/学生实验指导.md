# ç¬¬åå››èŠ‚è¯¾ï¼šç¼“å­˜ç­–ç•¥ - å­¦ç”Ÿå®éªŒæŒ‡å¯¼

## å®éªŒæ¦‚è¿°

**å®éªŒæ—¶é•¿**ï¼š15åˆ†é’Ÿ  
**å®éªŒç›®æ ‡**ï¼šå®ç°RAGç³»ç»Ÿçš„å¤šå±‚ç¼“å­˜ç­–ç•¥ï¼Œæå‡ç³»ç»Ÿæ€§èƒ½  
**å®éªŒç¯å¢ƒ**ï¼šPython 3.8+, Redis, å·²å®Œæˆå‰é¢è¯¾ç¨‹çš„RAGç³»ç»Ÿæ­å»º  

## å®éªŒå‡†å¤‡

### 1. ç¡®è®¤ç¯å¢ƒ
ç¡®ä¿ä½ å·²ç»å®Œæˆäº†å‰é¢è¯¾ç¨‹çš„å®éªŒï¼Œç‰¹åˆ«æ˜¯ï¼š
- å¼•ç”¨ä¸å¯æº¯æºè¾“å‡ºç³»ç»Ÿ
- å¤šæ–‡æ¡£å¤„ç†ç³»ç»Ÿ
- é‡æ’åºæ¨¡å‹é›†æˆ

### 2. å®‰è£…Redis

**macOSç”¨æˆ·**ï¼š
```bash
# ä½¿ç”¨Homebrewå®‰è£…Redis
brew install redis

# å¯åŠ¨RedisæœåŠ¡
brew services start redis

# éªŒè¯Redisæ˜¯å¦æ­£å¸¸è¿è¡Œ
redis-cli ping
# åº”è¯¥è¿”å› PONG
```

**Linuxç”¨æˆ·**ï¼š
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install redis-server

# å¯åŠ¨RedisæœåŠ¡
sudo systemctl start redis-server
sudo systemctl enable redis-server

# éªŒè¯Redis
redis-cli ping
```

**Windowsç”¨æˆ·**ï¼š
- ä¸‹è½½Redis for Windowsæˆ–ä½¿ç”¨WSL
- æˆ–ä½¿ç”¨Dockerï¼š`docker run -d -p 6379:6379 redis:alpine`

### 3. å®‰è£…Pythonä¾èµ–
```bash
# è¿›å…¥rag-systemç›®å½•
cd rag-system

# å®‰è£…Rediså®¢æˆ·ç«¯
uv add redis
uv add hiredis  # å¯é€‰ï¼Œæå‡æ€§èƒ½
```

## å®éªŒæ­¥éª¤

### æ­¥éª¤1ï¼šåˆ›å»ºlesson14åˆ†æ”¯

```bash
# ç¡®ä¿åœ¨rag-systemç›®å½•
cd rag-system

# ä»lesson13åˆ†æ”¯åˆ›å»ºlesson14åˆ†æ”¯
git checkout lesson13
git checkout -b lesson14

echo "å¼€å§‹ç¬¬14èŠ‚è¯¾ï¼šç¼“å­˜ç­–ç•¥å®éªŒ"
```

### æ­¥éª¤2ï¼šå®ç°ç¼“å­˜ç®¡ç†å™¨

åˆ›å»º `cache_manager.py`ï¼š

```python
import redis
import json
import time
import hashlib
from typing import Any, Optional, Dict
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    hits: int = 0
    misses: int = 0
    total_requests: int = 0
    
    @property
    def hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.hits / self.total_requests

class CacheManager:
    """å¤šå±‚ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_host='localhost', redis_port=6379, redis_db=0):
        # Redisè¿æ¥ï¼ˆL2ç¼“å­˜ï¼‰
        try:
            self.redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port, 
                db=redis_db,
                decode_responses=True
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            self.redis_available = True
            print("âœ… Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Redisè¿æ¥å¤±è´¥: {e}")
            self.redis_available = False
        
        # æœ¬åœ°å†…å­˜ç¼“å­˜ï¼ˆL1ç¼“å­˜ï¼‰
        self.local_cache: Dict[str, Dict] = {}
        self.local_cache_ttl: Dict[str, datetime] = {}
        
        # ç¼“å­˜é…ç½®
        self.ttl_config = {
            'query_result': 3600,      # æŸ¥è¯¢ç»“æœ1å°æ—¶
            'vector_search': 7200,     # å‘é‡æ£€ç´¢2å°æ—¶
            'document_content': 86400, # æ–‡æ¡£å†…å®¹24å°æ—¶
            'rerank_scores': 1800      # é‡æ’åºåˆ†æ•°30åˆ†é’Ÿ
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = CacheStats()
        
        # L1ç¼“å­˜å¤§å°é™åˆ¶
        self.max_local_cache_size = 1000
    
    def _generate_cache_key(self, prefix: str, data: Any) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if isinstance(data, str):
            content = data
        else:
            content = json.dumps(data, sort_keys=True)
        
        hash_obj = hashlib.md5(content.encode('utf-8'))
        return f"{prefix}:{hash_obj.hexdigest()}"
    
    def _cleanup_local_cache(self):
        """æ¸…ç†è¿‡æœŸçš„æœ¬åœ°ç¼“å­˜"""
        current_time = datetime.now()
        expired_keys = [
            key for key, ttl in self.local_cache_ttl.items()
            if ttl < current_time
        ]
        
        for key in expired_keys:
            self.local_cache.pop(key, None)
            self.local_cache_ttl.pop(key, None)
    
    def _evict_local_cache(self):
        """LRUæ·˜æ±°æœ¬åœ°ç¼“å­˜"""
        if len(self.local_cache) >= self.max_local_cache_size:
            # ç®€å•çš„FIFOæ·˜æ±°ç­–ç•¥
            oldest_key = next(iter(self.local_cache))
            self.local_cache.pop(oldest_key, None)
            self.local_cache_ttl.pop(oldest_key, None)
    
    def get(self, cache_type: str, key_data: Any) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        self.stats.total_requests += 1
        cache_key = self._generate_cache_key(cache_type, key_data)
        
        # æ¸…ç†è¿‡æœŸçš„æœ¬åœ°ç¼“å­˜
        self._cleanup_local_cache()
        
        # L1ç¼“å­˜æ£€æŸ¥
        if cache_key in self.local_cache:
            self.stats.hits += 1
            print(f"ğŸ¯ L1ç¼“å­˜å‘½ä¸­: {cache_key[:20]}...")
            return self.local_cache[cache_key]
        
        # L2ç¼“å­˜æ£€æŸ¥ï¼ˆRedisï¼‰
        if self.redis_available:
            try:
                redis_result = self.redis_client.get(cache_key)
                if redis_result:
                    result = json.loads(redis_result)
                    
                    # å›å¡«åˆ°L1ç¼“å­˜
                    self._evict_local_cache()
                    self.local_cache[cache_key] = result
                    ttl_seconds = self.ttl_config.get(cache_type, 3600)
                    self.local_cache_ttl[cache_key] = datetime.now() + timedelta(seconds=ttl_seconds)
                    
                    self.stats.hits += 1
                    print(f"ğŸ¯ L2ç¼“å­˜å‘½ä¸­: {cache_key[:20]}...")
                    return result
            except Exception as e:
                print(f"Redisè¯»å–é”™è¯¯: {e}")
        
        # ç¼“å­˜æœªå‘½ä¸­
        self.stats.misses += 1
        print(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key[:20]}...")
        return None
    
    def set(self, cache_type: str, key_data: Any, value: Any) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(cache_type, key_data)
        ttl_seconds = self.ttl_config.get(cache_type, 3600)
        
        try:
            # è®¾ç½®L2ç¼“å­˜ï¼ˆRedisï¼‰
            if self.redis_available:
                self.redis_client.setex(
                    cache_key, 
                    ttl_seconds, 
                    json.dumps(value)
                )
            
            # è®¾ç½®L1ç¼“å­˜
            self._evict_local_cache()
            self.local_cache[cache_key] = value
            self.local_cache_ttl[cache_key] = datetime.now() + timedelta(seconds=ttl_seconds)
            
            print(f"ğŸ’¾ ç¼“å­˜å·²è®¾ç½®: {cache_key[:20]}...")
            return True
            
        except Exception as e:
            print(f"ç¼“å­˜è®¾ç½®é”™è¯¯: {e}")
            return False
    
    def delete(self, cache_type: str, key_data: Any) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(cache_type, key_data)
        
        try:
            # åˆ é™¤L1ç¼“å­˜
            self.local_cache.pop(cache_key, None)
            self.local_cache_ttl.pop(cache_key, None)
            
            # åˆ é™¤L2ç¼“å­˜
            if self.redis_available:
                self.redis_client.delete(cache_key)
            
            print(f"ğŸ—‘ï¸ ç¼“å­˜å·²åˆ é™¤: {cache_key[:20]}...")
            return True
            
        except Exception as e:
            print(f"ç¼“å­˜åˆ é™¤é”™è¯¯: {e}")
            return False
    
    def clear_all(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        try:
            # æ¸…ç©ºL1ç¼“å­˜
            self.local_cache.clear()
            self.local_cache_ttl.clear()
            
            # æ¸…ç©ºL2ç¼“å­˜
            if self.redis_available:
                self.redis_client.flushdb()
            
            print("ğŸ§¹ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            print(f"æ¸…ç©ºç¼“å­˜é”™è¯¯: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'hit_rate': f"{self.stats.hit_rate:.2%}",
            'total_requests': self.stats.total_requests,
            'cache_hits': self.stats.hits,
            'cache_misses': self.stats.misses,
            'l1_cache_size': len(self.local_cache),
            'redis_available': self.redis_available
        }
    
    def print_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        print("\nğŸ“Š ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   å‘½ä¸­ç‡: {stats['hit_rate']}")
        print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"   ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
        print(f"   ç¼“å­˜æœªå‘½ä¸­: {stats['cache_misses']}")
        print(f"   L1ç¼“å­˜å¤§å°: {stats['l1_cache_size']}")
        print(f"   Rediså¯ç”¨: {stats['redis_available']}")
```

### æ­¥éª¤3ï¼šé›†æˆç¼“å­˜åˆ°RAGç³»ç»Ÿ

åˆ›å»º `cached_rag_system.py`ï¼š

```python
import time
from typing import List, Dict, Any
from cache_manager import CacheManager

class CachedRAGSystem:
    """å¸¦ç¼“å­˜çš„RAGç³»ç»Ÿ"""
    
    def __init__(self):
        self.cache_manager = CacheManager()
        print("ğŸš€ ç¼“å­˜RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def normalize_query(self, query: str) -> str:
        """æ ‡å‡†åŒ–æŸ¥è¯¢ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡"""
        import re
        # å»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€å¤§å°å†™
        normalized = re.sub(r'\s+', ' ', query.lower().strip())
        # å»é™¤æ ‡ç‚¹ç¬¦å·
        normalized = re.sub(r'[^\w\s]', '', normalized)
        return normalized
    
    def simulate_vector_search(self, query: str) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿå‘é‡æ£€ç´¢ï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®çš„å‘é‡æ£€ç´¢ï¼‰"""
        print(f"ğŸ” æ‰§è¡Œå‘é‡æ£€ç´¢: {query}")
        time.sleep(0.5)  # æ¨¡æ‹Ÿæ£€ç´¢è€—æ—¶
        
        # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
        results = [
            {
                'id': f'doc_{i}',
                'content': f'è¿™æ˜¯å…³äº"{query}"çš„æ–‡æ¡£å†…å®¹ {i}',
                'score': 0.9 - i * 0.1,
                'metadata': {'source': f'document_{i}.pdf', 'page': i+1}
            }
            for i in range(3)
        ]
        return results
    
    def simulate_rerank(self, query: str, documents: List[Dict]) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿé‡æ’åºï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®çš„é‡æ’åºï¼‰"""
        print(f"ğŸ”„ æ‰§è¡Œé‡æ’åº: {len(documents)} ä¸ªæ–‡æ¡£")
        time.sleep(0.3)  # æ¨¡æ‹Ÿé‡æ’åºè€—æ—¶
        
        # ç®€å•çš„é‡æ’åºé€»è¾‘
        for doc in documents:
            doc['rerank_score'] = doc['score'] * 1.1
        
        return sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
    
    def simulate_llm_generation(self, query: str, context_docs: List[Dict]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸLLMç”Ÿæˆï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®çš„LLMè°ƒç”¨ï¼‰"""
        print(f"ğŸ¤– æ‰§è¡ŒLLMç”Ÿæˆ")
        time.sleep(1.0)  # æ¨¡æ‹ŸLLMç”Ÿæˆè€—æ—¶
        
        # æ¨¡æ‹Ÿç”Ÿæˆç»“æœ
        context_text = "\n".join([doc['content'] for doc in context_docs[:2]])
        answer = f"æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå…³äº"{query}"çš„å›ç­”æ˜¯ï¼š{context_text[:100]}..."
        
        return {
            'answer': answer,
            'sources': [doc['metadata'] for doc in context_docs[:2]],
            'confidence': 0.85
        }
    
    def search_with_cache(self, query: str) -> Dict[str, Any]:
        """å¸¦ç¼“å­˜çš„RAGæŸ¥è¯¢"""
        start_time = time.time()
        
        # æ ‡å‡†åŒ–æŸ¥è¯¢
        normalized_query = self.normalize_query(query)
        print(f"\nğŸ” å¤„ç†æŸ¥è¯¢: {query}")
        print(f"ğŸ“ æ ‡å‡†åŒ–å: {normalized_query}")
        
        # 1. æ£€æŸ¥å®Œæ•´ç»“æœç¼“å­˜
        cached_result = self.cache_manager.get('query_result', normalized_query)
        if cached_result:
            elapsed_time = time.time() - start_time
            cached_result['response_time'] = f"{elapsed_time:.3f}s"
            cached_result['from_cache'] = True
            return cached_result
        
        # 2. æ£€æŸ¥å‘é‡æ£€ç´¢ç¼“å­˜
        cached_vector_results = self.cache_manager.get('vector_search', normalized_query)
        if cached_vector_results:
            print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„å‘é‡æ£€ç´¢ç»“æœ")
            vector_results = cached_vector_results
        else:
            vector_results = self.simulate_vector_search(normalized_query)
            self.cache_manager.set('vector_search', normalized_query, vector_results)
        
        # 3. æ£€æŸ¥é‡æ’åºç¼“å­˜
        rerank_cache_key = f"{normalized_query}_{len(vector_results)}"
        cached_rerank_results = self.cache_manager.get('rerank_scores', rerank_cache_key)
        if cached_rerank_results:
            print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„é‡æ’åºç»“æœ")
            reranked_results = cached_rerank_results
        else:
            reranked_results = self.simulate_rerank(normalized_query, vector_results)
            self.cache_manager.set('rerank_scores', rerank_cache_key, reranked_results)
        
        # 4. LLMç”Ÿæˆï¼ˆé€šå¸¸ä¸ç¼“å­˜ï¼Œå› ä¸ºå¯èƒ½éœ€è¦å®æ—¶æ€§ï¼‰
        final_result = self.simulate_llm_generation(normalized_query, reranked_results)
        
        # 5. ç¼“å­˜å®Œæ•´ç»“æœ
        elapsed_time = time.time() - start_time
        final_result['response_time'] = f"{elapsed_time:.3f}s"
        final_result['from_cache'] = False
        final_result['retrieved_docs'] = reranked_results
        
        self.cache_manager.set('query_result', normalized_query, final_result)
        
        return final_result
    
    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return self.cache_manager.get_stats()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        return self.cache_manager.clear_all()
```

### æ­¥éª¤4ï¼šåˆ›å»ºæµ‹è¯•è„šæœ¬

åˆ›å»º `test_cache_system.py`ï¼š

```python
import time
from cached_rag_system import CachedRAGSystem

def test_cache_performance():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    print("ğŸ§ª å¼€å§‹ç¼“å­˜æ€§èƒ½æµ‹è¯•\n")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag_system = CachedRAGSystem()
    
    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†",
        "æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",  # é‡å¤æŸ¥è¯¢
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
        "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†",  # é‡å¤æŸ¥è¯¢
        "è®¡ç®—æœºè§†è§‰åº”ç”¨",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",  # å†æ¬¡é‡å¤
    ]
    
    print("ğŸ“‹ æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨:")
    for i, query in enumerate(test_queries, 1):
        print(f"   {i}. {query}")
    print()
    
    # æ‰§è¡Œæµ‹è¯•
    total_time = 0
    results = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*50}")
        print(f"ğŸ” æµ‹è¯• {i}/{len(test_queries)}: {query}")
        print(f"{'='*50}")
        
        start_time = time.time()
        result = rag_system.search_with_cache(query)
        end_time = time.time()
        
        query_time = end_time - start_time
        total_time += query_time
        
        results.append({
            'query': query,
            'time': query_time,
            'from_cache': result.get('from_cache', False),
            'answer_length': len(result.get('answer', ''))
        })
        
        print(f"\nğŸ“Š æŸ¥è¯¢ç»“æœ:")
        print(f"   å“åº”æ—¶é—´: {query_time:.3f}s")
        print(f"   æ¥æº: {'ç¼“å­˜' if result.get('from_cache') else 'å®æ—¶è®¡ç®—'}")
        print(f"   ç­”æ¡ˆé•¿åº¦: {len(result.get('answer', ''))} å­—ç¬¦")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        rag_system.cache_manager.print_stats()
        
        time.sleep(0.5)  # çŸ­æš‚æš‚åœ
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ“ˆ æœ€ç»ˆæµ‹è¯•ç»Ÿè®¡")
    print(f"{'='*60}")
    
    cache_hits = sum(1 for r in results if r['from_cache'])
    cache_misses = len(results) - cache_hits
    avg_time = total_time / len(results)
    
    print(f"æ€»æŸ¥è¯¢æ•°: {len(results)}")
    print(f"ç¼“å­˜å‘½ä¸­: {cache_hits}")
    print(f"ç¼“å­˜æœªå‘½ä¸­: {cache_misses}")
    print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_hits/len(results):.2%}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
    print(f"æ€»è€—æ—¶: {total_time:.3f}s")
    
    # è¯¦ç»†ç»“æœ
    print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for i, result in enumerate(results, 1):
        status = "ğŸ¯ç¼“å­˜" if result['from_cache'] else "âš¡è®¡ç®—"
        print(f"   {i}. {result['query'][:20]}... - {result['time']:.3f}s - {status}")
    
    return results

def test_cache_strategies():
    """æµ‹è¯•ä¸åŒç¼“å­˜ç­–ç•¥"""
    print("\nğŸ”¬ æµ‹è¯•ç¼“å­˜ç­–ç•¥æ•ˆæœ\n")
    
    rag_system = CachedRAGSystem()
    
    # æµ‹è¯•æŸ¥è¯¢æ ‡å‡†åŒ–
    queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "  ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½  ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼ï¼ï¼"
    ]
    
    print("ğŸ§ª æµ‹è¯•æŸ¥è¯¢æ ‡å‡†åŒ–æ•ˆæœ:")
    for query in queries:
        normalized = rag_system.normalize_query(query)
        print(f"   åŸå§‹: '{query}'")
        print(f"   æ ‡å‡†åŒ–: '{normalized}'")
        print()
    
    # æµ‹è¯•ç›¸åŒæ ‡å‡†åŒ–æŸ¥è¯¢çš„ç¼“å­˜å‘½ä¸­
    print("ğŸ¯ æµ‹è¯•æ ‡å‡†åŒ–æŸ¥è¯¢çš„ç¼“å­˜å‘½ä¸­:")
    for i, query in enumerate(queries, 1):
        print(f"\n--- æŸ¥è¯¢ {i} ---")
        result = rag_system.search_with_cache(query)
        print(f"æ¥æº: {'ç¼“å­˜' if result.get('from_cache') else 'å®æ—¶è®¡ç®—'}")
    
    rag_system.cache_manager.print_stats()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ RAGç¼“å­˜ç³»ç»Ÿæµ‹è¯•å¼€å§‹\n")
    
    try:
        # æµ‹è¯•1ï¼šåŸºæœ¬ç¼“å­˜æ€§èƒ½
        test_cache_performance()
        
        # æµ‹è¯•2ï¼šç¼“å­˜ç­–ç•¥
        test_cache_strategies()
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

### æ­¥éª¤5ï¼šè¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œç¼“å­˜ç³»ç»Ÿæµ‹è¯•
python test_cache_system.py
```

### æ­¥éª¤6ï¼šæ€§èƒ½å¯¹æ¯”æµ‹è¯•

åˆ›å»º `performance_comparison.py`ï¼š

```python
import time
import statistics
from cached_rag_system import CachedRAGSystem

def simulate_no_cache_system(query: str) -> dict:
    """æ¨¡æ‹Ÿæ— ç¼“å­˜çš„RAGç³»ç»Ÿ"""
    print(f"ğŸ” æ— ç¼“å­˜æŸ¥è¯¢: {query}")
    
    # æ¨¡æ‹Ÿå„ä¸ªæ­¥éª¤çš„è€—æ—¶
    time.sleep(0.5)  # å‘é‡æ£€ç´¢
    time.sleep(0.3)  # é‡æ’åº
    time.sleep(1.0)  # LLMç”Ÿæˆ
    
    return {
        'answer': f'å…³äº"{query}"çš„å›ç­”ï¼ˆæ— ç¼“å­˜ï¼‰',
        'from_cache': False
    }

def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("âš¡ ç¼“å­˜vsæ— ç¼“å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•\n")
    
    # åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
    cached_system = CachedRAGSystem()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ åŸç†",
        "è‡ªç„¶è¯­è¨€å¤„ç†",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",  # é‡å¤
        "è®¡ç®—æœºè§†è§‰",
        "æ·±åº¦å­¦ä¹ åŸç†",   # é‡å¤
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",  # å†æ¬¡é‡å¤
    ]
    
    # æ— ç¼“å­˜æµ‹è¯•
    print("ğŸš« æ— ç¼“å­˜ç³»ç»Ÿæµ‹è¯•:")
    no_cache_times = []
    
    for i, query in enumerate(test_queries, 1):
        start_time = time.time()
        result = simulate_no_cache_system(query)
        end_time = time.time()
        
        query_time = end_time - start_time
        no_cache_times.append(query_time)
        
        print(f"   {i}. {query} - {query_time:.3f}s")
    
    # ç¼“å­˜ç³»ç»Ÿæµ‹è¯•
    print("\nğŸ’¾ ç¼“å­˜ç³»ç»Ÿæµ‹è¯•:")
    cached_times = []
    
    for i, query in enumerate(test_queries, 1):
        start_time = time.time()
        result = cached_system.search_with_cache(query)
        end_time = time.time()
        
        query_time = end_time - start_time
        cached_times.append(query_time)
        
        status = "ğŸ¯ç¼“å­˜" if result.get('from_cache') else "âš¡è®¡ç®—"
        print(f"   {i}. {query} - {query_time:.3f}s - {status}")
    
    # ç»Ÿè®¡å¯¹æ¯”
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»Ÿè®¡:")
    print(f"{'æŒ‡æ ‡':<15} {'æ— ç¼“å­˜':<10} {'æœ‰ç¼“å­˜':<10} {'æå‡':<10}")
    print("-" * 50)
    
    # æ€»æ—¶é—´
    total_no_cache = sum(no_cache_times)
    total_cached = sum(cached_times)
    improvement = (total_no_cache - total_cached) / total_no_cache * 100
    
    print(f"{'æ€»æ—¶é—´':<15} {total_no_cache:<10.3f} {total_cached:<10.3f} {improvement:<10.1f}%")
    
    # å¹³å‡æ—¶é—´
    avg_no_cache = statistics.mean(no_cache_times)
    avg_cached = statistics.mean(cached_times)
    avg_improvement = (avg_no_cache - avg_cached) / avg_no_cache * 100
    
    print(f"{'å¹³å‡æ—¶é—´':<15} {avg_no_cache:<10.3f} {avg_cached:<10.3f} {avg_improvement:<10.1f}%")
    
    # ç¼“å­˜ç»Ÿè®¡
    cache_stats = cached_system.get_cache_stats()
    print(f"\nğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']}")
    print(f"ğŸ“ˆ æ€»è¯·æ±‚æ•°: {cache_stats['total_requests']}")
    
    return {
        'no_cache_times': no_cache_times,
        'cached_times': cached_times,
        'improvement': improvement,
        'cache_stats': cache_stats
    }

if __name__ == "__main__":
    performance_comparison()
```

## å®éªŒéªŒè¯

### 1. è¿è¡Œæ€§èƒ½å¯¹æ¯”
```bash
python performance_comparison.py
```

### 2. æ£€æŸ¥Redisç¼“å­˜
```bash
# è¿æ¥RedisæŸ¥çœ‹ç¼“å­˜æ•°æ®
redis-cli

# æŸ¥çœ‹æ‰€æœ‰é”®
KEYS *

# æŸ¥çœ‹ç‰¹å®šé”®çš„å€¼
GET query_result:xxxxx

# æŸ¥çœ‹é”®çš„TTL
TTL query_result:xxxxx

# é€€å‡º
EXIT
```

### 3. ç›‘æ§ç¼“å­˜æ€§èƒ½
åˆ›å»º `cache_monitor.py`ï¼š

```python
import time
from cached_rag_system import CachedRAGSystem

def monitor_cache_performance():
    """ç›‘æ§ç¼“å­˜æ€§èƒ½"""
    rag_system = CachedRAGSystem()
    
    queries = [
        "äººå·¥æ™ºèƒ½å‘å±•å†å²",
        "æœºå™¨å­¦ä¹ ç®—æ³•åˆ†ç±»", 
        "æ·±åº¦å­¦ä¹ ç½‘ç»œç»“æ„",
        "äººå·¥æ™ºèƒ½å‘å±•å†å²",  # é‡å¤
        "è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨",
        "æœºå™¨å­¦ä¹ ç®—æ³•åˆ†ç±»",  # é‡å¤
    ]
    
    print("ğŸ“Š å®æ—¶ç¼“å­˜æ€§èƒ½ç›‘æ§\n")
    
    for i, query in enumerate(queries, 1):
        print(f"\n--- æŸ¥è¯¢ {i} ---")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = time.time()
        result = rag_system.search_with_cache(query)
        end_time = time.time()
        
        # æ˜¾ç¤ºç»“æœ
        print(f"æŸ¥è¯¢: {query}")
        print(f"è€—æ—¶: {end_time - start_time:.3f}s")
        print(f"æ¥æº: {'ç¼“å­˜' if result.get('from_cache') else 'å®æ—¶è®¡ç®—'}")
        
        # æ˜¾ç¤ºå®æ—¶ç»Ÿè®¡
        stats = rag_system.get_cache_stats()
        print(f"å½“å‰å‘½ä¸­ç‡: {stats['hit_rate']}")
        print(f"L1ç¼“å­˜å¤§å°: {stats['l1_cache_size']}")
        
        time.sleep(1)

if __name__ == "__main__":
    monitor_cache_performance()
```

## å®éªŒæ€»ç»“

### é¢„æœŸç»“æœ
å®Œæˆå®éªŒåï¼Œä½ åº”è¯¥è§‚å¯Ÿåˆ°ï¼š

1. **æ€§èƒ½æå‡**ï¼š
   - ç¼“å­˜å‘½ä¸­çš„æŸ¥è¯¢å“åº”æ—¶é—´æ˜¾è‘—é™ä½
   - é‡å¤æŸ¥è¯¢å‡ ä¹ç¬æ—¶è¿”å›ç»“æœ
   - æ•´ä½“ç³»ç»Ÿååé‡æå‡50%ä»¥ä¸Š

2. **ç¼“å­˜æ•ˆæœ**ï¼š
   - ç¼“å­˜å‘½ä¸­ç‡é€æ­¥æå‡
   - L1å’ŒL2ç¼“å­˜ååŒå·¥ä½œ
   - å†…å­˜ä½¿ç”¨åˆç†ï¼Œæ— æ˜æ˜¾æ³„æ¼

3. **ç³»ç»Ÿç¨³å®šæ€§**ï¼š
   - Redisè¿æ¥ç¨³å®š
   - ç¼“å­˜å¤±æ•ˆæœºåˆ¶æ­£å¸¸å·¥ä½œ
   - é”™è¯¯å¤„ç†æœºåˆ¶æœ‰æ•ˆ

### å¸¸è§é—®é¢˜

**Q1: Redisè¿æ¥å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**
A1: æ£€æŸ¥RedisæœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Œç«¯å£æ˜¯å¦æ­£ç¡®ï¼Œé˜²ç«å¢™è®¾ç½®ç­‰ã€‚

**Q2: ç¼“å­˜å‘½ä¸­ç‡å¾ˆä½æ€ä¹ˆåŠï¼Ÿ**
A2: æ£€æŸ¥æŸ¥è¯¢æ ‡å‡†åŒ–é€»è¾‘ï¼Œè°ƒæ•´TTLè®¾ç½®ï¼Œå¢åŠ é¢„çƒ­æ•°æ®ã€‚

**Q3: å†…å­˜ä½¿ç”¨è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ**
A3: è°ƒæ•´L1ç¼“å­˜å¤§å°é™åˆ¶ï¼Œä¼˜åŒ–æ•°æ®ç»“æ„ï¼Œå¢åŠ æ¸…ç†é¢‘ç‡ã€‚

### æ‰©å±•å®éªŒ

1. **å®ç°ç¼“å­˜é¢„çƒ­**ï¼šåœ¨ç³»ç»Ÿå¯åŠ¨æ—¶é¢„åŠ è½½çƒ­é—¨æŸ¥è¯¢
2. **æ·»åŠ ç¼“å­˜ç›‘æ§**ï¼šå®ç°è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†
3. **ä¼˜åŒ–ç¼“å­˜ç­–ç•¥**ï¼šå®ç°æ›´æ™ºèƒ½çš„LRUç®—æ³•
4. **åˆ†å¸ƒå¼ç¼“å­˜**ï¼šé…ç½®Redisé›†ç¾¤æµ‹è¯•

## æäº¤è¦æ±‚

è¯·æäº¤ä»¥ä¸‹æ–‡ä»¶ï¼š
- `cache_manager.py` - ç¼“å­˜ç®¡ç†å™¨å®ç°
- `cached_rag_system.py` - é›†æˆç¼“å­˜çš„RAGç³»ç»Ÿ
- `test_cache_system.py` - åŸºç¡€æµ‹è¯•è„šæœ¬
- `performance_comparison.py` - æ€§èƒ½å¯¹æ¯”æµ‹è¯•
- æµ‹è¯•è¿è¡Œæˆªå›¾å’Œæ€§èƒ½æ•°æ®
- å®éªŒæ€»ç»“æŠ¥å‘Šï¼ˆåŒ…å«æ€§èƒ½æå‡æ•°æ®å’Œé—®é¢˜åˆ†æï¼‰

---

**å®éªŒå®Œæˆæ ‡å¿—**ï¼š
âœ… ç¼“å­˜ç³»ç»Ÿæ­£å¸¸è¿è¡Œ  
âœ… æ€§èƒ½æå‡è¾¾åˆ°é¢„æœŸ  
âœ… ç¼“å­˜å‘½ä¸­ç‡>60%  
âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡