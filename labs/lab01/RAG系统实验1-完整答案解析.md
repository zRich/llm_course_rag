# RAGç³»ç»Ÿå®éªŒ1 - å®Œæ•´ç­”æ¡ˆè§£æ

## ğŸ“‹ å®éªŒæ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›RAGç³»ç»Ÿå®éªŒ1çš„å®Œæ•´ç­”æ¡ˆè§£æï¼ŒåŒ…å«æ‰€æœ‰5ä¸ªå®éªŒä»»åŠ¡çš„è¯¦ç»†å®ç°æ–¹æ¡ˆã€å…³é”®æŠ€æœ¯ç‚¹è§£æå’Œæµ‹è¯•éªŒè¯æ–¹æ³•ã€‚å­¦ç”Ÿå¯ä»¥å‚è€ƒæœ¬æ–‡æ¡£ç†è§£æ¯ä¸ªä»»åŠ¡çš„å®ç°åŸç†å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ¯ å®éªŒä»»åŠ¡æ€»è§ˆ

| ä»»åŠ¡ç¼–å· | ä»»åŠ¡åç§° | æ ¸å¿ƒæŠ€æœ¯ | éš¾åº¦ç­‰çº§ |
|---------|---------|---------|---------|
| ä»»åŠ¡1 | æ–‡æ¡£ä¸Šä¼ å’Œå¤„ç† | æ–‡ä»¶å¤„ç†ã€PDFè§£æã€æ–‡æœ¬åˆ†å— | â­â­ |
| ä»»åŠ¡2 | æ–‡æ¡£å‘é‡åŒ– | åµŒå…¥æ¨¡å‹ã€å‘é‡ç”Ÿæˆã€æ‰¹å¤„ç† | â­â­â­ |
| ä»»åŠ¡3 | è¯­ä¹‰æœç´¢å®ç° | å‘é‡æœç´¢ã€ç›¸ä¼¼åº¦è®¡ç®—ã€ç»“æœæ’åº | â­â­â­ |
| ä»»åŠ¡4 | RAGé—®ç­”ç³»ç»Ÿ | æ£€ç´¢å¢å¼ºç”Ÿæˆã€æç¤ºå·¥ç¨‹ã€LLMè°ƒç”¨ | â­â­â­â­ |
| ä»»åŠ¡5 | ç³»ç»Ÿç›‘æ§å’Œå¥åº·æ£€æŸ¥ | ç³»ç»Ÿç›‘æ§ã€çŠ¶æ€æ£€æŸ¥ã€æ€§èƒ½ç»Ÿè®¡ | â­â­ |

---

## ğŸ“„ ä»»åŠ¡1ï¼šæ–‡æ¡£ä¸Šä¼ å’Œå¤„ç†

### ğŸ¯ ä»»åŠ¡ç›®æ ‡

å®ç°PDFæ–‡æ¡£çš„ä¸Šä¼ ã€è§£æå’Œæ–‡æœ¬åˆ†å—åŠŸèƒ½ï¼Œä¸ºåç»­çš„å‘é‡åŒ–å’Œæ£€ç´¢å¥ å®šåŸºç¡€ã€‚

### ğŸ”§ æ ¸å¿ƒå®ç°

#### 1.1 æ–‡æ¡£å¤„ç†æœåŠ¡ (`src/services/document_processor.py`)

```python
import os
import uuid
from typing import List, Optional
from pathlib import Path
import logging
from sqlalchemy.orm import Session

from ..models.document import Document
from ..models.chunk import Chunk
from ..services.pdf_parser import PDFParser
from ..services.txt_parser import TxtParser
from ..services.text_splitter import TextSplitter
from ..utils.logger import get_logger

logger = get_logger(__name__)

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†æœåŠ¡ - è´Ÿè´£æ–‡æ¡£ä¸Šä¼ ã€è§£æå’Œåˆ†å—"""
    
    def __init__(self, upload_dir: str = "uploads"):
        self.upload_dir = Path(upload_dir)
        self.upload_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–è§£æå™¨
        self.pdf_parser = PDFParser()
        self.txt_parser = TxtParser()
        self.text_splitter = TextSplitter()
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_types = {
            '.pdf': self.pdf_parser,
            '.txt': self.txt_parser
        }
    
    async def process_document(
        self, 
        file_content: bytes, 
        filename: str,
        title: Optional[str] = None,
        description: Optional[str] = None,
        db: Session = None
    ) -> Document:
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            title: æ–‡æ¡£æ ‡é¢˜
            description: æ–‡æ¡£æè¿°
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            Document: åˆ›å»ºçš„æ–‡æ¡£å¯¹è±¡
        """
        try:
            # TODO(lab01-task1): å®ç°æ–‡æ¡£å¤„ç†é€»è¾‘
            # 1. éªŒè¯æ–‡ä»¶ç±»å‹å’Œå¤§å°
            file_ext = Path(filename).suffix.lower()
            if file_ext not in self.supported_types:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶ä¸º10MB)
            max_size = 10 * 1024 * 1024  # 10MB
            if len(file_content) > max_size:
                raise ValueError(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {len(file_content)} bytes")
            
            # 2. ä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°
            file_id = str(uuid.uuid4())
            file_path = self.upload_dir / f"{file_id}{file_ext}"
            
            with open(file_path, 'wb') as f:
                f.write(file_content)
            
            # 3. åˆ›å»ºæ–‡æ¡£è®°å½•
            document = Document(
                id=file_id,
                title=title or filename,
                description=description or "",
                filename=filename,
                file_path=str(file_path),
                file_size=len(file_content),
                status="processing"
            )
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if db:
                db.add(document)
                db.commit()
                db.refresh(document)
            
            # 4. è§£ææ–‡æ¡£å†…å®¹
            parser = self.supported_types[file_ext]
            text_content = await parser.parse(file_path)
            
            if not text_content.strip():
                raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–è§£æå¤±è´¥")
            
            # 5. æ–‡æœ¬åˆ†å—å¤„ç†
            chunks = await self._split_text_into_chunks(
                text_content, 
                document.id, 
                db
            )
            
            # 6. æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document.status = "completed"
            document.chunk_count = len(chunks)
            
            if db:
                db.commit()
            
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}, ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
            return document
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
            if db and 'document' in locals():
                document.status = "failed"
                db.commit()
            raise
    
    async def _split_text_into_chunks(
        self, 
        text: str, 
        document_id: str, 
        db: Session
    ) -> List[Chunk]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå—
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            document_id: æ–‡æ¡£ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            List[Chunk]: æ–‡æœ¬å—åˆ—è¡¨
        """
        # TODO(lab01-task1): å®ç°æ–‡æœ¬åˆ†å—é€»è¾‘
        # ä½¿ç”¨TextSplitterè¿›è¡Œæ™ºèƒ½åˆ†å—
        text_chunks = self.text_splitter.split_text(text)
        
        chunks = []
        for i, chunk_text in enumerate(text_chunks):
            chunk = Chunk(
                id=str(uuid.uuid4()),
                document_id=document_id,
                content=chunk_text,
                chunk_index=i,
                start_char=text.find(chunk_text),
                end_char=text.find(chunk_text) + len(chunk_text),
                token_count=len(chunk_text.split())  # ç®€å•çš„tokenè®¡æ•°
            )
            chunks.append(chunk)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if db:
                db.add(chunk)
        
        if db:
            db.commit()
        
        return chunks
    
    def get_supported_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
        return list(self.supported_types.keys())
    
    async def delete_document(self, document_id: str, db: Session) -> bool:
        """
        åˆ é™¤æ–‡æ¡£åŠå…¶ç›¸å…³æ–‡ä»¶
        
        Args:
            document_id: æ–‡æ¡£ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            # æŸ¥æ‰¾æ–‡æ¡£
            document = db.query(Document).filter(Document.id == document_id).first()
            if not document:
                return False
            
            # åˆ é™¤ç‰©ç†æ–‡ä»¶
            if document.file_path and os.path.exists(document.file_path):
                os.remove(document.file_path)
            
            # åˆ é™¤æ•°æ®åº“è®°å½•ï¼ˆçº§è”åˆ é™¤chunksï¼‰
            db.delete(document)
            db.commit()
            
            logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ é™¤å¤±è´¥: {document_id}, é”™è¯¯: {str(e)}")
            return False
```

#### 1.2 æ–‡æœ¬åˆ†å‰²å™¨ (`src/services/text_splitter.py`)

```python
import re
from typing import List, Optional
from ..utils.logger import get_logger

logger = get_logger(__name__)

class TextSplitter:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨ - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
    
    def __init__(
        self, 
        chunk_size: int = 1000, 
        chunk_overlap: int = 200,
        separators: Optional[List[str]] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # é»˜è®¤åˆ†éš”ç¬¦ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        self.separators = separators or [
            "\n\n",  # æ®µè½åˆ†éš”
            "\n",    # è¡Œåˆ†éš”
            "ã€‚",    # ä¸­æ–‡å¥å·
            "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
            "ï¼Ÿ",    # ä¸­æ–‡é—®å·
            ".",     # è‹±æ–‡å¥å·
            "!",     # è‹±æ–‡æ„Ÿå¹å·
            "?",     # è‹±æ–‡é—®å·
            ";",     # åˆ†å·
            ",",     # é€—å·
            " ",     # ç©ºæ ¼
            ""       # å­—ç¬¦çº§åˆ«åˆ†å‰²
        ]
    
    def split_text(self, text: str) -> List[str]:
        """
        æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        
        Args:
            text: å¾…åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        # TODO(lab01-task1): å®ç°æ™ºèƒ½æ–‡æœ¬åˆ†å‰²ç®—æ³•
        if not text.strip():
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = self._clean_text(text)
        
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºchunk_sizeï¼Œç›´æ¥è¿”å›
        if len(text) <= self.chunk_size:
            return [text]
        
        # é€’å½’åˆ†å‰²
        chunks = self._split_text_recursive(text, self.separators)
        
        # åˆå¹¶è¿‡å°çš„å—
        chunks = self._merge_small_chunks(chunks)
        
        logger.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆ: åŸé•¿åº¦ {len(text)}, åˆ†å‰²ä¸º {len(chunks)} å—")
        return chunks
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºç™½å­—ç¬¦"""
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        return text
    
    def _split_text_recursive(
        self, 
        text: str, 
        separators: List[str]
    ) -> List[str]:
        """é€’å½’åˆ†å‰²æ–‡æœ¬"""
        if not separators:
            # æ²¡æœ‰åˆ†éš”ç¬¦æ—¶ï¼Œå¼ºåˆ¶æŒ‰å­—ç¬¦åˆ†å‰²
            return [text[i:i+self.chunk_size] 
                   for i in range(0, len(text), self.chunk_size)]
        
        separator = separators[0]
        remaining_separators = separators[1:]
        
        if separator == "":
            # å­—ç¬¦çº§åˆ«åˆ†å‰²
            return [text[i:i+self.chunk_size] 
                   for i in range(0, len(text), self.chunk_size)]
        
        # æŒ‰å½“å‰åˆ†éš”ç¬¦åˆ†å‰²
        splits = text.split(separator)
        
        # é‡æ–°ç»„åˆåˆ†éš”ç¬¦
        if len(splits) > 1:
            splits = [splits[0]] + [separator + s for s in splits[1:]]
        
        chunks = []
        current_chunk = ""
        
        for split in splits:
            # å¦‚æœå•ä¸ªsplitå°±è¶…è¿‡chunk_sizeï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if len(split) > self.chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # é€’å½’åˆ†å‰²å¤§çš„split
                sub_chunks = self._split_text_recursive(split, remaining_separators)
                chunks.extend(sub_chunks)
            else:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ åˆ°å½“å‰chunk
                if len(current_chunk) + len(split) <= self.chunk_size:
                    current_chunk += split
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = split
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _merge_small_chunks(self, chunks: List[str]) -> List[str]:
        """åˆå¹¶è¿‡å°çš„æ–‡æœ¬å—"""
        if not chunks:
            return chunks
        
        merged_chunks = []
        current_chunk = chunks[0]
        
        for i in range(1, len(chunks)):
            next_chunk = chunks[i]
            
            # å¦‚æœå½“å‰å—å¤ªå°ï¼Œå°è¯•ä¸ä¸‹ä¸€å—åˆå¹¶
            if (len(current_chunk) < self.chunk_size // 2 and 
                len(current_chunk) + len(next_chunk) <= self.chunk_size):
                current_chunk += " " + next_chunk
            else:
                merged_chunks.append(current_chunk)
                current_chunk = next_chunk
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        merged_chunks.append(current_chunk)
        
        return merged_chunks
```

### ğŸ§ª æµ‹è¯•éªŒè¯

#### 1.3 æµ‹è¯•ç”¨ä¾‹

```python
# test_document_processor.py
import pytest
import tempfile
from pathlib import Path
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.services.document_processor import DocumentProcessor
from src.models.database import Base

@pytest.fixture
def db_session():
    """åˆ›å»ºæµ‹è¯•æ•°æ®åº“ä¼šè¯"""
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)
    SessionLocal = sessionmaker(bind=engine)
    session = SessionLocal()
    yield session
    session.close()

@pytest.fixture
def document_processor():
    """åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨å®ä¾‹"""
    with tempfile.TemporaryDirectory() as temp_dir:
        yield DocumentProcessor(upload_dir=temp_dir)

@pytest.mark.asyncio
async def test_process_pdf_document(document_processor, db_session):
    """æµ‹è¯•PDFæ–‡æ¡£å¤„ç†"""
    # åˆ›å»ºæµ‹è¯•PDFå†…å®¹
    test_content = b"Test PDF content"
    filename = "test.pdf"
    
    # å¤„ç†æ–‡æ¡£
    document = await document_processor.process_document(
        file_content=test_content,
        filename=filename,
        title="æµ‹è¯•æ–‡æ¡£",
        description="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
        db=db_session
    )
    
    # éªŒè¯ç»“æœ
    assert document.title == "æµ‹è¯•æ–‡æ¡£"
    assert document.filename == filename
    assert document.status == "completed"
    assert document.chunk_count > 0

@pytest.mark.asyncio
async def test_text_splitting(document_processor):
    """æµ‹è¯•æ–‡æœ¬åˆ†å‰²åŠŸèƒ½"""
    long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬ã€‚" * 100
    
    chunks = document_processor.text_splitter.split_text(long_text)
    
    # éªŒè¯åˆ†å‰²ç»“æœ
    assert len(chunks) > 1
    for chunk in chunks:
        assert len(chunk) <= document_processor.text_splitter.chunk_size
```

### ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹è§£æ

1. **æ–‡ä»¶ç±»å‹éªŒè¯**ï¼šé€šè¿‡æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹ï¼Œç¡®ä¿åªå¤„ç†æ”¯æŒçš„æ ¼å¼
2. **æ–‡ä»¶å¤§å°é™åˆ¶**ï¼šé˜²æ­¢è¿‡å¤§æ–‡ä»¶å½±å“ç³»ç»Ÿæ€§èƒ½
3. **æ™ºèƒ½æ–‡æœ¬åˆ†å—**ï¼šæŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å‰²ï¼Œä¿æŒæ–‡æœ¬å®Œæ•´æ€§
4. **å¼‚å¸¸å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç®¡ç†
5. **æ•°æ®åº“äº‹åŠ¡**ï¼šç¡®ä¿æ•°æ®ä¸€è‡´æ€§

### ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

**Q1: PDFè§£æå¤±è´¥æ€ä¹ˆåŠï¼Ÿ**
- æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦æŸå
- ç¡®è®¤PDFä¸æ˜¯æ‰«æç‰ˆï¼ˆéœ€è¦OCRï¼‰
- å°è¯•ä½¿ç”¨å…¶ä»–PDFè§£æåº“

**Q2: æ–‡æœ¬åˆ†å—æ•ˆæœä¸å¥½ï¼Ÿ**
- è°ƒæ•´chunk_sizeå’Œchunk_overlapå‚æ•°
- ä¼˜åŒ–åˆ†éš”ç¬¦ä¼˜å…ˆçº§
- é’ˆå¯¹ç‰¹å®šæ–‡æ¡£ç±»å‹å®šåˆ¶åˆ†å‰²ç­–ç•¥

---

## ğŸ”¢ ä»»åŠ¡2ï¼šæ–‡æ¡£å‘é‡åŒ–

### ğŸ¯ ä»»åŠ¡ç›®æ ‡

å°†æ–‡æ¡£æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œæ”¯æŒè¯­ä¹‰æœç´¢å’Œç›¸ä¼¼åº¦è®¡ç®—ã€‚

### ğŸ”§ æ ¸å¿ƒå®ç°

#### 2.1 åµŒå…¥æœåŠ¡ (`src/services/embedding_service.py`)

```python
import asyncio
import numpy as np
from typing import List, Optional, Union
import logging
from sentence_transformers import SentenceTransformer
import torch
from concurrent.futures import ThreadPoolExecutor

from ..config.settings import get_settings
from ..utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()

class EmbeddingService:
    """åµŒå…¥å‘é‡ç”ŸæˆæœåŠ¡"""
    
    def __init__(self):
        self.model = None
        self.model_name = settings.LOCAL_EMBEDDING_MODEL
        self.dimension = settings.LOCAL_EMBEDDING_DIMENSION
        self.batch_size = getattr(settings, 'EMBEDDING_BATCH_SIZE', 32)
        self.max_length = getattr(settings, 'MAX_TEXT_LENGTH', 512)
        self.executor = ThreadPoolExecutor(max_workers=4)
        
    async def initialize(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        if self.model is None:
            try:
                logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
                
                # TODO(lab01-task2): å®ç°æ¨¡å‹åˆå§‹åŒ–é€»è¾‘
                # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
                loop = asyncio.get_event_loop()
                self.model = await loop.run_in_executor(
                    self.executor, 
                    self._load_model
                )
                
                logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ: {self.model_name}")
                
                # éªŒè¯æ¨¡å‹ç»´åº¦
                test_embedding = await self.embed_text("æµ‹è¯•æ–‡æœ¬")
                actual_dim = len(test_embedding)
                
                if actual_dim != self.dimension:
                    logger.warning(
                        f"æ¨¡å‹ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.dimension}, å®é™… {actual_dim}"
                    )
                    self.dimension = actual_dim
                
            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                raise
    
    def _load_model(self) -> SentenceTransformer:
        """åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹"""
        # è®¾ç½®è®¾å¤‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        model = SentenceTransformer(self.model_name, device=device)
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        return model
    
    async def embed_text(self, text: str) -> List[float]:
        """
        ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[float]: åµŒå…¥å‘é‡
        """
        if not self.model:
            await self.initialize()
        
        # TODO(lab01-task2): å®ç°å•æ–‡æœ¬å‘é‡åŒ–
        try:
            # æ–‡æœ¬é¢„å¤„ç†
            processed_text = self._preprocess_text(text)
            
            # åœ¨çº¿ç¨‹æ± ä¸­ç”ŸæˆåµŒå…¥
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                self.executor,
                self._generate_embedding,
                processed_text
            )
            
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not self.model:
            await self.initialize()
        
        if not texts:
            return []
        
        # TODO(lab01-task2): å®ç°æ‰¹é‡å‘é‡åŒ–
        try:
            # æ–‡æœ¬é¢„å¤„ç†
            processed_texts = [self._preprocess_text(text) for text in texts]
            
            # åˆ†æ‰¹å¤„ç†
            all_embeddings = []
            for i in range(0, len(processed_texts), self.batch_size):
                batch_texts = processed_texts[i:i + self.batch_size]
                
                # åœ¨çº¿ç¨‹æ± ä¸­ç”ŸæˆåµŒå…¥
                loop = asyncio.get_event_loop()
                batch_embeddings = await loop.run_in_executor(
                    self.executor,
                    self._generate_batch_embeddings,
                    batch_texts
                )
                
                all_embeddings.extend(batch_embeddings)
                
                # è®°å½•è¿›åº¦
                logger.info(f"æ‰¹é‡å‘é‡åŒ–è¿›åº¦: {min(i + self.batch_size, len(texts))}/{len(texts)}")
            
            return [embedding.tolist() for embedding in all_embeddings]
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text or not text.strip():
            return ""
        
        # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = ' '.join(text.split())
        
        # æˆªæ–­è¿‡é•¿æ–‡æœ¬
        if len(text) > self.max_length:
            text = text[:self.max_length]
            logger.warning(f"æ–‡æœ¬è¢«æˆªæ–­åˆ° {self.max_length} å­—ç¬¦")
        
        return text
    
    def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        with torch.no_grad():
            embedding = self.model.encode(text, convert_to_numpy=True)
            return embedding
    
    def _generate_batch_embeddings(self, texts: List[str]) -> List[np.ndarray]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
        with torch.no_grad():
            embeddings = self.model.encode(texts, convert_to_numpy=True)
            return [embedding for embedding in embeddings]
    
    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension
    
    def is_initialized(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–"""
        return self.model is not None
```

#### 2.2 å‘é‡æœåŠ¡ (`src/services/vector_service.py`)

```python
import asyncio
from typing import List, Optional, Dict, Any
from sqlalchemy.orm import Session

from ..models.chunk import Chunk
from ..services.embedding_service import EmbeddingService
from ..services.vector_store import VectorStore
from ..utils.logger import get_logger

logger = get_logger(__name__)

class VectorService:
    """å‘é‡åŒ–æœåŠ¡ - ç®¡ç†æ–‡æ¡£å‘é‡åŒ–å’Œå­˜å‚¨"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore()
        
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        await self.embedding_service.initialize()
        await self.vector_store.initialize()
    
    async def vectorize_document(
        self, 
        document_id: str, 
        db: Session
    ) -> bool:
        """
        å‘é‡åŒ–æ–‡æ¡£çš„æ‰€æœ‰æ–‡æœ¬å—
        
        Args:
            document_id: æ–‡æ¡£ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            bool: å‘é‡åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            # TODO(lab01-task2): å®ç°æ–‡æ¡£å‘é‡åŒ–é€»è¾‘
            # 1. è·å–æ–‡æ¡£çš„æ‰€æœ‰æ–‡æœ¬å—
            chunks = db.query(Chunk).filter(
                Chunk.document_id == document_id,
                Chunk.is_vector_stored == False
            ).all()
            
            if not chunks:
                logger.info(f"æ–‡æ¡£ {document_id} æ²¡æœ‰éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬å—")
                return True
            
            # 2. æå–æ–‡æœ¬å†…å®¹
            texts = [chunk.content for chunk in chunks]
            chunk_ids = [chunk.id for chunk in chunks]
            
            logger.info(f"å¼€å§‹å‘é‡åŒ–æ–‡æ¡£ {document_id}, å…± {len(texts)} ä¸ªæ–‡æœ¬å—")
            
            # 3. æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = await self.embedding_service.embed_texts(texts)
            
            if len(embeddings) != len(chunks):
                raise ValueError(f"å‘é‡æ•°é‡ä¸åŒ¹é…: {len(embeddings)} vs {len(chunks)}")
            
            # 4. å­˜å‚¨å‘é‡åˆ°å‘é‡æ•°æ®åº“
            success = await self.vector_store.add_vectors(
                vectors=embeddings,
                chunk_ids=chunk_ids,
                metadata=[{
                    'document_id': chunk.document_id,
                    'chunk_index': chunk.chunk_index,
                    'content': chunk.content[:200]  # å­˜å‚¨å‰200å­—ç¬¦ä½œä¸ºå…ƒæ•°æ®
                } for chunk in chunks]
            )
            
            if not success:
                raise RuntimeError("å‘é‡å­˜å‚¨å¤±è´¥")
            
            # 5. æ›´æ–°æ•°æ®åº“ä¸­çš„å‘é‡åŒ–çŠ¶æ€
            for chunk in chunks:
                chunk.is_vector_stored = True
                chunk.vector_dimension = self.embedding_service.get_dimension()
            
            db.commit()
            
            logger.info(f"æ–‡æ¡£ {document_id} å‘é‡åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {document_id}, é”™è¯¯: {str(e)}")
            db.rollback()
            return False
    
    async def vectorize_text(self, text: str) -> Optional[List[float]]:
        """
        å‘é‡åŒ–å•ä¸ªæ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            Optional[List[float]]: åµŒå…¥å‘é‡ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # TODO(lab01-task2): å®ç°å•æ–‡æœ¬å‘é‡åŒ–
            if not text.strip():
                return None
            
            embedding = await self.embedding_service.embed_text(text)
            return embedding
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {str(e)}")
            return None
    
    async def search_similar_chunks(
        self, 
        query_text: str, 
        top_k: int = 5,
        similarity_threshold: float = 0.7,
        document_ids: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼çš„æ–‡æœ¬å—
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            document_ids: é™åˆ¶æœç´¢çš„æ–‡æ¡£IDåˆ—è¡¨
            
        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # TODO(lab01-task2): å®ç°ç›¸ä¼¼åº¦æœç´¢
            # 1. å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
            query_embedding = await self.vectorize_text(query_text)
            if not query_embedding:
                return []
            
            # 2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            search_results = await self.vector_store.search(
                query_vector=query_embedding,
                top_k=top_k,
                similarity_threshold=similarity_threshold,
                filter_metadata={'document_id': document_ids} if document_ids else None
            )
            
            return search_results
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def delete_document_vectors(self, document_id: str, db: Session) -> bool:
        """
        åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡
        
        Args:
            document_id: æ–‡æ¡£ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            # è·å–æ–‡æ¡£çš„æ‰€æœ‰chunk ID
            chunks = db.query(Chunk).filter(
                Chunk.document_id == document_id
            ).all()
            
            chunk_ids = [chunk.id for chunk in chunks]
            
            # ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤
            success = await self.vector_store.delete_vectors(chunk_ids)
            
            if success:
                # æ›´æ–°æ•°æ®åº“çŠ¶æ€
                for chunk in chunks:
                    chunk.is_vector_stored = False
                db.commit()
            
            return success
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {document_id}, é”™è¯¯: {str(e)}")
            return False
    
    async def get_vector_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = await self.vector_store.get_collection_info()
            return {
                'total_vectors': stats.get('vectors_count', 0),
                'dimension': self.embedding_service.get_dimension(),
                'model_name': self.embedding_service.model_name,
                'is_initialized': self.embedding_service.is_initialized()
            }
        except Exception as e:
            logger.error(f"è·å–å‘é‡ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
```

### ğŸ§ª æµ‹è¯•éªŒè¯

#### 2.3 æµ‹è¯•ç”¨ä¾‹

```python
# test_embedding_service.py
import pytest
import asyncio

from src.services.embedding_service import EmbeddingService

@pytest.fixture
async def embedding_service():
    """åˆ›å»ºåµŒå…¥æœåŠ¡å®ä¾‹"""
    service = EmbeddingService()
    await service.initialize()
    return service

@pytest.mark.asyncio
async def test_embed_single_text(embedding_service):
    """æµ‹è¯•å•æ–‡æœ¬å‘é‡åŒ–"""
    text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
    
    embedding = await embedding_service.embed_text(text)
    
    # éªŒè¯ç»“æœ
    assert isinstance(embedding, list)
    assert len(embedding) == embedding_service.dimension
    assert all(isinstance(x, float) for x in embedding)

@pytest.mark.asyncio
async def test_embed_batch_texts(embedding_service):
    """æµ‹è¯•æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–"""
    texts = [
        "ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
        "ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æœ¬",
        "ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æœ¬"
    ]
    
    embeddings = await embedding_service.embed_texts(texts)
    
    # éªŒè¯ç»“æœ
    assert len(embeddings) == len(texts)
    for embedding in embeddings:
        assert len(embedding) == embedding_service.dimension

@pytest.mark.asyncio
async def test_similarity_calculation():
    """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
    service = EmbeddingService()
    await service.initialize()
    
    # ç›¸ä¼¼æ–‡æœ¬
    text1 = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯"
    text2 = "AIæ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦é¢†åŸŸ"
    
    # ä¸ç›¸ä¼¼æ–‡æœ¬
    text3 = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    
    embedding1 = await service.embed_text(text1)
    embedding2 = await service.embed_text(text2)
    embedding3 = await service.embed_text(text3)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    import numpy as np
    
    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    sim_12 = cosine_similarity(embedding1, embedding2)
    sim_13 = cosine_similarity(embedding1, embedding3)
    
    # ç›¸ä¼¼æ–‡æœ¬çš„ç›¸ä¼¼åº¦åº”è¯¥æ›´é«˜
    assert sim_12 > sim_13
    assert sim_12 > 0.5  # ç›¸ä¼¼æ–‡æœ¬ç›¸ä¼¼åº¦åº”è¯¥è¾ƒé«˜
```

### ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹è§£æ

1. **å¼‚æ­¥å¤„ç†**ï¼šä½¿ç”¨çº¿ç¨‹æ± é¿å…æ¨¡å‹æ¨ç†é˜»å¡ä¸»çº¿ç¨‹
2. **æ‰¹é‡å¤„ç†**ï¼šæé«˜å‘é‡åŒ–æ•ˆç‡ï¼Œå‡å°‘æ¨¡å‹è°ƒç”¨æ¬¡æ•°
3. **å†…å­˜ç®¡ç†**ï¼šåˆç†æ§åˆ¶æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
4. **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ¨¡å‹é¢„çƒ­ã€è®¾å¤‡é€‰æ‹©ã€æ¨ç†ä¼˜åŒ–

### ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

**Q1: æ¨¡å‹åŠ è½½æ…¢æ€ä¹ˆåŠï¼Ÿ**
- ä½¿ç”¨æ¨¡å‹ç¼“å­˜
- é€‰æ‹©æ›´å°çš„æ¨¡å‹
- é¢„çƒ­æ¨¡å‹

**Q2: å‘é‡åŒ–å†…å­˜ä¸è¶³ï¼Ÿ**
- å‡å°æ‰¹å¤„ç†å¤§å°
- ä½¿ç”¨CPUæ¨ç†
- æ¸…ç†ä¸å¿…è¦çš„å˜é‡

---

## ğŸ” ä»»åŠ¡3ï¼šè¯­ä¹‰æœç´¢å®ç°

### ğŸ¯ ä»»åŠ¡ç›®æ ‡

å®ç°åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰æœç´¢åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥å’Œç»“æœæ’åºã€‚

### ğŸ”§ æ ¸å¿ƒå®ç°

#### 3.1 å‘é‡å­˜å‚¨æœåŠ¡ (`src/services/vector_store.py`)

```python
import asyncio
from typing import List, Dict, Any, Optional
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams, PointStruct

from ..config.settings import get_settings
from ..utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()

class VectorStore:
    """å‘é‡å­˜å‚¨æœåŠ¡ - åŸºäºQdrantå®ç°"""
    
    def __init__(self):
        self.client = None
        self.collection_name = "documents"
        self.qdrant_url = settings.QDRANT_URL
        
    async def initialize(self):
        """åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯å’Œé›†åˆ"""
        try:
            # TODO(lab01-task3): å®ç°å‘é‡æ•°æ®åº“åˆå§‹åŒ–
            # 1. åˆ›å»ºQdrantå®¢æˆ·ç«¯
            self.client = QdrantClient(url=self.qdrant_url)
            
            # 2. æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.collection_name not in collection_names:
                # 3. åˆ›å»ºé›†åˆ
                await self._create_collection()
                logger.info(f"åˆ›å»ºå‘é‡é›†åˆ: {self.collection_name}")
            else:
                logger.info(f"å‘é‡é›†åˆå·²å­˜åœ¨: {self.collection_name}")
                
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def _create_collection(self):
        """åˆ›å»ºå‘é‡é›†åˆ"""
        # TODO(lab01-task3): å®ç°é›†åˆåˆ›å»ºé€»è¾‘
        # è·å–å‘é‡ç»´åº¦ï¼ˆä»é…ç½®æˆ–é»˜è®¤å€¼ï¼‰
        dimension = getattr(settings, 'LOCAL_EMBEDDING_DIMENSION', 384)
        
        # åˆ›å»ºé›†åˆé…ç½®
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=dimension,
                distance=Distance.COSINE  # ä½¿ç”¨ä½™å¼¦è·ç¦»
            )
        )
    
    async def add_vectors(
        self, 
        vectors: List[List[float]], 
        chunk_ids: List[str],
        metadata: List[Dict[str, Any]]
    ) -> bool:
        """
        æ·»åŠ å‘é‡åˆ°å­˜å‚¨
        
        Args:
            vectors: å‘é‡åˆ—è¡¨
            chunk_ids: å¯¹åº”çš„chunk IDåˆ—è¡¨
            metadata: å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        try:
            # TODO(lab01-task3): å®ç°å‘é‡æ·»åŠ é€»è¾‘
            if len(vectors) != len(chunk_ids) or len(vectors) != len(metadata):
                raise ValueError("å‘é‡ã€IDå’Œå…ƒæ•°æ®æ•°é‡ä¸åŒ¹é…")
            
            # æ„å»ºç‚¹æ•°æ®
            points = []
            for i, (vector, chunk_id, meta) in enumerate(zip(vectors, chunk_ids, metadata)):
                point = PointStruct(
                    id=chunk_id,  # ä½¿ç”¨chunk_idä½œä¸ºç‚¹ID
                    vector=vector,
                    payload=meta  # å­˜å‚¨å…ƒæ•°æ®
                )
                points.append(point)
            
            # æ‰¹é‡æ’å…¥å‘é‡
            operation_info = self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            logger.info(f"æˆåŠŸæ·»åŠ  {len(points)} ä¸ªå‘é‡åˆ°é›†åˆ")
            return True
            
        except Exception as e:
            logger.error(f"å‘é‡æ·»åŠ å¤±è´¥: {str(e)}")
            return False
    
    async def search(
        self, 
        query_vector: List[float], 
        top_k: int = 5,
        similarity_threshold: float = 0.7,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            filter_metadata: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœ
        """
        try:
            # TODO(lab01-task3): å®ç°å‘é‡æœç´¢é€»è¾‘
            # æ„å»ºæœç´¢è¿‡æ»¤å™¨
            search_filter = None
            if filter_metadata:
                conditions = []
                for key, value in filter_metadata.items():
                    if isinstance(value, list):
                        # å¤šå€¼åŒ¹é…
                        conditions.append(
                            models.FieldCondition(
                                key=key,
                                match=models.MatchAny(any=value)
                            )
                        )
                    else:
                        # å•å€¼åŒ¹é…
                        conditions.append(
                            models.FieldCondition(
                                key=key,
                                match=models.MatchValue(value=value)
                            )
                        )
                
                if conditions:
                    search_filter = models.Filter(must=conditions)
            
            # æ‰§è¡Œæœç´¢
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=top_k,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False  # ä¸è¿”å›å‘é‡æ•°æ®ï¼ŒèŠ‚çœå¸¦å®½
            )
            
            # å¤„ç†æœç´¢ç»“æœ
            results = []
            for result in search_results:
                # æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
                if result.score < similarity_threshold:
                    continue
                
                result_dict = {
                    'chunk_id': result.id,
                    'similarity_score': result.score,
                    'metadata': result.payload
                }
                results.append(result_dict)
            
            logger.info(f"æœç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def delete_vectors(self, chunk_ids: List[str]) -> bool:
        """
        åˆ é™¤æŒ‡å®šçš„å‘é‡
        
        Args:
            chunk_ids: è¦åˆ é™¤çš„chunk IDåˆ—è¡¨
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            # TODO(lab01-task3): å®ç°å‘é‡åˆ é™¤é€»è¾‘
            if not chunk_ids:
                return True
            
            # æ‰¹é‡åˆ é™¤å‘é‡
            operation_info = self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.PointIdsList(
                    points=chunk_ids
                )
            )
            
            logger.info(f"æˆåŠŸåˆ é™¤ {len(chunk_ids)} ä¸ªå‘é‡")
            return True
            
        except Exception as e:
            logger.error(f"å‘é‡åˆ é™¤å¤±è´¥: {str(e)}")
            return False
    
    async def get_collection_info(self) -> Dict[str, Any]:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            # TODO(lab01-task3): å®ç°é›†åˆä¿¡æ¯è·å–
            collection_info = self.client.get_collection(self.collection_name)
            
            return {
                'vectors_count': collection_info.vectors_count,
                'indexed_vectors_count': collection_info.indexed_vectors_count,
                'points_count': collection_info.points_count,
                'status': collection_info.status,
                'optimizer_status': collection_info.optimizer_status
            }
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥
            collections = self.client.get_collections()
            return True
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
```

#### 3.2 æœç´¢APIè·¯ç”± (`src/api/routes/vectors.py`)

```python
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import List, Optional
from sqlalchemy.orm import Session

from ...api.dependencies import get_db
from ...api.schemas import SearchRequest, SearchResponse, SearchResult
from ...services.vector_service import VectorService
from ...models.chunk import Chunk
from ...utils.logger import get_logger

logger = get_logger(__name__)
router = APIRouter()

# å…¨å±€å‘é‡æœåŠ¡å®ä¾‹
vector_service = VectorService()

@router.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–å‘é‡æœåŠ¡"""
    await vector_service.initialize()

@router.post("/search", response_model=SearchResponse)
async def search_vectors(
    request: SearchRequest,
    db: Session = Depends(get_db)
):
    """
    å‘é‡è¯­ä¹‰æœç´¢
    
    Args:
        request: æœç´¢è¯·æ±‚
        db: æ•°æ®åº“ä¼šè¯
        
    Returns:
        SearchResponse: æœç´¢ç»“æœ
    """
    try:
        # TODO(lab01-task3): å®ç°æœç´¢APIé€»è¾‘
        # 1. å‚æ•°éªŒè¯
        if not request.query.strip():
            raise HTTPException(status_code=400, detail="æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        if request.top_k <= 0 or request.top_k > 50:
            raise HTTPException(status_code=400, detail="top_kå¿…é¡»åœ¨1-50ä¹‹é—´")
        
        # 2. æ‰§è¡Œå‘é‡æœç´¢
        search_results = await vector_service.search_similar_chunks(
            query_text=request.query,
            top_k=request.top_k,
            similarity_threshold=request.similarity_threshold,
            document_ids=request.document_ids
        )
        
        # 3. è·å–è¯¦ç»†çš„chunkä¿¡æ¯
        results = []
        for result in search_results:
            chunk_id = result['chunk_id']
            
            # ä»æ•°æ®åº“è·å–å®Œæ•´çš„chunkä¿¡æ¯
            chunk = db.query(Chunk).filter(Chunk.id == chunk_id).first()
            if not chunk:
                continue
            
            search_result = SearchResult(
                chunk_id=chunk_id,
                document_id=chunk.document_id,
                content=chunk.content,
                similarity_score=result['similarity_score'],
                metadata={
                    'chunk_index': chunk.chunk_index,
                    'token_count': chunk.token_count,
                    'start_char': chunk.start_char,
                    'end_char': chunk.end_char
                }
            )
            results.append(search_result)
        
        # 4. æ„å»ºå“åº”
        response = SearchResponse(
            results=results,
            total_results=len(results),
            processing_time=0.0  # å¯ä»¥æ·»åŠ å®é™…çš„å¤„ç†æ—¶é—´ç»Ÿè®¡
        )
        
        logger.info(f"æœç´¢å®Œæˆ: æŸ¥è¯¢='{request.query}', ç»“æœæ•°={len(results)}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="æœç´¢æœåŠ¡å†…éƒ¨é”™è¯¯")

@router.get("/search", response_model=SearchResponse)
async def search_vectors_get(
    query: str = Query(..., description="æœç´¢æŸ¥è¯¢æ–‡æœ¬"),
    top_k: int = Query(5, ge=1, le=50, description="è¿”å›ç»“æœæ•°é‡"),
    similarity_threshold: float = Query(0.7, ge=0.0, le=1.0, description="ç›¸ä¼¼åº¦é˜ˆå€¼"),
    document_ids: Optional[str] = Query(None, description="æ–‡æ¡£IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”"),
    db: Session = Depends(get_db)
):
    """
    GETæ–¹å¼çš„å‘é‡æœç´¢ï¼ˆä¾¿äºæµ‹è¯•ï¼‰
    """
    # TODO(lab01-task3): å®ç°GETæœç´¢æ¥å£
    # è§£ædocument_ids
    doc_ids = None
    if document_ids:
        doc_ids = [doc_id.strip() for doc_id in document_ids.split(',') if doc_id.strip()]
    
    # æ„å»ºæœç´¢è¯·æ±‚
    request = SearchRequest(
        query=query,
        top_k=top_k,
        similarity_threshold=similarity_threshold,
        document_ids=doc_ids
    )
    
    # è°ƒç”¨POSTæ¥å£é€»è¾‘
    return await search_vectors(request, db)

@router.post("/vectorize/{document_id}")
async def vectorize_document(
    document_id: str,
    db: Session = Depends(get_db)
):
    """
    æ‰‹åŠ¨è§¦å‘æ–‡æ¡£å‘é‡åŒ–
    
    Args:
        document_id: æ–‡æ¡£ID
        db: æ•°æ®åº“ä¼šè¯
        
    Returns:
        dict: å‘é‡åŒ–ç»“æœ
    """
    try:
        # TODO(lab01-task3): å®ç°æ‰‹åŠ¨å‘é‡åŒ–æ¥å£
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        from ...models.document import Document
        document = db.query(Document).filter(Document.id == document_id).first()
        if not document:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        # æ‰§è¡Œå‘é‡åŒ–
        success = await vector_service.vectorize_document(document_id, db)
        
        if success:
            return {
                "message": f"æ–‡æ¡£ {document_id} å‘é‡åŒ–æˆåŠŸ",
                "document_id": document_id,
                "status": "completed"
            }
        else:
            raise HTTPException(status_code=500, detail="å‘é‡åŒ–å¤±è´¥")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="å‘é‡åŒ–æœåŠ¡å†…éƒ¨é”™è¯¯")

@router.get("/stats")
async def get_vector_stats():
    """è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # TODO(lab01-task3): å®ç°ç»Ÿè®¡ä¿¡æ¯æ¥å£
        stats = await vector_service.get_vector_stats()
        return stats
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥")
```

### ğŸ§ª æµ‹è¯•éªŒè¯

#### 3.3 æµ‹è¯•ç”¨ä¾‹

```python
# test_vector_search.py
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.main import app
from src.models.database import Base
from src.api.dependencies import get_db

# åˆ›å»ºæµ‹è¯•æ•°æ®åº“
engine = create_engine("sqlite:///:memory:")
Base.metadata.create_all(engine)
TestingSessionLocal = sessionmaker(bind=engine)

def override_get_db():
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()

app.dependency_overrides[get_db] = override_get_db
client = TestClient(app)

def test_search_vectors():
    """æµ‹è¯•å‘é‡æœç´¢API"""
    # å‡†å¤‡æœç´¢è¯·æ±‚
    search_data = {
        "query": "äººå·¥æ™ºèƒ½",
        "top_k": 5,
        "similarity_threshold": 0.7
    }
    
    # å‘é€æœç´¢è¯·æ±‚
    response = client.post("/api/v1/search", json=search_data)
    
    # éªŒè¯å“åº”
    assert response.status_code == 200
    data = response.json()
    assert "results" in data
    assert "total_results" in data
    assert isinstance(data["results"], list)

def test_search_with_filters():
    """æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢"""
    search_data = {
        "query": "æœºå™¨å­¦ä¹ ",
        "top_k": 3,
        "similarity_threshold": 0.6,
        "document_ids": ["doc_123", "doc_456"]
    }
    
    response = client.post("/api/v1/search", json=search_data)
    assert response.status_code == 200

def test_get_search():
    """æµ‹è¯•GETæ–¹å¼æœç´¢"""
    response = client.get(
        "/api/v1/search",
        params={
            "query": "æ·±åº¦å­¦ä¹ ",
            "top_k": 3,
            "similarity_threshold": 0.8
        }
    )
    assert response.status_code == 200

def test_vector_stats():
    """æµ‹è¯•å‘é‡ç»Ÿè®¡ä¿¡æ¯"""
    response = client.get("/api/v1/stats")
    assert response.status_code == 200
    
    data = response.json()
    expected_keys = ["total_vectors", "dimension", "model_name", "is_initialized"]
    for key in expected_keys:
        assert key in data
```

### ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹è§£æ

1. **å‘é‡æ•°æ®åº“é€‰æ‹©**ï¼šQdrantæä¾›é«˜æ€§èƒ½çš„å‘é‡æœç´¢èƒ½åŠ›
2. **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šä½™å¼¦ç›¸ä¼¼åº¦é€‚åˆæ–‡æœ¬å‘é‡æ¯”è¾ƒ
3. **æœç´¢ä¼˜åŒ–**ï¼šæ”¯æŒè¿‡æ»¤æ¡ä»¶å’Œé˜ˆå€¼è®¾ç½®
4. **æ‰¹é‡æ“ä½œ**ï¼šæé«˜å‘é‡æ’å…¥å’Œåˆ é™¤æ•ˆç‡
5. **APIè®¾è®¡**ï¼šRESTfulæ¥å£ï¼Œæ”¯æŒå¤šç§æŸ¥è¯¢æ–¹å¼

### ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

**Q1: æœç´¢ç»“æœä¸å‡†ç¡®ï¼Ÿ**
- è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
- ä¼˜åŒ–æ–‡æœ¬é¢„å¤„ç†
- ä½¿ç”¨æ›´å¥½çš„åµŒå…¥æ¨¡å‹

**Q2: æœç´¢é€Ÿåº¦æ…¢ï¼Ÿ**
- å»ºç«‹å‘é‡ç´¢å¼•
- å‡å°‘è¿”å›ç»“æœæ•°é‡
- ä¼˜åŒ–å‘é‡ç»´åº¦

---

## ğŸ¤– ä»»åŠ¡4ï¼šRAGé—®ç­”ç³»ç»Ÿ

### ğŸ¯ ä»»åŠ¡ç›®æ ‡

å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é—®ç­”ç³»ç»Ÿï¼Œç»“åˆè¯­ä¹‰æœç´¢å’Œå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå‡†ç¡®å›ç­”ã€‚

### ğŸ”§ æ ¸å¿ƒå®ç°

#### 4.1 é—®ç­”æœåŠ¡ (`src/services/qa_service.py`)

```python
import asyncio
import time
from typing import List, Dict, Any, Optional
import httpx
from sqlalchemy.orm import Session

from ..config.settings import get_settings
from ..services.vector_service import VectorService
from ..models.chunk import Chunk
from ..utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()

class QAService:
    """é—®ç­”æœåŠ¡ - å®ç°RAGé—®ç­”é€»è¾‘"""
    
    def __init__(self):
        self.vector_service = VectorService()
        self.llm_client = None
        
        # LLMé…ç½®
        self.api_key = getattr(settings, 'VOLCENGINE_API_KEY', '')
        self.base_url = getattr(settings, 'VOLCENGINE_BASE_URL', '')
        self.model_name = getattr(settings, 'VOLCENGINE_MODEL', 'doubao-seed-1-6-250615')
        
        # é—®ç­”é…ç½®
        self.max_context_length = getattr(settings, 'MAX_CONTEXT_LENGTH', 4000)
        self.max_tokens = getattr(settings, 'MAX_TOKENS', 1000)
        self.temperature = getattr(settings, 'TEMPERATURE', 0.7)
        
    async def initialize(self):
        """åˆå§‹åŒ–é—®ç­”æœåŠ¡"""
        await self.vector_service.initialize()
        
        # åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯
        self.llm_client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            },
            timeout=30.0
        )
    
    async def answer_question(
        self,
        question: str,
        document_ids: Optional[List[str]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        db: Session = None
    ) -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document_ids: é™åˆ¶æœç´¢çš„æ–‡æ¡£IDåˆ—è¡¨
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            Dict[str, Any]: é—®ç­”ç»“æœ
        """
        start_time = time.time()
        
        try:
            # TODO(lab01-task4): å®ç°RAGé—®ç­”é€»è¾‘
            # 1. å‚æ•°éªŒè¯å’Œé¢„å¤„ç†
            if not question.strip():
                raise ValueError("é—®é¢˜ä¸èƒ½ä¸ºç©º")
            
            question = question.strip()
            max_tokens = max_tokens or self.max_tokens
            temperature = temperature or self.temperature
            
            logger.info(f"å¼€å§‹å¤„ç†é—®é¢˜: {question}")
            
            # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
            relevant_chunks = await self._retrieve_relevant_chunks(
                question, 
                document_ids,
                top_k=5,
                similarity_threshold=0.6
            )
            
            if not relevant_chunks:
                return {
                    'answer': 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚',
                    'sources': [],
                    'processing_time': time.time() - start_time,
                    'model_used': self.model_name
                }
            
            # 3. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(relevant_chunks)
            
            # 4. ç”Ÿæˆæç¤ºè¯
            prompt = self._build_prompt(question, context)
            
            # 5. è°ƒç”¨LLMç”Ÿæˆå›ç­”
            answer = await self._generate_answer(
                prompt, 
                max_tokens, 
                temperature
            )
            
            # 6. åå¤„ç†å’ŒéªŒè¯
            answer = self._post_process_answer(answer, question)
            
            # 7. æ„å»ºå“åº”
            processing_time = time.time() - start_time
            
            response = {
                'answer': answer,
                'sources': self._format_sources(relevant_chunks, db),
                'processing_time': processing_time,
                'model_used': self.model_name,
                'context_length': len(context),
                'chunks_used': len(relevant_chunks)
            }
            
            logger.info(f"é—®ç­”å®Œæˆ: è€—æ—¶ {processing_time:.2f}s")
            return response
            
        except Exception as e:
            logger.error(f"é—®ç­”å¤±è´¥: {str(e)}")
            return {
                'answer': f'æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯: {str(e)}',
                'sources': [],
                'processing_time': time.time() - start_time,
                'model_used': self.model_name,
                'error': str(e)
            }
    
    async def _retrieve_relevant_chunks(
        self,
        question: str,
        document_ids: Optional[List[str]] = None,
        top_k: int = 5,
        similarity_threshold: float = 0.6
    ) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ"""
        # TODO(lab01-task4): å®ç°æ–‡æ¡£æ£€ç´¢é€»è¾‘
        try:
            # ä½¿ç”¨å‘é‡æœåŠ¡æœç´¢ç›¸å…³ç‰‡æ®µ
            search_results = await self.vector_service.search_similar_chunks(
                query_text=question,
                top_k=top_k,
                similarity_threshold=similarity_threshold,
                document_ids=document_ids
            )
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            search_results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            return search_results
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _build_context(self, chunks: List[Dict[str, Any]]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        # TODO(lab01-task4): å®ç°ä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘
        if not chunks:
            return ""
        
        context_parts = []
        current_length = 0
        
        for i, chunk in enumerate(chunks):
            # è·å–chunkå†…å®¹
            content = chunk.get('metadata', {}).get('content', '')
            if not content:
                continue
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(content) > self.max_context_length:
                break
            
            # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            context_parts.append(f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{content}\n")
            current_length += len(content)
        
        return "\n".join(context_parts)
    
    def _build_prompt(self, question: str, context: str) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        # TODO(lab01-task4): å®ç°æç¤ºè¯æ„å»ºé€»è¾‘
        if not context:
            prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}

è¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚"""
        else:
            prompt = f"""è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ä»”ç»†é˜…è¯»æ–‡æ¡£å†…å®¹ï¼Œå¹¶åŸºäºæ–‡æ¡£ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†ã€‚"""
        
        return prompt
    
    async def _generate_answer(
        self, 
        prompt: str, 
        max_tokens: int, 
        temperature: float
    ) -> str:
        """è°ƒç”¨LLMç”Ÿæˆå›ç­”"""
        # TODO(lab01-task4): å®ç°LLMè°ƒç”¨é€»è¾‘
        try:
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": False
            }
            
            # å‘é€è¯·æ±‚
            response = await self.llm_client.post(
                "/v1/chat/completions",
                json=request_data
            )
            
            if response.status_code != 200:
                raise Exception(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            
            # è§£æå“åº”
            result = response.json()
            
            if 'choices' not in result or not result['choices']:
                raise Exception("LLMå“åº”æ ¼å¼é”™è¯¯")
            
            answer = result['choices'][0]['message']['content']
            return answer.strip()
            
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    def _post_process_answer(self, answer: str, question: str) -> str:
        """åå¤„ç†ç”Ÿæˆçš„å›ç­”"""
        # TODO(lab01-task4): å®ç°å›ç­”åå¤„ç†é€»è¾‘
        if not answer:
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚"
        
        # å»é™¤å¤šä½™ç©ºç™½
        answer = answer.strip()
        
        # ç¡®ä¿å›ç­”å®Œæ•´ï¼ˆä¸ä»¥å¥å·ç»“å°¾çš„æ·»åŠ å¥å·ï¼‰
        if answer and not answer.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
            answer += 'ã€‚'
        
        return answer
    
    def _format_sources(
        self, 
        chunks: List[Dict[str, Any]], 
        db: Session
    ) -> List[Dict[str, Any]]:
        """æ ¼å¼åŒ–å¼•ç”¨æ¥æº"""
        # TODO(lab01-task4): å®ç°æ¥æºæ ¼å¼åŒ–é€»è¾‘
        sources = []
        
        for chunk in chunks:
            chunk_id = chunk.get('chunk_id')
            if not chunk_id or not db:
                continue
            
            # ä»æ•°æ®åº“è·å–è¯¦ç»†ä¿¡æ¯
            chunk_obj = db.query(Chunk).filter(Chunk.id == chunk_id).first()
            if not chunk_obj:
                continue
            
            # è·å–æ–‡æ¡£ä¿¡æ¯
            from ..models.document import Document
            document = db.query(Document).filter(
                Document.id == chunk_obj.document_id
            ).first()
            
            source = {
                'chunk_id': chunk_id,
                'document_id': chunk_obj.document_id,
                'document_title': document.title if document else 'æœªçŸ¥æ–‡æ¡£',
                'similarity_score': chunk.get('similarity_score', 0.0),
                'content_preview': chunk_obj.content[:200] + '...' if len(chunk_obj.content) > 200 else chunk_obj.content
            }
            sources.append(source)
        
        return sources
    
    async def batch_answer_questions(
        self,
        questions: List[str],
        document_ids: Optional[List[str]] = None,
        db: Session = None
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡å›ç­”é—®é¢˜"""
        # TODO(lab01-task4): å®ç°æ‰¹é‡é—®ç­”é€»è¾‘
        results = []
        
        for i, question in enumerate(questions):
            try:
                logger.info(f"å¤„ç†é—®é¢˜ {i+1}/{len(questions)}: {question}")
                
                result = await self.answer_question(
                    question=question,
                    document_ids=document_ids,
                    db=db
                )
                
                results.append({
                    'question': question,
                    'result': result,
                    'status': 'success'
                })
                
            except Exception as e:
                logger.error(f"æ‰¹é‡é—®ç­”å¤±è´¥: é—®é¢˜ {i+1}, é”™è¯¯: {str(e)}")
                results.append({
                    'question': question,
                    'result': {
                        'answer': f'å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}',
                        'sources': [],
                        'error': str(e)
                    },
                    'status': 'error'
                })
        
        return results
    
    async def check_answer_accuracy(
        self,
        question: str,
        answer: str,
        expected_keywords: List[str] = None
    ) -> Dict[str, Any]:
        """æ£€æŸ¥å›ç­”å‡†ç¡®æ€§"""
        # TODO(lab01-task4): å®ç°å‡†ç¡®æ€§æ£€æŸ¥é€»è¾‘
        accuracy_score = 0.0
        feedback = []
        
        # åŸºæœ¬æ£€æŸ¥
        if not answer or len(answer.strip()) < 10:
            feedback.append("å›ç­”è¿‡äºç®€çŸ­")
        else:
            accuracy_score += 0.3
        
        # å…³é”®è¯æ£€æŸ¥
        if expected_keywords:
            found_keywords = []
            answer_lower = answer.lower()
            
            for keyword in expected_keywords:
                if keyword.lower() in answer_lower:
                    found_keywords.append(keyword)
            
            keyword_score = len(found_keywords) / len(expected_keywords)
            accuracy_score += keyword_score * 0.7
            
            feedback.append(f"åŒ…å«å…³é”®è¯: {found_keywords}")
            if len(found_keywords) < len(expected_keywords):
                missing = set(expected_keywords) - set(found_keywords)
                feedback.append(f"ç¼ºå°‘å…³é”®è¯: {list(missing)}")
        
        return {
             'accuracy_score': min(accuracy_score, 1.0),
             'feedback': feedback,
             'answer_length': len(answer),
             'has_sources': 'sources' in answer.lower()
         }
```

#### 4.2 é—®ç­”APIè·¯ç”± (`src/api/routes/qa.py`)

```python
from fastapi import APIRouter, Depends, HTTPException
from typing import List, Optional
from sqlalchemy.orm import Session

from ...api.dependencies import get_db
from ...api.schemas import QuestionRequest, QuestionResponse, BatchQuestionRequest, BatchQuestionResponse
from ...services.qa_service import QAService
from ...utils.logger import get_logger

logger = get_logger(__name__)
router = APIRouter()

# å…¨å±€é—®ç­”æœåŠ¡å®ä¾‹
qa_service = QAService()

@router.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–é—®ç­”æœåŠ¡"""
    await qa_service.initialize()

@router.post("/ask", response_model=QuestionResponse)
async def ask_question(
    request: QuestionRequest,
    db: Session = Depends(get_db)
):
    """
    å•ä¸ªé—®é¢˜é—®ç­”
    
    Args:
        request: é—®é¢˜è¯·æ±‚
        db: æ•°æ®åº“ä¼šè¯
        
    Returns:
        QuestionResponse: é—®ç­”ç»“æœ
    """
    try:
        # TODO(lab01-task4): å®ç°é—®ç­”APIé€»è¾‘
        # å‚æ•°éªŒè¯
        if not request.question.strip():
            raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        # è°ƒç”¨é—®ç­”æœåŠ¡
        result = await qa_service.answer_question(
            question=request.question,
            document_ids=request.document_ids,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            db=db
        )
        
        # æ„å»ºå“åº”
        response = QuestionResponse(
            question=request.question,
            answer=result['answer'],
            sources=result.get('sources', []),
            processing_time=result.get('processing_time', 0.0),
            model_used=result.get('model_used', ''),
            context_length=result.get('context_length', 0),
            chunks_used=result.get('chunks_used', 0)
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é—®ç­”APIå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="é—®ç­”æœåŠ¡å†…éƒ¨é”™è¯¯")

@router.post("/batch-ask", response_model=BatchQuestionResponse)
async def batch_ask_questions(
    request: BatchQuestionRequest,
    db: Session = Depends(get_db)
):
    """
    æ‰¹é‡é—®é¢˜é—®ç­”
    
    Args:
        request: æ‰¹é‡é—®é¢˜è¯·æ±‚
        db: æ•°æ®åº“ä¼šè¯
        
    Returns:
        BatchQuestionResponse: æ‰¹é‡é—®ç­”ç»“æœ
    """
    try:
        # TODO(lab01-task4): å®ç°æ‰¹é‡é—®ç­”APIé€»è¾‘
        # å‚æ•°éªŒè¯
        if not request.questions:
            raise HTTPException(status_code=400, detail="é—®é¢˜åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(request.questions) > 10:
            raise HTTPException(status_code=400, detail="æ‰¹é‡é—®é¢˜æ•°é‡ä¸èƒ½è¶…è¿‡10ä¸ª")
        
        # è°ƒç”¨æ‰¹é‡é—®ç­”æœåŠ¡
        results = await qa_service.batch_answer_questions(
            questions=request.questions,
            document_ids=request.document_ids,
            db=db
        )
        
        # æ„å»ºå“åº”
        responses = []
        for result in results:
            if result['status'] == 'success':
                qa_result = result['result']
                response = QuestionResponse(
                    question=result['question'],
                    answer=qa_result['answer'],
                    sources=qa_result.get('sources', []),
                    processing_time=qa_result.get('processing_time', 0.0),
                    model_used=qa_result.get('model_used', ''),
                    context_length=qa_result.get('context_length', 0),
                    chunks_used=qa_result.get('chunks_used', 0)
                )
            else:
                response = QuestionResponse(
                    question=result['question'],
                    answer=result['result']['answer'],
                    sources=[],
                    processing_time=0.0,
                    model_used='',
                    error=result['result'].get('error', '')
                )
            responses.append(response)
        
        batch_response = BatchQuestionResponse(
            results=responses,
            total_questions=len(request.questions),
            successful_answers=sum(1 for r in results if r['status'] == 'success')
        )
        
        return batch_response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡é—®ç­”APIå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="æ‰¹é‡é—®ç­”æœåŠ¡å†…éƒ¨é”™è¯¯")
```

### ğŸ§ª æµ‹è¯•éªŒè¯

#### 4.3 æµ‹è¯•ç”¨ä¾‹

```python
# test_qa_service.py
import pytest
from unittest.mock import AsyncMock, patch
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.services.qa_service import QAService
from src.models.database import Base

@pytest.fixture
def db_session():
    """åˆ›å»ºæµ‹è¯•æ•°æ®åº“ä¼šè¯"""
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)
    SessionLocal = sessionmaker(bind=engine)
    session = SessionLocal()
    yield session
    session.close()

@pytest.fixture
async def qa_service():
    """åˆ›å»ºé—®ç­”æœåŠ¡å®ä¾‹"""
    service = QAService()
    
    # Mock LLMå®¢æˆ·ç«¯
    service.llm_client = AsyncMock()
    service.llm_client.post.return_value.status_code = 200
    service.llm_client.post.return_value.json.return_value = {
        'choices': [{
            'message': {
                'content': 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›ç­”ã€‚'
            }
        }]
    }
    
    # Mockå‘é‡æœåŠ¡
    service.vector_service = AsyncMock()
    service.vector_service.search_similar_chunks.return_value = [
        {
            'chunk_id': 'chunk_123',
            'similarity_score': 0.85,
            'metadata': {
                'content': 'è¿™æ˜¯ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚'
            }
        }
    ]
    
    return service

@pytest.mark.asyncio
async def test_answer_question(qa_service, db_session):
    """æµ‹è¯•å•é—®é¢˜å›ç­”"""
    question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    
    result = await qa_service.answer_question(
        question=question,
        db=db_session
    )
    
    # éªŒè¯ç»“æœ
    assert 'answer' in result
    assert 'sources' in result
    assert 'processing_time' in result
    assert result['answer'] != ''

@pytest.mark.asyncio
async def test_batch_answer_questions(qa_service, db_session):
    """æµ‹è¯•æ‰¹é‡é—®é¢˜å›ç­”"""
    questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    results = await qa_service.batch_answer_questions(
        questions=questions,
        db=db_session
    )
    
    # éªŒè¯ç»“æœ
    assert len(results) == len(questions)
    for result in results:
        assert 'question' in result
        assert 'result' in result
        assert 'status' in result

@pytest.mark.asyncio
async def test_check_answer_accuracy(qa_service):
    """æµ‹è¯•å›ç­”å‡†ç¡®æ€§æ£€æŸ¥"""
    question = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
    answer = "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚"
    keywords = ["æ·±åº¦å­¦ä¹ ", "æœºå™¨å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"]
    
    accuracy = await qa_service.check_answer_accuracy(
        question=question,
        answer=answer,
        expected_keywords=keywords
    )
    
    # éªŒè¯ç»“æœ
    assert 'accuracy_score' in accuracy
    assert 'feedback' in accuracy
    assert accuracy['accuracy_score'] > 0.5
```

### ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹è§£æ

1. **RAGæ¶æ„**ï¼šæ£€ç´¢-å¢å¼º-ç”Ÿæˆçš„å®Œæ•´æµç¨‹
2. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šæ™ºèƒ½é€‰æ‹©å’Œç»„ç»‡ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
3. **æç¤ºå·¥ç¨‹**ï¼šè®¾è®¡æœ‰æ•ˆçš„LLMæç¤ºè¯æ¨¡æ¿
4. **APIé›†æˆ**ï¼šä¸å¤–éƒ¨LLMæœåŠ¡çš„ç¨³å®šé›†æˆ
5. **è´¨é‡æ§åˆ¶**ï¼šå›ç­”åå¤„ç†å’Œå‡†ç¡®æ€§éªŒè¯

### ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

**Q1: LLMå›ç­”ä¸å‡†ç¡®ï¼Ÿ**
- ä¼˜åŒ–æç¤ºè¯æ¨¡æ¿
- æé«˜æ£€ç´¢è´¨é‡
- è°ƒæ•´ç”Ÿæˆå‚æ•°

**Q2: APIè°ƒç”¨å¤±è´¥ï¼Ÿ**
- æ£€æŸ¥APIå¯†é’¥å’Œé…ç½®
- å®ç°é‡è¯•æœºåˆ¶
- æ·»åŠ é™çº§ç­–ç•¥

---

## ğŸ“Š ä»»åŠ¡5ï¼šç³»ç»Ÿç›‘æ§å’Œå¥åº·æ£€æŸ¥

### ğŸ¯ ä»»åŠ¡ç›®æ ‡

å®ç°ç³»ç»Ÿç›‘æ§ã€å¥åº·æ£€æŸ¥å’Œæ€§èƒ½ç»Ÿè®¡åŠŸèƒ½ï¼Œç¡®ä¿RAGç³»ç»Ÿç¨³å®šè¿è¡Œã€‚

### ğŸ”§ æ ¸å¿ƒå®ç°

#### 5.1 å¥åº·æ£€æŸ¥æœåŠ¡ (`src/services/health_service.py`)

```python
import asyncio
import time
from typing import Dict, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import text

from ..config.settings import get_settings
from ..services.vector_service import VectorService
from ..services.qa_service import QAService
from ..utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()

class HealthService:
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥æœåŠ¡"""
    
    def __init__(self):
        self.vector_service = VectorService()
        self.qa_service = QAService()
        
    async def check_system_health(self, db: Session) -> Dict[str, Any]:
        """
        å…¨é¢çš„ç³»ç»Ÿå¥åº·æ£€æŸ¥
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            Dict[str, Any]: å¥åº·æ£€æŸ¥ç»“æœ
        """
        start_time = time.time()
        
        # TODO(lab01-task5): å®ç°ç³»ç»Ÿå¥åº·æ£€æŸ¥é€»è¾‘
        health_status = {
            'overall_status': 'healthy',
            'timestamp': time.time(),
            'checks': {},
            'performance': {},
            'errors': []
        }
        
        try:
            # 1. æ•°æ®åº“å¥åº·æ£€æŸ¥
            db_health = await self._check_database_health(db)
            health_status['checks']['database'] = db_health
            
            # 2. å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥
            vector_health = await self._check_vector_store_health()
            health_status['checks']['vector_store'] = vector_health
            
            # 3. åµŒå…¥æœåŠ¡å¥åº·æ£€æŸ¥
            embedding_health = await self._check_embedding_service_health()
            health_status['checks']['embedding_service'] = embedding_health
            
            # 4. LLMæœåŠ¡å¥åº·æ£€æŸ¥
            llm_health = await self._check_llm_service_health()
            health_status['checks']['llm_service'] = llm_health
            
            # 5. ç³»ç»Ÿèµ„æºæ£€æŸ¥
            resource_health = await self._check_system_resources()
            health_status['checks']['system_resources'] = resource_health
            
            # 6. è®¡ç®—æ€»ä½“çŠ¶æ€
            failed_checks = [
                name for name, check in health_status['checks'].items()
                if not check.get('status', False)
            ]
            
            if failed_checks:
                health_status['overall_status'] = 'unhealthy'
                health_status['errors'] = [
                    f"{check}æœåŠ¡å¼‚å¸¸" for check in failed_checks
                ]
            elif any(
                check.get('warning', False) 
                for check in health_status['checks'].values()
            ):
                health_status['overall_status'] = 'warning'
            
            # 7. æ€§èƒ½ç»Ÿè®¡
            health_status['performance'] = {
                'check_duration': time.time() - start_time,
                'total_checks': len(health_status['checks']),
                'passed_checks': len(health_status['checks']) - len(failed_checks),
                'failed_checks': len(failed_checks)
            }
            
            return health_status
            
        except Exception as e:
            logger.error(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            health_status['overall_status'] = 'error'
            health_status['errors'].append(f"å¥åº·æ£€æŸ¥å¼‚å¸¸: {str(e)}")
            return health_status
    
    async def _check_database_health(self, db: Session) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        # TODO(lab01-task5): å®ç°æ•°æ®åº“å¥åº·æ£€æŸ¥
        try:
            start_time = time.time()
            
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
            result = db.execute(text("SELECT 1"))
            result.fetchone()
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            from ..models.document import Document
            from ..models.chunk import Chunk
            
            doc_count = db.query(Document).count()
            chunk_count = db.query(Chunk).count()
            
            response_time = time.time() - start_time
            
            return {
                'status': True,
                'response_time': response_time,
                'document_count': doc_count,
                'chunk_count': chunk_count,
                'warning': response_time > 1.0  # å“åº”æ—¶é—´è¶…è¿‡1ç§’è­¦å‘Š
            }
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                'status': False,
                'error': str(e),
                'response_time': 0
            }
    
    async def _check_vector_store_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥å‘é‡æ•°æ®åº“å¥åº·çŠ¶æ€"""
        # TODO(lab01-task5): å®ç°å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥
        try:
            start_time = time.time()
            
            # æ£€æŸ¥å‘é‡å­˜å‚¨è¿æ¥
            health_ok = await self.vector_service.vector_store.health_check()
            
            if health_ok:
                # è·å–é›†åˆä¿¡æ¯
                collection_info = await self.vector_service.vector_store.get_collection_info()
                response_time = time.time() - start_time
                
                return {
                    'status': True,
                    'response_time': response_time,
                    'vectors_count': collection_info.get('vectors_count', 0),
                    'collection_status': collection_info.get('status', 'unknown'),
                    'warning': response_time > 2.0
                }
            else:
                return {
                    'status': False,
                    'error': 'å‘é‡æ•°æ®åº“è¿æ¥å¤±è´¥',
                    'response_time': time.time() - start_time
                }
                
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                'status': False,
                'error': str(e),
                'response_time': 0
            }
    
    async def _check_embedding_service_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥åµŒå…¥æœåŠ¡å¥åº·çŠ¶æ€"""
        # TODO(lab01-task5): å®ç°åµŒå…¥æœåŠ¡å¥åº·æ£€æŸ¥
        try:
            start_time = time.time()
            
            # æµ‹è¯•åµŒå…¥ç”Ÿæˆ
            test_text = "å¥åº·æ£€æŸ¥æµ‹è¯•æ–‡æœ¬"
            embedding = await self.vector_service.embedding_service.embed_text(test_text)
            
            response_time = time.time() - start_time
            
            if embedding and len(embedding) > 0:
                return {
                    'status': True,
                    'response_time': response_time,
                    'model_name': self.vector_service.embedding_service.model_name,
                    'dimension': len(embedding),
                    'is_initialized': self.vector_service.embedding_service.is_initialized(),
                    'warning': response_time > 5.0  # åµŒå…¥ç”Ÿæˆè¶…è¿‡5ç§’è­¦å‘Š
                }
            else:
                return {
                    'status': False,
                    'error': 'åµŒå…¥ç”Ÿæˆå¤±è´¥',
                    'response_time': response_time
                }
                
        except Exception as e:
            logger.error(f"åµŒå…¥æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                'status': False,
                'error': str(e),
                'response_time': 0
            }
    
    async def _check_llm_service_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥LLMæœåŠ¡å¥åº·çŠ¶æ€"""
        # TODO(lab01-task5): å®ç°LLMæœåŠ¡å¥åº·æ£€æŸ¥
        try:
            start_time = time.time()
            
            # æµ‹è¯•LLMè°ƒç”¨
            test_prompt = "è¯·å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿ"
            
            if not self.qa_service.llm_client:
                await self.qa_service.initialize()
            
            # å‘é€æµ‹è¯•è¯·æ±‚
            response = await self.qa_service.llm_client.post(
                "/v1/chat/completions",
                json={
                    "model": self.qa_service.model_name,
                    "messages": [{"role": "user", "content": test_prompt}],
                    "max_tokens": 10,
                    "temperature": 0.1
                }
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and result['choices']:
                    return {
                        'status': True,
                        'response_time': response_time,
                        'model_name': self.qa_service.model_name,
                        'api_status': 'connected',
                        'warning': response_time > 10.0  # LLMè°ƒç”¨è¶…è¿‡10ç§’è­¦å‘Š
                    }
            
            return {
                'status': False,
                'error': f'LLM APIè¿”å›é”™è¯¯: {response.status_code}',
                'response_time': response_time
            }
            
        except Exception as e:
            logger.error(f"LLMæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                'status': False,
                'error': str(e),
                'response_time': 0
            }
    
    async def _check_system_resources(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶æ€"""
        # TODO(lab01-task5): å®ç°ç³»ç»Ÿèµ„æºæ£€æŸ¥
        try:
            import psutil
            
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            disk = psutil.disk_usage('/')
            disk_percent = disk.percent
            
            # åˆ¤æ–­è­¦å‘ŠçŠ¶æ€
            warning = (
                cpu_percent > 80 or 
                memory_percent > 85 or 
                disk_percent > 90
            )
            
            return {
                'status': True,
                'cpu_percent': cpu_percent,
                'memory_percent': memory_percent,
                'disk_percent': disk_percent,
                'warning': warning,
                'details': {
                    'memory_total': memory.total,
                    'memory_available': memory.available,
                    'disk_total': disk.total,
                    'disk_free': disk.free
                }
            }
            
        except ImportError:
            # psutilæœªå®‰è£…æ—¶çš„ç®€åŒ–æ£€æŸ¥
            return {
                'status': True,
                'message': 'psutilæœªå®‰è£…ï¼Œæ— æ³•è·å–è¯¦ç»†èµ„æºä¿¡æ¯',
                'warning': False
            }
        except Exception as e:
            logger.error(f"ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                'status': False,
                'error': str(e)
            }
    
    async def get_system_stats(self, db: Session) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        # TODO(lab01-task5): å®ç°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯è·å–
        try:
            from ..models.document import Document
            from ..models.chunk import Chunk
            
            # æ•°æ®åº“ç»Ÿè®¡
            total_documents = db.query(Document).count()
            total_chunks = db.query(Chunk).count()
            vectorized_chunks = db.query(Chunk).filter(
                Chunk.is_vector_stored == True
            ).count()
            
            # å‘é‡æ•°æ®åº“ç»Ÿè®¡
            vector_stats = await self.vector_service.get_vector_stats()
            
            # ç³»ç»Ÿè¿è¡Œæ—¶é—´ï¼ˆç®€åŒ–å®ç°ï¼‰
            import psutil
            boot_time = psutil.boot_time()
            uptime = time.time() - boot_time
            
            return {
                'database_stats': {
                    'total_documents': total_documents,
                    'total_chunks': total_chunks,
                    'vectorized_chunks': vectorized_chunks,
                    'vectorization_rate': vectorized_chunks / total_chunks if total_chunks > 0 else 0
                },
                'vector_stats': vector_stats,
                'system_stats': {
                    'uptime_seconds': uptime,
                    'uptime_hours': uptime / 3600,
                    'timestamp': time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                'error': str(e),
                'timestamp': time.time()
            }