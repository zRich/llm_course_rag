#!/usr/bin/env python3
"""
Lesson 05: Embeddingä¸å‘é‡å…¥åº“ - å®Œæ•´å‘é‡åŒ–æµæ°´çº¿æ¼”ç¤º

æœ¬æ–‡ä»¶æ¼”ç¤ºå®Œæ•´çš„å‘é‡åŒ–æµæ°´çº¿ï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡æ¡£é¢„å¤„ç†å’Œåˆ†å—
2. æ‰¹é‡å‘é‡åŒ–å¤„ç†
3. å‘é‡æ•°æ®åº“å…¥åº“
4. è´¨é‡æ£€æŸ¥å’ŒéªŒè¯
5. æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

ä½œè€…: RAGè¯¾ç¨‹å›¢é˜Ÿ
æ—¥æœŸ: 2024-01-01
ç”¨é€”: Lesson 05è¯¾å ‚æ¼”ç¤º
"""

import time
import uuid
import json
import hashlib
from typing import List, Dict, Any, Optional, Tuple, Generator
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from sentence_transformers import SentenceTransformer
import numpy as np


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—æ•°æ®ç»“æ„"""
    id: str
    text: str
    source: str
    chunk_index: int
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class VectorData:
    """å‘é‡æ•°æ®ç»“æ„"""
    id: str
    vector: List[float]
    chunk: DocumentChunk
    
    def to_point(self) -> PointStruct:
        """è½¬æ¢ä¸ºQdrant Pointç»“æ„"""
        return PointStruct(
            id=self.id,
            vector=self.vector,
            payload={
                "text": self.chunk.text,
                "source": self.chunk.source,
                "chunk_index": self.chunk.chunk_index,
                "text_length": len(self.chunk.text),
                "text_hash": hashlib.md5(self.chunk.text.encode()).hexdigest(),
                **self.chunk.metadata
            }
        )


@dataclass
class PipelineStats:
    """æµæ°´çº¿ç»Ÿè®¡ä¿¡æ¯"""
    total_documents: int = 0
    total_chunks: int = 0
    total_vectors: int = 0
    processing_time: float = 0.0
    vectorization_time: float = 0.0
    insertion_time: float = 0.0
    average_chunk_length: float = 0.0
    success_rate: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def process_text(self, text: str, source: str = "unknown") -> List[DocumentChunk]:
        """å¤„ç†æ–‡æœ¬å¹¶åˆ†å—"""
        logger.info(f"å¤„ç†æ–‡æ¡£: {source}, é•¿åº¦: {len(text)}å­—ç¬¦")
        
        # ç®€å•çš„åˆ†å—ç­–ç•¥ï¼ˆæŒ‰å¥å­åˆ†å‰²ï¼‰
        sentences = self._split_sentences(text)
        chunks = self._create_chunks(sentences, source)
        
        logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunks)}ä¸ªå—")
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPå·¥å…·ï¼‰
        import re
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _create_chunks(self, sentences: List[str], source: str) -> List[DocumentChunk]:
        """åˆ›å»ºæ–‡æ¡£å—"""
        chunks = []
        current_chunk = ""
        chunk_index = 0
        
        for sentence in sentences:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°å—
            if len(current_chunk) + len(sentence) > self.chunk_size and current_chunk:
                # åˆ›å»ºå½“å‰å—
                chunk = DocumentChunk(
                    id=str(uuid.uuid4()),
                    text=current_chunk.strip(),
                    source=source,
                    chunk_index=chunk_index,
                    metadata={
                        "sentence_count": len(current_chunk.split('.')),
                        "created_at": time.time(),
                        "language": self._detect_language(current_chunk)
                    }
                )
                chunks.append(chunk)
                
                # å‡†å¤‡ä¸‹ä¸€ä¸ªå—ï¼ˆè€ƒè™‘é‡å ï¼‰
                if self.chunk_overlap > 0:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + " " + sentence
                else:
                    current_chunk = sentence
                
                chunk_index += 1
            else:
                current_chunk += " " + sentence if current_chunk else sentence
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunk = DocumentChunk(
                id=str(uuid.uuid4()),
                text=current_chunk.strip(),
                source=source,
                chunk_index=chunk_index,
                metadata={
                    "sentence_count": len(current_chunk.split('.')),
                    "created_at": time.time(),
                    "language": self._detect_language(current_chunk)
                }
            )
            chunks.append(chunk)
        
        return chunks
    
    def _detect_language(self, text: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        return "zh" if chinese_chars > len(text) * 0.3 else "en"


class VectorPipeline:
    """å‘é‡åŒ–æµæ°´çº¿"""
    
    def __init__(self, 
                 model_name: str = "BAAI/bge-m3",
                 qdrant_host: str = "localhost",
                 qdrant_port: int = 6333,
                 collection_name: str = "pipeline_demo",
                 batch_size: int = 32,
                 max_workers: int = 4):
        
        self.model_name = model_name
        self.qdrant_host = qdrant_host
        self.qdrant_port = qdrant_port
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.client = None
        self.doc_processor = DocumentProcessor()
        self.stats = PipelineStats()
        
        self._initialize_components()
    
    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–æµæ°´çº¿ç»„ä»¶"""
        logger.info("åˆå§‹åŒ–å‘é‡åŒ–æµæ°´çº¿...")
        
        # åŠ è½½å‘é‡åŒ–æ¨¡å‹
        logger.info(f"åŠ è½½å‘é‡åŒ–æ¨¡å‹: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        
        # è¿æ¥Qdrantæ•°æ®åº“
        logger.info(f"è¿æ¥Qdrantæ•°æ®åº“: {self.qdrant_host}:{self.qdrant_port}")
        self.client = QdrantClient(host=self.qdrant_host, port=self.qdrant_port)
        
        # åˆ›å»ºæˆ–é‡ç½®é›†åˆ
        self._setup_collection()
        
        logger.info("æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_collection(self) -> None:
        """è®¾ç½®Qdranté›†åˆ"""
        logger.info(f"è®¾ç½®é›†åˆ: {self.collection_name}")
        
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collections = self.client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if self.collection_name in collection_names:
            logger.info(f"åˆ é™¤ç°æœ‰é›†åˆ: {self.collection_name}")
            self.client.delete_collection(self.collection_name)
        
        # åˆ›å»ºæ–°é›†åˆ
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=768,  # bge-m3æ¨¡å‹ç»´åº¦
                distance=Distance.COSINE
            )
        )
        
        logger.info(f"é›†åˆåˆ›å»ºæˆåŠŸ: {self.collection_name}")
    
    def process_documents(self, documents: List[Tuple[str, str]]) -> List[DocumentChunk]:
        """å¤„ç†å¤šä¸ªæ–‡æ¡£"""
        logger.info(f"å¼€å§‹å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£")
        
        all_chunks = []
        start_time = time.time()
        
        for i, (text, source) in enumerate(documents, 1):
            logger.info(f"å¤„ç†æ–‡æ¡£ {i}/{len(documents)}: {source}")
            chunks = self.doc_processor.process_text(text, source)
            all_chunks.extend(chunks)
        
        processing_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats.total_documents = len(documents)
        self.stats.total_chunks = len(all_chunks)
        self.stats.processing_time = processing_time
        self.stats.average_chunk_length = np.mean([len(chunk.text) for chunk in all_chunks])
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {len(all_chunks)}ä¸ªå—, è€—æ—¶: {processing_time:.2f}ç§’")
        return all_chunks
    
    def vectorize_chunks(self, chunks: List[DocumentChunk]) -> List[VectorData]:
        """æ‰¹é‡å‘é‡åŒ–æ–‡æ¡£å—"""
        logger.info(f"å¼€å§‹å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        start_time = time.time()
        vector_data_list = []
        
        # æ‰¹é‡å¤„ç†
        for i in range(0, len(chunks), self.batch_size):
            batch_chunks = chunks[i:i + self.batch_size]
            batch_texts = [chunk.text for chunk in batch_chunks]
            
            logger.info(f"å‘é‡åŒ–æ‰¹æ¬¡ {i//self.batch_size + 1}/{(len(chunks)-1)//self.batch_size + 1}")
            
            # æ‰¹é‡å‘é‡åŒ–
            batch_vectors = self.model.encode(batch_texts, show_progress_bar=False)
            
            # åˆ›å»ºå‘é‡æ•°æ®
            for chunk, vector in zip(batch_chunks, batch_vectors):
                vector_data = VectorData(
                    id=chunk.id,
                    vector=vector.tolist(),
                    chunk=chunk
                )
                vector_data_list.append(vector_data)
        
        vectorization_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats.total_vectors = len(vector_data_list)
        self.stats.vectorization_time = vectorization_time
        
        logger.info(f"å‘é‡åŒ–å®Œæˆ: {len(vector_data_list)}ä¸ªå‘é‡, è€—æ—¶: {vectorization_time:.2f}ç§’")
        return vector_data_list
    
    def insert_vectors(self, vector_data_list: List[VectorData]) -> bool:
        """æ‰¹é‡æ’å…¥å‘é‡åˆ°æ•°æ®åº“"""
        logger.info(f"å¼€å§‹æ’å…¥ {len(vector_data_list)} ä¸ªå‘é‡")
        
        start_time = time.time()
        success_count = 0
        
        try:
            # è½¬æ¢ä¸ºQdrant Points
            points = [vd.to_point() for vd in vector_data_list]
            
            # æ‰¹é‡æ’å…¥
            for i in range(0, len(points), self.batch_size):
                batch_points = points[i:i + self.batch_size]
                
                logger.info(f"æ’å…¥æ‰¹æ¬¡ {i//self.batch_size + 1}/{(len(points)-1)//self.batch_size + 1}")
                
                result = self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch_points
                )
                
                if result.status == "completed":
                    success_count += len(batch_points)
                else:
                    logger.warning(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥: {result.status}")
            
            insertion_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats.insertion_time = insertion_time
            self.stats.success_rate = success_count / len(vector_data_list)
            
            logger.info(f"å‘é‡æ’å…¥å®Œæˆ: {success_count}/{len(vector_data_list)}, è€—æ—¶: {insertion_time:.2f}ç§’")
            return success_count == len(vector_data_list)
            
        except Exception as e:
            logger.error(f"å‘é‡æ’å…¥å¤±è´¥: {e}")
            return False
    
    def validate_pipeline(self, sample_queries: List[str] = None) -> Dict[str, Any]:
        """éªŒè¯æµæ°´çº¿ç»“æœ"""
        logger.info("å¼€å§‹éªŒè¯æµæ°´çº¿ç»“æœ")
        
        validation_results = {
            "collection_info": {},
            "sample_search": [],
            "data_quality": {}
        }
        
        try:
            # 1. æ£€æŸ¥é›†åˆä¿¡æ¯
            collection_info = self.client.get_collection(self.collection_name)
            validation_results["collection_info"] = {
                "name": self.collection_name,
                "status": collection_info.status,
                "points_count": collection_info.points_count,
                "vector_size": collection_info.config.params.vectors.size
            }
            
            logger.info(f"é›†åˆéªŒè¯: {collection_info.points_count}ä¸ªå‘é‡")
            
            # 2. æ ·æœ¬æœç´¢æµ‹è¯•
            if sample_queries:
                for query in sample_queries[:3]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡
                    query_vector = self.model.encode([query])[0]
                    search_results = self.client.search(
                        collection_name=self.collection_name,
                        query_vector=query_vector.tolist(),
                        limit=3,
                        with_payload=True
                    )
                    
                    validation_results["sample_search"].append({
                        "query": query,
                        "results_count": len(search_results),
                        "top_score": search_results[0].score if search_results else 0,
                        "top_result": search_results[0].payload.get("text", "")[:100] if search_results else ""
                    })
            
            # 3. æ•°æ®è´¨é‡æ£€æŸ¥
            sample_points = self.client.scroll(
                collection_name=self.collection_name,
                limit=100,
                with_payload=True
            )[0]
            
            if sample_points:
                text_lengths = [len(point.payload.get("text", "")) for point in sample_points]
                sources = [point.payload.get("source", "unknown") for point in sample_points]
                languages = [point.payload.get("language", "unknown") for point in sample_points]
                
                validation_results["data_quality"] = {
                    "sample_size": len(sample_points),
                    "avg_text_length": np.mean(text_lengths),
                    "text_length_std": np.std(text_lengths),
                    "unique_sources": len(set(sources)),
                    "language_distribution": dict(zip(*np.unique(languages, return_counts=True)))
                }
            
            logger.info("æµæ°´çº¿éªŒè¯å®Œæˆ")
            return validation_results
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿éªŒè¯å¤±è´¥: {e}")
            return validation_results
    
    def run_pipeline(self, documents: List[Tuple[str, str]], 
                    sample_queries: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å‘é‡åŒ–æµæ°´çº¿"""
        logger.info("ğŸš€ å¯åŠ¨å®Œæ•´å‘é‡åŒ–æµæ°´çº¿")
        
        pipeline_start_time = time.time()
        
        try:
            # 1. æ–‡æ¡£å¤„ç†å’Œåˆ†å—
            logger.info("ğŸ“„ æ­¥éª¤1: æ–‡æ¡£å¤„ç†å’Œåˆ†å—")
            chunks = self.process_documents(documents)
            
            # 2. å‘é‡åŒ–
            logger.info("ğŸ§  æ­¥éª¤2: æ‰¹é‡å‘é‡åŒ–")
            vector_data_list = self.vectorize_chunks(chunks)
            
            # 3. å‘é‡å…¥åº“
            logger.info("ğŸ’¾ æ­¥éª¤3: å‘é‡æ•°æ®åº“å…¥åº“")
            insertion_success = self.insert_vectors(vector_data_list)
            
            # 4. éªŒè¯ç»“æœ
            logger.info("âœ… æ­¥éª¤4: ç»“æœéªŒè¯")
            validation_results = self.validate_pipeline(sample_queries)
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - pipeline_start_time
            
            # æ±‡æ€»ç»“æœ
            pipeline_results = {
                "success": insertion_success,
                "total_time": total_time,
                "stats": self.stats.to_dict(),
                "validation": validation_results,
                "performance_metrics": {
                    "documents_per_second": self.stats.total_documents / total_time,
                    "chunks_per_second": self.stats.total_chunks / total_time,
                    "vectors_per_second": self.stats.total_vectors / total_time,
                    "vectorization_efficiency": self.stats.total_vectors / self.stats.vectorization_time,
                    "insertion_efficiency": self.stats.total_vectors / self.stats.insertion_time
                }
            }
            
            logger.info(f"ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")
            return pipeline_results
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        logger.info("æ¸…ç†æµæ°´çº¿èµ„æº")
        try:
            if self.client:
                self.client.delete_collection(self.collection_name)
                logger.info(f"é›†åˆ {self.collection_name} å·²åˆ é™¤")
        except Exception as e:
            logger.warning(f"æ¸…ç†å¤±è´¥: {e}")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ Lesson 05: å®Œæ•´å‘é‡åŒ–æµæ°´çº¿æ¼”ç¤º")
    print("=" * 80)
    
    # å‡†å¤‡æ¼”ç¤ºæ–‡æ¡£
    demo_documents = [
        (
            "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æ™ºèƒ½æœºå™¨ã€‚"
            "AIçš„ä¸»è¦ç›®æ ‡æ˜¯å¼€å‘èƒ½å¤Ÿå­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€ç†è§£è‡ªç„¶è¯­è¨€å’Œè§£å†³é—®é¢˜çš„ç³»ç»Ÿã€‚ç°ä»£AIæŠ€æœ¯åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€"
            "è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ç­‰å¤šä¸ªé¢†åŸŸã€‚è¿™äº›æŠ€æœ¯æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œä»æ™ºèƒ½æ‰‹æœºçš„è¯­éŸ³åŠ©æ‰‹åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ"
            "AIæ­£åœ¨å„ä¸ªè¡Œä¸šä¸­å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚",
            "AI_introduction.txt"
        ),
        (
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚"
            "æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ã€‚"
            "ä¸»è¦çš„æœºå™¨å­¦ä¹ ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚"
            "æ— ç›‘ç£å­¦ä¹ åœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ã€‚å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å¹¶æ¥æ”¶å¥–åŠ±æˆ–æƒ©ç½šæ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºã€‚",
            "machine_learning.txt"
        ),
        (
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚"
            "æ·±åº¦ç¥ç»ç½‘ç»œç”±å¤šä¸ªå±‚ç»„æˆï¼Œæ¯ä¸€å±‚éƒ½èƒ½å­¦ä¹ æ•°æ®çš„ä¸åŒç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§åˆ†å±‚çš„ç‰¹å¾å­¦ä¹ ä½¿æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€"
            "è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç‰¹åˆ«é€‚åˆå¤„ç†å›¾åƒæ•°æ®ï¼Œ"
            "è€Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’ŒTransformeræ¶æ„åˆ™åœ¨åºåˆ—æ•°æ®å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ·±åº¦å­¦ä¹ çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ã€"
            "å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å’Œå…ˆè¿›çš„ä¼˜åŒ–ç®—æ³•ã€‚",
            "deep_learning.txt"
        ),
        (
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. "
            "NLP combines computational linguistics with machine learning and deep learning to enable computers to understand, interpret, and generate human language. "
            "Key NLP tasks include text classification, sentiment analysis, named entity recognition, machine translation, and question answering. "
            "Modern NLP systems use transformer-based models like BERT, GPT, and T5, which have achieved remarkable performance on various language understanding tasks. "
            "These models are pre-trained on large text corpora and can be fine-tuned for specific applications.",
            "nlp_overview.txt"
        ),
        (
            "Computer vision is a field of artificial intelligence that enables computers to interpret and understand visual information from the world. "
            "It involves developing algorithms and techniques to extract meaningful information from digital images and videos. "
            "Computer vision applications include object detection, image classification, facial recognition, medical image analysis, and autonomous vehicles. "
            "Deep learning has revolutionized computer vision, with convolutional neural networks (CNNs) becoming the standard approach for most vision tasks. "
            "Recent advances include attention mechanisms, vision transformers, and multi-modal models that combine vision with language understanding.",
            "computer_vision.txt"
        )
    ]
    
    # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢
    sample_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„ä¸»è¦ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«",
        "What is natural language processing?",
        "Computer vision applications"
    ]
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = VectorPipeline(
        collection_name="lesson05_pipeline_demo",
        batch_size=16,
        max_workers=2
    )
    
    try:
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        results = pipeline.run_pipeline(demo_documents, sample_queries)
        
        # å±•ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ“Š æµæ°´çº¿æ‰§è¡Œç»“æœ")
        print("=" * 80)
        
        print(f"âœ… æ‰§è¡ŒçŠ¶æ€: {'æˆåŠŸ' if results['success'] else 'å¤±è´¥'}")
        print(f"â±ï¸  æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
        print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {results['stats']['total_documents']}ä¸ª")
        print(f"ğŸ“ ç”Ÿæˆå—æ•°: {results['stats']['total_chunks']}ä¸ª")
        print(f"ğŸ§  å‘é‡æ•°é‡: {results['stats']['total_vectors']}ä¸ª")
        print(f"ğŸ“Š æˆåŠŸç‡: {results['stats']['success_rate']:.2%}")
        print(f"ğŸ“ å¹³å‡å—é•¿åº¦: {results['stats']['average_chunk_length']:.1f}å­—ç¬¦")
        
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        metrics = results['performance_metrics']
        print(f"   - æ–‡æ¡£å¤„ç†é€Ÿåº¦: {metrics['documents_per_second']:.2f}æ–‡æ¡£/ç§’")
        print(f"   - åˆ†å—å¤„ç†é€Ÿåº¦: {metrics['chunks_per_second']:.2f}å—/ç§’")
        print(f"   - å‘é‡åŒ–é€Ÿåº¦: {metrics['vectorization_efficiency']:.2f}å‘é‡/ç§’")
        print(f"   - å…¥åº“é€Ÿåº¦: {metrics['insertion_efficiency']:.2f}å‘é‡/ç§’")
        
        print(f"\nğŸ” éªŒè¯ç»“æœ:")
        validation = results['validation']
        collection_info = validation['collection_info']
        print(f"   - é›†åˆçŠ¶æ€: {collection_info['status']}")
        print(f"   - å­˜å‚¨å‘é‡æ•°: {collection_info['points_count']}")
        print(f"   - å‘é‡ç»´åº¦: {collection_info['vector_size']}")
        
        if validation['sample_search']:
            print(f"\nğŸ¯ æœç´¢æµ‹è¯•:")
            for i, search in enumerate(validation['sample_search'], 1):
                print(f"   æŸ¥è¯¢{i}: '{search['query']}'")
                print(f"   - ç»“æœæ•°: {search['results_count']}")
                print(f"   - æœ€é«˜åˆ†: {search['top_score']:.4f}")
                print(f"   - æœ€ä½³åŒ¹é…: '{search['top_result']}...'")
                print()
        
        if validation['data_quality']:
            quality = validation['data_quality']
            print(f"ğŸ“ˆ æ•°æ®è´¨é‡:")
            print(f"   - æ ·æœ¬å¤§å°: {quality['sample_size']}")
            print(f"   - å¹³å‡æ–‡æœ¬é•¿åº¦: {quality['avg_text_length']:.1f}Â±{quality['text_length_std']:.1f}")
            print(f"   - æ•°æ®æºæ•°é‡: {quality['unique_sources']}")
            print(f"   - è¯­è¨€åˆ†å¸ƒ: {quality['language_distribution']}")
        
        print("\n" + "=" * 80)
        print("ğŸ¯ å…³é”®å­¦ä¹ è¦ç‚¹")
        print("=" * 80)
        print("1. æµæ°´çº¿è®¾è®¡: æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„æ¶æ„è®¾è®¡")
        print("2. æ‰¹é‡å¤„ç†: æé«˜å‘é‡åŒ–å’Œå…¥åº“æ•ˆç‡")
        print("3. é”™è¯¯å¤„ç†: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•")
        print("4. æ€§èƒ½ç›‘æ§: å®æ—¶ç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡")
        print("5. è´¨é‡éªŒè¯: è‡ªåŠ¨åŒ–çš„ç»“æœéªŒè¯æœºåˆ¶")
        print("6. èµ„æºç®¡ç†: åˆç†çš„å†…å­˜å’Œè®¡ç®—èµ„æºä½¿ç”¨")
        
        # è¯¢é—®æ˜¯å¦è¿›è¡Œäº¤äº’å¼æœç´¢æµ‹è¯•
        print("\n" + "=" * 80)
        interactive = input("æ˜¯å¦è¿›è¡Œäº¤äº’å¼æœç´¢æµ‹è¯•ï¼Ÿ(y/N): ").lower().strip()
        if interactive == 'y':
            print("è¾“å…¥æŸ¥è¯¢æ–‡æœ¬è¿›è¡Œæœç´¢æµ‹è¯• (è¾“å…¥'quit'é€€å‡º):")
            while True:
                query = input("\nğŸ” æŸ¥è¯¢: ").strip()
                if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break
                
                if query:
                    try:
                        query_vector = pipeline.model.encode([query])[0]
                        search_results = pipeline.client.search(
                            collection_name=pipeline.collection_name,
                            query_vector=query_vector.tolist(),
                            limit=3,
                            with_payload=True
                        )
                        
                        print(f"\nğŸ“‹ æœç´¢ç»“æœ (å…±{len(search_results)}ä¸ª):")
                        for i, result in enumerate(search_results, 1):
                            print(f"  {i}. ç›¸ä¼¼åº¦: {result.score:.4f}")
                            print(f"     æ¥æº: {result.payload.get('source', 'unknown')}")
                            print(f"     æ–‡æœ¬: '{result.payload.get('text', '')[:150]}...'")
                            print()
                    except Exception as e:
                        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"Pipeline execution failed: {e}")
    
    finally:
        # è¯¢é—®æ˜¯å¦æ¸…ç†æ•°æ®
        cleanup = input("\næ˜¯å¦æ¸…ç†æ¼”ç¤ºæ•°æ®ï¼Ÿ(y/N): ").lower().strip()
        if cleanup == 'y':
            pipeline.cleanup()
        else:
            print(f"æ¼”ç¤ºæ•°æ®ä¿ç•™åœ¨é›†åˆ '{pipeline.collection_name}' ä¸­")
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()