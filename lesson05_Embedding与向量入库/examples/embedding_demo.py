#!/usr/bin/env python3
"""
Lesson 05: Embeddingä¸Žå‘é‡å…¥åº“ - å‘é‡åŒ–æ¼”ç¤ºç¤ºä¾‹

æœ¬æ–‡ä»¶æ¼”ç¤ºbge-m3æ¨¡åž‹çš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š
1. æ¨¡åž‹åŠ è½½å’Œåˆå§‹åŒ–
2. å•æ–‡æœ¬å‘é‡åŒ–
3. æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
4. å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
5. æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

ä½œè€…: RAGè¯¾ç¨‹å›¢é˜Ÿ
æ—¥æœŸ: 2024-01-01
ç”¨é€”: Lesson 05è¯¾å ‚æ¼”ç¤º
"""

import time
import numpy as np
from typing import List, Tuple, Dict, Any
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')


class BGEEmbeddingDemo:
    """bge-m3æ¨¡åž‹å‘é‡åŒ–æ¼”ç¤ºç±»"""
    
    def __init__(self, model_name: str = 'BAAI/bge-m3'):
        """
        åˆå§‹åŒ–å‘é‡åŒ–æ¼”ç¤ºå™¨
        
        Args:
            model_name: æ¨¡åž‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨bge-m3
        """
        self.model_name = model_name
        self.model = None
        self.load_model()
    
    def load_model(self) -> None:
        """åŠ è½½bge-m3æ¨¡åž‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡åž‹: {self.model_name}")
        print("é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦ä¸‹è½½æ¨¡åž‹æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        start_time = time.time()
        try:
            self.model = SentenceTransformer(self.model_name)
            load_time = time.time() - start_time
            print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f}ç§’")
            print(f"ðŸ“Š æ¨¡åž‹ä¿¡æ¯:")
            print(f"   - æ¨¡åž‹åç§°: {self.model_name}")
            print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {self.model.max_seq_length}")
            print(f"   - å‘é‡ç»´åº¦: 768")
        except Exception as e:
            print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode_single_text(self, text: str) -> np.ndarray:
        """
        å•æ–‡æœ¬å‘é‡åŒ–æ¼”ç¤º
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å‘é‡åŒ–ç»“æžœ
        """
        print(f"\nðŸ”„ å•æ–‡æœ¬å‘é‡åŒ–æ¼”ç¤º")
        print(f"è¾“å…¥æ–‡æœ¬: '{text}'")
        
        start_time = time.time()
        embedding = self.model.encode(text)
        encode_time = time.time() - start_time
        
        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼")
        print(f"   - å‘é‡ç»´åº¦: {embedding.shape}")
        print(f"   - å‘é‡ç±»åž‹: {type(embedding)}")
        print(f"   - å¤„ç†è€—æ—¶: {encode_time*1000:.2f}æ¯«ç§’")
        print(f"   - å‘é‡å‰5ç»´: {embedding[:5]}")
        print(f"   - å‘é‡èŒƒå›´: [{embedding.min():.4f}, {embedding.max():.4f}]")
        
        return embedding
    
    def encode_batch_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–æ¼”ç¤º
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å‘é‡åŒ–ç»“æžœçŸ©é˜µ
        """
        print(f"\nðŸ”„ æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–æ¼”ç¤º")
        print(f"æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†è¾“å…¥æ–‡æœ¬
        print("è¾“å…¥æ–‡æœ¬ç¤ºä¾‹:")
        for i, text in enumerate(texts[:3]):
            print(f"  {i+1}. '{text}'")
        if len(texts) > 3:
            print(f"  ... è¿˜æœ‰{len(texts)-3}ä¸ªæ–‡æœ¬")
        
        start_time = time.time()
        embeddings = self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)
        encode_time = time.time() - start_time
        
        print(f"âœ… æ‰¹é‡å‘é‡åŒ–å®Œæˆï¼")
        print(f"   - è¾“å‡ºå½¢çŠ¶: {embeddings.shape}")
        print(f"   - æ€»è€—æ—¶: {encode_time:.2f}ç§’")
        print(f"   - å¹³å‡æ¯æ–‡æœ¬: {encode_time/len(texts)*1000:.2f}æ¯«ç§’")
        print(f"   - å¤„ç†é€Ÿåº¦: {len(texts)/encode_time:.1f}æ–‡æœ¬/ç§’")
        
        return embeddings
    
    def calculate_similarity_matrix(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """
        è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µæ¼”ç¤º
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            (embeddings, similarity_matrix)
        """
        print(f"\nðŸ”„ ç›¸ä¼¼åº¦è®¡ç®—æ¼”ç¤º")
        print("è®¡ç®—æ–‡æœ¬é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦...")
        
        # æ‰¹é‡å‘é‡åŒ–
        embeddings = self.model.encode(texts)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(embeddings)
        
        print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼")
        print(f"ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")
        
        # æ˜¾ç¤ºç›¸ä¼¼åº¦ç»“æžœ
        print("\nðŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ:")
        print("æ–‡æœ¬åˆ—è¡¨:")
        for i, text in enumerate(texts):
            print(f"  {i}: '{text}'")
        
        print("\nç›¸ä¼¼åº¦è¯¦æƒ…:")
        for i in range(len(texts)):
            for j in range(i+1, len(texts)):
                similarity = similarity_matrix[i][j]
                print(f"  æ–‡æœ¬{i} â†” æ–‡æœ¬{j}: {similarity:.4f}")
        
        return embeddings, similarity_matrix
    
    def semantic_search_demo(self, query: str, documents: List[str], top_k: int = 3) -> List[Tuple[int, str, float]]:
        """
        è¯­ä¹‰æœç´¢æ¼”ç¤º
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›žtop-kç»“æžœ
            
        Returns:
            æœç´¢ç»“æžœåˆ—è¡¨ [(index, document, score), ...]
        """
        print(f"\nðŸ” è¯­ä¹‰æœç´¢æ¼”ç¤º")
        print(f"æŸ¥è¯¢æ–‡æœ¬: '{query}'")
        print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
        print(f"è¿”å›žtop-{top_k}ç»“æžœ")
        
        # å‘é‡åŒ–æŸ¥è¯¢å’Œæ–‡æ¡£
        query_embedding = self.model.encode([query])
        doc_embeddings = self.model.encode(documents)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
        
        # èŽ·å–top-kç»“æžœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        print(f"\nðŸŽ¯ æœç´¢ç»“æžœ:")
        for rank, idx in enumerate(top_indices, 1):
            score = similarities[idx]
            document = documents[idx]
            results.append((idx, document, score))
            print(f"  æŽ’å{rank}: (ç›¸ä¼¼åº¦: {score:.4f})")
            print(f"    æ–‡æ¡£{idx}: '{document}'")
        
        return results
    
    def performance_benchmark(self, text_counts: List[int] = [10, 50, 100, 500]) -> Dict[int, Dict[str, float]]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            text_counts: æµ‹è¯•çš„æ–‡æœ¬æ•°é‡åˆ—è¡¨
            
        Returns:
            æ€§èƒ½æµ‹è¯•ç»“æžœ
        """
        print(f"\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("æµ‹è¯•ä¸åŒæ–‡æœ¬æ•°é‡ä¸‹çš„å‘é‡åŒ–æ€§èƒ½...")
        
        results = {}
        
        for count in text_counts:
            print(f"\næµ‹è¯• {count} ä¸ªæ–‡æœ¬:")
            
            # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
            test_texts = [f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œå†…å®¹å…³äºŽäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„å‘å±•åº”ç”¨ã€‚" for i in range(count)]
            
            # æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°
            batch_sizes = [16, 32, 64] if count >= 64 else [16, 32]
            
            best_time = float('inf')
            best_batch_size = 16
            
            for batch_size in batch_sizes:
                start_time = time.time()
                embeddings = self.model.encode(test_texts, batch_size=batch_size)
                encode_time = time.time() - start_time
                
                if encode_time < best_time:
                    best_time = encode_time
                    best_batch_size = batch_size
                
                print(f"  æ‰¹å¤§å°{batch_size}: {encode_time:.2f}ç§’ ({count/encode_time:.1f}æ–‡æœ¬/ç§’)")
            
            results[count] = {
                'best_time': best_time,
                'best_batch_size': best_batch_size,
                'texts_per_second': count / best_time,
                'ms_per_text': best_time / count * 1000
            }
            
            print(f"  âœ… æœ€ä½³é…ç½®: æ‰¹å¤§å°{best_batch_size}, {best_time:.2f}ç§’")
        
        # æ€§èƒ½æ€»ç»“
        print(f"\nðŸ“ˆ æ€§èƒ½æ€»ç»“:")
        for count, result in results.items():
            print(f"  {count}æ–‡æœ¬: {result['texts_per_second']:.1f}æ–‡æœ¬/ç§’ "
                  f"({result['ms_per_text']:.2f}ms/æ–‡æœ¬)")
        
        return results


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("=" * 60)
    print("ðŸš€ Lesson 05: bge-m3å‘é‡åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¼”ç¤ºå™¨
    demo = BGEEmbeddingDemo()
    
    # 1. å•æ–‡æœ¬å‘é‡åŒ–æ¼”ç¤º
    single_text = "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
    embedding = demo.encode_single_text(single_text)
    
    # 2. æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–æ¼”ç¤º
    batch_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "æœºå™¨å­¦ä¹ æ˜¯å®žçŽ°äººå·¥æ™ºèƒ½çš„é‡è¦æ–¹æ³•",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸ",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„é‡è¦åº”ç”¨æ–¹å‘",
        "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿç†è§£å›¾åƒ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥"
    ]
    batch_embeddings = demo.encode_batch_texts(batch_texts)
    
    # 3. ç›¸ä¼¼åº¦è®¡ç®—æ¼”ç¤º
    similarity_texts = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿ",
        "AIæŠ€æœ¯è¿›æ­¥å¾ˆå¿«",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯"
    ]
    embeddings, similarity_matrix = demo.calculate_similarity_matrix(similarity_texts)
    
    # 4. è¯­ä¹‰æœç´¢æ¼”ç¤º
    query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    documents = [
        "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œè®©è®¡ç®—æœºä»Žæ•°æ®ä¸­å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥žç»ç½‘ç»œè¿›è¡Œå¤æ‚çš„æ¨¡å¼è¯†åˆ«",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œç†è§£å›¾åƒå†…å®¹",
        "ä»Šå¤©çš„å¤©æ°”é¢„æŠ¥æ˜¾ç¤ºä¼šæœ‰å°é›¨",
        "æ•°æ®ç§‘å­¦ç»“åˆç»Ÿè®¡å­¦å’Œè®¡ç®—æœºç§‘å­¦æ¥åˆ†æžæ•°æ®"
    ]
    search_results = demo.semantic_search_demo(query, documents)
    
    # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
    performance_results = demo.performance_benchmark([10, 50, 100])
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)
    
    # å…³é”®è¦ç‚¹æ€»ç»“
    print("\nðŸŽ¯ å…³é”®è¦ç‚¹æ€»ç»“:")
    print("1. bge-m3æ¨¡åž‹æ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬å‘é‡åŒ–")
    print("2. å‘é‡ç»´åº¦å›ºå®šä¸º768ç»´ï¼Œé€‚åˆå¤§å¤šæ•°åº”ç”¨åœºæ™¯")
    print("3. æ‰¹å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜å¤„ç†æ•ˆçŽ‡")
    print("4. ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯è¡¡é‡æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼æ€§çš„æœ‰æ•ˆæ–¹æ³•")
    print("5. è¯­ä¹‰æœç´¢æ¯”å…³é”®è¯åŒ¹é…æ›´èƒ½ç†è§£ç”¨æˆ·æ„å›¾")
    print("6. åˆé€‚çš„æ‰¹å¤„ç†å¤§å°å¯ä»¥ä¼˜åŒ–æ€§èƒ½")


if __name__ == "__main__":
    main()