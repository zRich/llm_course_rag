# Lesson 05 - Embedding与向量入库 - 学生实验指导

## 代码基础准备

在开始本节课的实验之前，我们需要基于上一节课的代码继续开发。现在我们使用Git分支管理来获取代码。

### 步骤1：进入项目目录并切换分支

```bash
# 进入rag-system项目目录
cd rag-system

# 切换到lesson05分支
git checkout lesson05

# 验证当前分支
git branch
# 应该显示 * lesson05
```

### 步骤2：验证代码状态

```bash
# 检查项目结构
ls -la
# 应该看到：src/ scripts/ test_documents/ docker-compose.yml 等文件和目录

# 检查向量化相关文件
ls -la src/embedding/
# 应该看到：embedder.py vector_store.py 等文件
```

### 步骤3：验证PDF解析环境

```bash
# 启动依赖服务
docker-compose up -d

# 测试PDF解析功能
python -c "from src.document.pdf_parser import PDFParser; print('PDF解析器导入成功')"

# 验证文档解析
ls test_documents/  # 确认有测试PDF文件
```

**说明**：lesson05分支包含了lesson04的所有代码，并新增了Embedding向量化和Qdrant向量数据库相关的模块和配置。

---

## 🎯 实验目标

通过本实验，你将学会：
1. 使用sentence-transformers库进行文本向量化
2. 配置和使用bge-m3模型
3. 操作Qdrant向量数据库
4. 实现批量文档向量化和入库
5. 进行向量相似度搜索和性能优化

---

## 🛠️ 环境准备

### 1. 安装依赖包

**更新 `pyproject.toml`：**
```toml
[tool.uv.dependencies]
# 现有依赖...
sentence-transformers = "^2.2.2"
qdrant-client = "^1.7.0"
torch = "^2.1.0"
numpy = "^1.24.0"
scipy = "^1.11.0"
scikit-learn = "^1.3.0"
tqdm = "^4.66.0"
matplotlib = "^3.7.0"
seaborn = "^0.12.0"
```

**安装依赖：**
```bash
uv sync
```

### 2. 启动Qdrant服务

**确保 `docker-compose.yml` 包含Qdrant配置：**
```yaml
services:
  # 其他服务...
  
  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag_qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  # 其他卷...
  qdrant_data:
```

**启动服务：**
```bash
docker-compose up -d qdrant
```

**验证Qdrant服务：**
```bash
curl http://localhost:6333/health
```

### 3. 测试环境

**创建 `test_environment.py`：**
```python
"""测试向量化环境"""

import torch
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient

def test_torch():
    """测试PyTorch环境"""
    print(f"🔥 PyTorch版本: {torch.__version__}")
    print(f"🚀 CUDA可用: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"📱 GPU设备: {torch.cuda.get_device_name(0)}")
        print(f"💾 GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

def test_sentence_transformers():
    """测试sentence-transformers"""
    print("\n🤖 测试sentence-transformers...")
    try:
        # 使用轻量级模型进行测试
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # 测试编码
        sentences = ["这是一个测试句子", "This is a test sentence"]
        embeddings = model.encode(sentences)
        
        print(f"✅ 模型加载成功")
        print(f"📏 向量维度: {embeddings.shape[1]}")
        print(f"📊 向量形状: {embeddings.shape}")
        
        return True
    except Exception as e:
        print(f"❌ sentence-transformers测试失败: {e}")
        return False

def test_qdrant():
    """测试Qdrant连接"""
    print("\n🗄️ 测试Qdrant连接...")
    try:
        client = QdrantClient(host="localhost", port=6333)
        
        # 获取集合信息
        collections = client.get_collections()
        print(f"✅ Qdrant连接成功")
        print(f"📚 现有集合数量: {len(collections.collections)}")
        
        return True
    except Exception as e:
        print(f"❌ Qdrant连接失败: {e}")
        print("💡 请确保Qdrant服务正在运行: docker-compose up -d qdrant")
        return False

if __name__ == "__main__":
    print("🔍 环境测试开始...")
    
    test_torch()
    st_ok = test_sentence_transformers()
    qdrant_ok = test_qdrant()
    
    if st_ok and qdrant_ok:
        print("\n🎉 环境测试通过！可以开始实验了！")
    else:
        print("\n⚠️ 环境测试未完全通过，请检查相关配置")
```

**运行测试：**
```bash
python test_environment.py
```

---

## 🔬 实验一：sentence-transformers基础使用

### 1.1 创建向量化模块

**创建 `src/embedding/embedder.py`：**
```python
"""文本向量化模块"""

from typing import List, Union, Optional, Dict, Any
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import logging
from pathlib import Path
import json

logger = logging.getLogger(__name__)

class TextEmbedder:
    """文本向量化器"""
    
    def __init__(self, 
                 model_name: str = 'BAAI/bge-m3',
                 device: Optional[str] = None,
                 max_seq_length: int = 512,
                 batch_size: int = 32):
        """
        初始化向量化器
        
        Args:
            model_name: 模型名称
            device: 设备类型 ('cuda', 'cpu', None为自动选择)
            max_seq_length: 最大序列长度
            batch_size: 批处理大小
        """
        self.model_name = model_name
        self.batch_size = batch_size
        
        # 设备选择
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
        
        logger.info(f"使用设备: {self.device}")
        
        # 加载模型
        self.model = self._load_model(max_seq_length)
        
        # 模型信息
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"模型: {model_name}, 向量维度: {self.embedding_dim}")
    
    def _load_model(self, max_seq_length: int) -> SentenceTransformer:
        """加载sentence-transformers模型"""
        try:
            logger.info(f"正在加载模型: {self.model_name}")
            
            model = SentenceTransformer(self.model_name, device=self.device)
            
            # 设置最大序列长度
            model.max_seq_length = max_seq_length
            
            logger.info(f"模型加载成功，最大序列长度: {max_seq_length}")
            return model
            
        except Exception as e:
            logger.error(f"模型加载失败: {e}")
            # 回退到轻量级模型
            logger.info("回退到轻量级模型: all-MiniLM-L6-v2")
            model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
            model.max_seq_length = max_seq_length
            return model
    
    def encode(self, 
               texts: Union[str, List[str]], 
               show_progress: bool = True,
               normalize_embeddings: bool = True) -> np.ndarray:
        """
        对文本进行向量化编码
        
        Args:
            texts: 单个文本或文本列表
            show_progress: 是否显示进度条
            normalize_embeddings: 是否归一化向量
            
        Returns:
            向量数组
        """
        if isinstance(texts, str):
            texts = [texts]
        
        logger.info(f"开始向量化 {len(texts)} 个文本")
        
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                show_progress_bar=show_progress,
                normalize_embeddings=normalize_embeddings,
                convert_to_numpy=True
            )
            
            logger.info(f"向量化完成，形状: {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            logger.error(f"向量化失败: {e}")
            raise
    
    def encode_batch(self, 
                    texts: List[str], 
                    batch_size: Optional[int] = None) -> np.ndarray:
        """
        批量向量化（手动控制批次）
        
        Args:
            texts: 文本列表
            batch_size: 批次大小
            
        Returns:
            向量数组
        """
        if batch_size is None:
            batch_size = self.batch_size
        
        embeddings_list = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="向量化进度"):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(
                batch_texts,
                show_progress_bar=False,
                normalize_embeddings=True,
                convert_to_numpy=True
            )
            embeddings_list.append(batch_embeddings)
        
        # 合并所有批次的结果
        all_embeddings = np.vstack(embeddings_list)
        logger.info(f"批量向量化完成，总形状: {all_embeddings.shape}")
        
        return all_embeddings
    
    def similarity(self, 
                  embeddings1: np.ndarray, 
                  embeddings2: np.ndarray,
                  metric: str = 'cosine') -> np.ndarray:
        """
        计算向量相似度
        
        Args:
            embeddings1: 第一组向量
            embeddings2: 第二组向量
            metric: 相似度度量 ('cosine', 'euclidean', 'dot')
            
        Returns:
            相似度矩阵
        """
        if metric == 'cosine':
            # 余弦相似度
            from sklearn.metrics.pairwise import cosine_similarity
            return cosine_similarity(embeddings1, embeddings2)
        
        elif metric == 'euclidean':
            # 欧几里得距离（转换为相似度）
            from sklearn.metrics.pairwise import euclidean_distances
            distances = euclidean_distances(embeddings1, embeddings2)
            return 1 / (1 + distances)  # 转换为相似度
        
        elif metric == 'dot':
            # 点积相似度
            return np.dot(embeddings1, embeddings2.T)
        
        else:
            raise ValueError(f"不支持的相似度度量: {metric}")
    
    def save_embeddings(self, 
                       embeddings: np.ndarray, 
                       texts: List[str],
                       output_file: Path,
                       metadata: Optional[Dict[str, Any]] = None):
        """
        保存向量化结果
        
        Args:
            embeddings: 向量数组
            texts: 对应的文本列表
            output_file: 输出文件路径
            metadata: 额外的元数据
        """
        data = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'num_texts': len(texts),
            'embeddings': embeddings.tolist(),
            'texts': texts,
            'metadata': metadata or {}
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"向量化结果已保存到: {output_file}")
    
    def load_embeddings(self, input_file: Path) -> tuple[np.ndarray, List[str], Dict[str, Any]]:
        """
        加载向量化结果
        
        Args:
            input_file: 输入文件路径
            
        Returns:
            (向量数组, 文本列表, 元数据)
        """
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        embeddings = np.array(data['embeddings'])
        texts = data['texts']
        metadata = data.get('metadata', {})
        
        logger.info(f"从 {input_file} 加载了 {len(texts)} 个向量")
        return embeddings, texts, metadata
    
    def get_model_info(self) -> Dict[str, Any]:
        """获取模型信息"""
        return {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'device': self.device,
            'max_seq_length': self.model.max_seq_length,
            'batch_size': self.batch_size
        }
```

### 1.2 创建向量化测试脚本

**创建 `test_embedding.py`：**
```python
"""向量化功能测试"""

from pathlib import Path
import numpy as np
from src.embedding.embedder import TextEmbedder

def test_basic_embedding():
    """测试基础向量化功能"""
    print("🔬 测试基础向量化功能...")
    
    # 创建向量化器
    embedder = TextEmbedder(
        model_name='all-MiniLM-L6-v2',  # 使用轻量级模型进行测试
        batch_size=4
    )
    
    # 测试文本
    test_texts = [
        "人工智能是计算机科学的一个分支",
        "机器学习是人工智能的重要组成部分",
        "深度学习基于神经网络",
        "自然语言处理处理人类语言",
        "计算机视觉处理图像和视频"
    ]
    
    # 向量化
    embeddings = embedder.encode(test_texts)
    
    print(f"📊 向量化结果:")
    print(f"  文本数量: {len(test_texts)}")
    print(f"  向量维度: {embeddings.shape[1]}")
    print(f"  向量形状: {embeddings.shape}")
    
    # 计算相似度
    similarities = embedder.similarity(embeddings, embeddings)
    
    print(f"\n🔍 相似度分析:")
    for i, text1 in enumerate(test_texts):
        print(f"\n文本 {i+1}: {text1[:30]}...")
        
        # 找到最相似的文本（除了自己）
        sim_scores = similarities[i]
        sim_scores[i] = -1  # 排除自己
        most_similar_idx = np.argmax(sim_scores)
        
        print(f"  最相似: {test_texts[most_similar_idx][:30]}...")
        print(f"  相似度: {sim_scores[most_similar_idx]:.3f}")
    
    # 保存结果
    output_file = Path("test_embeddings.json")
    embedder.save_embeddings(embeddings, test_texts, output_file)
    
    # 加载测试
    loaded_embeddings, loaded_texts, metadata = embedder.load_embeddings(output_file)
    print(f"\n💾 加载测试:")
    print(f"  加载向量形状: {loaded_embeddings.shape}")
    print(f"  加载文本数量: {len(loaded_texts)}")
    
    # 清理测试文件
    if output_file.exists():
        output_file.unlink()
    
    return embedder

def test_batch_processing():
    """测试批量处理"""
    print("\n🔄 测试批量处理...")
    
    embedder = TextEmbedder(
        model_name='all-MiniLM-L6-v2',
        batch_size=2
    )
    
    # 生成更多测试文本
    test_texts = []
    topics = ["人工智能", "机器学习", "深度学习", "自然语言处理", "计算机视觉"]
    
    for i, topic in enumerate(topics):
        for j in range(3):
            test_texts.append(f"{topic}是一个重要的技术领域，第{j+1}个相关描述。")
    
    print(f"📝 生成了 {len(test_texts)} 个测试文本")
    
    # 批量向量化
    embeddings = embedder.encode_batch(test_texts, batch_size=3)
    
    print(f"📊 批量向量化结果:")
    print(f"  向量形状: {embeddings.shape}")
    
    # 分析每个主题的向量
    for i, topic in enumerate(topics):
        topic_embeddings = embeddings[i*3:(i+1)*3]
        
        # 计算主题内相似度
        topic_similarities = embedder.similarity(topic_embeddings, topic_embeddings)
        avg_similarity = np.mean(topic_similarities[np.triu_indices_from(topic_similarities, k=1)])
        
        print(f"  {topic} 主题内平均相似度: {avg_similarity:.3f}")

def test_different_models():
    """测试不同模型"""
    print("\n🤖 测试不同模型...")
    
    models = [
        'all-MiniLM-L6-v2',
        'paraphrase-MiniLM-L6-v2'
    ]
    
    test_text = "人工智能正在改变我们的世界"
    
    for model_name in models:
        try:
            print(f"\n测试模型: {model_name}")
            embedder = TextEmbedder(model_name=model_name)
            
            embedding = embedder.encode([test_text])
            
            print(f"  向量维度: {embedding.shape[1]}")
            print(f"  向量范数: {np.linalg.norm(embedding[0]):.3f}")
            print(f"  向量前5维: {embedding[0][:5]}")
            
        except Exception as e:
            print(f"  ❌ 模型 {model_name} 测试失败: {e}")

if __name__ == "__main__":
    print("开始向量化测试...")
    
    try:
        embedder = test_basic_embedding()
        test_batch_processing()
        test_different_models()
        
        print("\n🎉 所有测试完成！")
        
        # 显示模型信息
        print(f"\n📋 模型信息:")
        info = embedder.get_model_info()
        for key, value in info.items():
            print(f"  {key}: {value}")
            
    except Exception as e:
        print(f"\n💥 测试过程中出现错误: {e}")
```

**运行测试：**
```bash
python test_embedding.py
```

---

## 🔬 实验二：Qdrant向量数据库操作

### 2.1 创建Qdrant客户端模块

**创建 `src/vector_store/qdrant_client.py`：**
```python
"""Qdrant向量数据库客户端"""

from typing import List, Dict, Any, Optional, Union
import uuid
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, Filter, 
    FieldCondition, MatchValue, SearchRequest
)
from qdrant_client.http.exceptions import ResponseHandlingException
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """搜索结果"""
    id: str
    score: float
    payload: Dict[str, Any]
    vector: Optional[np.ndarray] = None

class QdrantVectorStore:
    """Qdrant向量存储客户端"""
    
    def __init__(self, 
                 host: str = "localhost", 
                 port: int = 6333,
                 timeout: int = 60):
        """
        初始化Qdrant客户端
        
        Args:
            host: Qdrant服务器地址
            port: Qdrant服务器端口
            timeout: 连接超时时间
        """
        self.host = host
        self.port = port
        
        try:
            self.client = QdrantClient(
                host=host, 
                port=port, 
                timeout=timeout
            )
            
            # 测试连接
            self.client.get_collections()
            logger.info(f"Qdrant客户端连接成功: {host}:{port}")
            
        except Exception as e:
            logger.error(f"Qdrant连接失败: {e}")
            raise
    
    def create_collection(self, 
                         collection_name: str, 
                         vector_size: int,
                         distance: str = "Cosine",
                         recreate: bool = False) -> bool:
        """
        创建向量集合
        
        Args:
            collection_name: 集合名称
            vector_size: 向量维度
            distance: 距离度量 ("Cosine", "Euclidean", "Dot")
            recreate: 是否重新创建（删除已存在的）
            
        Returns:
            是否创建成功
        """
        try:
            # 检查集合是否存在
            collections = self.client.get_collections()
            collection_exists = any(
                col.name == collection_name 
                for col in collections.collections
            )
            
            if collection_exists:
                if recreate:
                    logger.info(f"删除已存在的集合: {collection_name}")
                    self.client.delete_collection(collection_name)
                else:
                    logger.info(f"集合已存在: {collection_name}")
                    return True
            
            # 距离度量映射
            distance_map = {
                "Cosine": Distance.COSINE,
                "Euclidean": Distance.EUCLID,
                "Dot": Distance.DOT
            }
            
            if distance not in distance_map:
                raise ValueError(f"不支持的距离度量: {distance}")
            
            # 创建集合
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=distance_map[distance]
                )
            )
            
            logger.info(f"集合创建成功: {collection_name} (维度: {vector_size}, 距离: {distance})")
            return True
            
        except Exception as e:
            logger.error(f"创建集合失败: {e}")
            return False
    
    def insert_vectors(self, 
                      collection_name: str,
                      vectors: np.ndarray,
                      payloads: List[Dict[str, Any]],
                      ids: Optional[List[str]] = None) -> bool:
        """
        插入向量
        
        Args:
            collection_name: 集合名称
            vectors: 向量数组
            payloads: 元数据列表
            ids: 向量ID列表（可选，自动生成UUID）
            
        Returns:
            是否插入成功
        """
        try:
            if len(vectors) != len(payloads):
                raise ValueError("向量数量与元数据数量不匹配")
            
            # 生成ID（如果未提供）
            if ids is None:
                ids = [str(uuid.uuid4()) for _ in range(len(vectors))]
            
            # 创建点结构
            points = []
            for i, (vector, payload) in enumerate(zip(vectors, payloads)):
                point = PointStruct(
                    id=ids[i],
                    vector=vector.tolist() if isinstance(vector, np.ndarray) else vector,
                    payload=payload
                )
                points.append(point)
            
            # 批量插入
            self.client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            logger.info(f"成功插入 {len(points)} 个向量到集合 {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"插入向量失败: {e}")
            return False
    
    def search(self, 
              collection_name: str,
              query_vector: np.ndarray,
              limit: int = 10,
              score_threshold: Optional[float] = None,
              filter_conditions: Optional[Dict[str, Any]] = None) -> List[SearchResult]:
        """
        向量相似度搜索
        
        Args:
            collection_name: 集合名称
            query_vector: 查询向量
            limit: 返回结果数量
            score_threshold: 分数阈值
            filter_conditions: 过滤条件
            
        Returns:
            搜索结果列表
        """
        try:
            # 构建过滤器
            search_filter = None
            if filter_conditions:
                conditions = []
                for key, value in filter_conditions.items():
                    condition = FieldCondition(
                        key=key,
                        match=MatchValue(value=value)
                    )
                    conditions.append(condition)
                
                if conditions:
                    search_filter = Filter(must=conditions)
            
            # 执行搜索
            search_results = self.client.search(
                collection_name=collection_name,
                query_vector=query_vector.tolist() if isinstance(query_vector, np.ndarray) else query_vector,
                limit=limit,
                score_threshold=score_threshold,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False
            )
            
            # 转换结果格式
            results = []
            for result in search_results:
                search_result = SearchResult(
                    id=str(result.id),
                    score=result.score,
                    payload=result.payload or {}
                )
                results.append(search_result)
            
            logger.info(f"搜索完成，返回 {len(results)} 个结果")
            return results
            
        except Exception as e:
            logger.error(f"搜索失败: {e}")
            return []
    
    def get_collection_info(self, collection_name: str) -> Optional[Dict[str, Any]]:
        """
        获取集合信息
        
        Args:
            collection_name: 集合名称
            
        Returns:
            集合信息字典
        """
        try:
            info = self.client.get_collection(collection_name)
            
            return {
                'name': collection_name,
                'vectors_count': info.vectors_count,
                'indexed_vectors_count': info.indexed_vectors_count,
                'points_count': info.points_count,
                'segments_count': info.segments_count,
                'config': {
                    'vector_size': info.config.params.vectors.size,
                    'distance': info.config.params.vectors.distance.name
                },
                'status': info.status.name
            }
            
        except Exception as e:
            logger.error(f"获取集合信息失败: {e}")
            return None
    
    def delete_collection(self, collection_name: str) -> bool:
        """
        删除集合
        
        Args:
            collection_name: 集合名称
            
        Returns:
            是否删除成功
        """
        try:
            self.client.delete_collection(collection_name)
            logger.info(f"集合删除成功: {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"删除集合失败: {e}")
            return False
    
    def list_collections(self) -> List[str]:
        """
        列出所有集合
        
        Returns:
            集合名称列表
        """
        try:
            collections = self.client.get_collections()
            return [col.name for col in collections.collections]
            
        except Exception as e:
            logger.error(f"获取集合列表失败: {e}")
            return []
    
    def count_points(self, collection_name: str) -> int:
        """
        统计集合中的点数量
        
        Args:
            collection_name: 集合名称
            
        Returns:
            点数量
        """
        try:
            info = self.client.get_collection(collection_name)
            return info.points_count
            
        except Exception as e:
            logger.error(f"统计点数量失败: {e}")
            return 0

### 2.2 创建Qdrant测试脚本

**创建 `test_qdrant.py`：**
```python
"""Qdrant向量数据库测试"""

import numpy as np
from pathlib import Path
from src.vector_store.qdrant_client import QdrantVectorStore
from src.embedding.embedder import TextEmbedder

def test_qdrant_basic():
    """测试Qdrant基础功能"""
    print("🗄️ 测试Qdrant基础功能...")
    
    # 创建客户端
    vector_store = QdrantVectorStore()
    
    # 列出现有集合
    collections = vector_store.list_collections()
    print(f"📚 现有集合: {collections}")
    
    # 创建测试集合
    collection_name = "test_collection"
    vector_size = 384  # all-MiniLM-L6-v2的向量维度
    
    success = vector_store.create_collection(
        collection_name=collection_name,
        vector_size=vector_size,
        distance="Cosine",
        recreate=True
    )
    
    if not success:
        print("❌ 集合创建失败")
        return None
    
    # 获取集合信息
    info = vector_store.get_collection_info(collection_name)
    if info:
        print(f"📋 集合信息:")
        for key, value in info.items():
            print(f"  {key}: {value}")
    
    return vector_store, collection_name

def test_vector_operations():
    """测试向量操作"""
    print("\n🔄 测试向量操作...")
    
    # 初始化
    vector_store, collection_name = test_qdrant_basic()
    if not vector_store:
        return
    
    # 创建向量化器
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
    
    # 测试文档
    documents = [
        {
            "text": "人工智能是模拟人类智能的技术",
            "category": "AI",
            "source": "doc1"
        },
        {
            "text": "机器学习是人工智能的一个分支",
            "category": "ML", 
            "source": "doc2"
        },
        {
            "text": "深度学习使用神经网络进行学习",
            "category": "DL",
            "source": "doc3"
        },
        {
            "text": "自然语言处理处理人类语言",
            "category": "NLP",
            "source": "doc4"
        },
        {
            "text": "计算机视觉分析图像和视频",
            "category": "CV",
            "source": "doc5"
        }
    ]
    
    # 向量化文档
    texts = [doc["text"] for doc in documents]
    vectors = embedder.encode(texts)
    
    print(f"📊 向量化完成: {vectors.shape}")
    
    # 准备元数据
    payloads = []
    for i, doc in enumerate(documents):
        payload = {
            "text": doc["text"],
            "category": doc["category"],
            "source": doc["source"],
            "length": len(doc["text"]),
            "index": i
        }
        payloads.append(payload)
    
    # 插入向量
    success = vector_store.insert_vectors(
        collection_name=collection_name,
        vectors=vectors,
        payloads=payloads
    )
    
    if not success:
        print("❌ 向量插入失败")
        return
    
    # 验证插入
    point_count = vector_store.count_points(collection_name)
    print(f"✅ 成功插入 {point_count} 个向量")
    
    return vector_store, collection_name, embedder, documents

def test_vector_search():
    """测试向量搜索"""
    print("\n🔍 测试向量搜索...")
    
    # 初始化数据
    result = test_vector_operations()
    if not result:
        return
    
    vector_store, collection_name, embedder, documents = result
    
    # 测试查询
    queries = [
        "什么是人工智能？",
        "机器学习算法",
        "神经网络深度学习",
        "语言模型处理"
    ]
    
    for query in queries:
        print(f"\n🔎 查询: {query}")
        
        # 向量化查询
        query_vector = embedder.encode([query])[0]
        
        # 搜索相似向量
        results = vector_store.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=3,
            score_threshold=0.3
        )
        
        print(f"📋 找到 {len(results)} 个相关结果:")
        for i, result in enumerate(results, 1):
            print(f"  {i}. [{result.score:.3f}] {result.payload['text']}")
            print(f"     类别: {result.payload['category']}, 来源: {result.payload['source']}")
    
    # 测试过滤搜索
    print(f"\n🎯 测试过滤搜索 (category='AI'):")
    query_vector = embedder.encode(["人工智能技术"])[0]
    
    filtered_results = vector_store.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=5,
        filter_conditions={"category": "AI"}
    )
    
    print(f"📋 过滤结果 ({len(filtered_results)} 个):")
    for result in filtered_results:
        print(f"  [{result.score:.3f}] {result.payload['text']}")
    
    # 清理测试集合
    print(f"\n🧹 清理测试集合...")
    vector_store.delete_collection(collection_name)
    print(f"✅ 集合 {collection_name} 已删除")

def test_performance():
    """测试性能"""
    print("\n⚡ 测试性能...")
    
    import time
    
    vector_store = QdrantVectorStore()
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
    
    collection_name = "performance_test"
    vector_size = 384
    
    # 创建集合
    vector_store.create_collection(
        collection_name=collection_name,
        vector_size=vector_size,
        recreate=True
    )
    
    # 生成测试数据
    num_docs = 100
    test_docs = []
    for i in range(num_docs):
        text = f"这是第{i+1}个测试文档，包含一些随机内容用于性能测试。文档编号：{i+1}"
        test_docs.append({
            "text": text,
            "doc_id": f"doc_{i+1}",
            "category": f"cat_{i % 5}"
        })
    
    # 向量化性能测试
    print(f"📊 向量化 {num_docs} 个文档...")
    start_time = time.time()
    
    texts = [doc["text"] for doc in test_docs]
    vectors = embedder.encode_batch(texts, batch_size=16)
    
    embedding_time = time.time() - start_time
    print(f"⏱️ 向量化耗时: {embedding_time:.2f}秒 ({num_docs/embedding_time:.1f} docs/sec)")
    
    # 插入性能测试
    print(f"💾 插入 {num_docs} 个向量...")
    start_time = time.time()
    
    payloads = [{
        "text": doc["text"],
        "doc_id": doc["doc_id"],
        "category": doc["category"]
    } for doc in test_docs]
    
    vector_store.insert_vectors(
        collection_name=collection_name,
        vectors=vectors,
        payloads=payloads
    )
    
    insert_time = time.time() - start_time
    print(f"⏱️ 插入耗时: {insert_time:.2f}秒 ({num_docs/insert_time:.1f} docs/sec)")
    
    # 搜索性能测试
    print(f"🔍 搜索性能测试...")
    query_text = "测试文档内容"
    query_vector = embedder.encode([query_text])[0]
    
    # 多次搜索测试
    search_times = []
    for _ in range(10):
        start_time = time.time()
        
        results = vector_store.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=10
        )
        
        search_time = time.time() - start_time
        search_times.append(search_time)
    
    avg_search_time = np.mean(search_times)
    print(f"⏱️ 平均搜索耗时: {avg_search_time*1000:.1f}ms")
    print(f"📊 搜索结果数量: {len(results)}")
    
    # 清理
    vector_store.delete_collection(collection_name)
    print(f"✅ 性能测试完成")

if __name__ == "__main__":
    print("开始Qdrant测试...")
    
    try:
        test_vector_search()
        test_performance()
        
        print("\n🎉 所有Qdrant测试完成！")
        
    except Exception as e:
        print(f"\n💥 测试过程中出现错误: {e}")
        import traceback
        traceback.print_exc()
```

**运行测试：**
```bash
python test_qdrant.py
```

---

## 🔬 实验三：批量文档向量化与入库

### 3.1 创建文档向量化管理器

**创建 `src/vector_store/document_vectorizer.py`：**
```python
"""文档向量化与入库管理器"""

from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import json
import hashlib
from datetime import datetime
from tqdm import tqdm
import logging

from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore
from src.chunking.chunk_manager import ChunkManager
from src.document.document_manager import DocumentManager

logger = logging.getLogger(__name__)

class DocumentVectorizer:
    """文档向量化与入库管理器"""
    
    def __init__(self,
                 embedder: TextEmbedder,
                 vector_store: QdrantVectorStore,
                 collection_name: str = "documents"):
        """
        初始化文档向量化器
        
        Args:
            embedder: 文本向量化器
            vector_store: 向量存储客户端
            collection_name: 集合名称
        """
        self.embedder = embedder
        self.vector_store = vector_store
        self.collection_name = collection_name
        
        # 初始化其他组件
        self.doc_manager = DocumentManager()
        self.chunk_manager = ChunkManager()
        
        # 确保集合存在
        self._ensure_collection()
    
    def _ensure_collection(self):
        """确保向量集合存在"""
        collections = self.vector_store.list_collections()
        
        if self.collection_name not in collections:
            logger.info(f"创建向量集合: {self.collection_name}")
            
            success = self.vector_store.create_collection(
                collection_name=self.collection_name,
                vector_size=self.embedder.embedding_dim,
                distance="Cosine"
            )
            
            if not success:
                raise RuntimeError(f"无法创建集合: {self.collection_name}")
        else:
            logger.info(f"使用现有集合: {self.collection_name}")
    
    def _generate_chunk_id(self, file_path: str, chunk_index: int) -> str:
        """生成chunk的唯一ID"""
        content = f"{file_path}_{chunk_index}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def process_document(self, 
                        file_path: Path,
                        chunk_strategy: str = "sentence",
                        chunk_size: int = 500,
                        chunk_overlap: int = 50) -> Dict[str, Any]:
        """
        处理单个文档：解析 -> 分块 -> 向量化 -> 入库
        
        Args:
            file_path: 文档路径
            chunk_strategy: 分块策略
            chunk_size: 分块大小
            chunk_overlap: 分块重叠
            
        Returns:
            处理结果统计
        """
        logger.info(f"开始处理文档: {file_path}")
        
        try:
            # 1. 解析文档
            doc_content = self.doc_manager.parse_document(file_path)
            if not doc_content:
                return {"error": "文档解析失败"}
            
            # 2. 分块处理
            chunks = self.chunk_manager.chunk_text(
                text=doc_content.content,
                strategy=chunk_strategy,
                chunk_size=chunk_size,
                overlap_size=chunk_overlap
            )
            
            if not chunks:
                return {"error": "文档分块失败"}
            
            # 3. 向量化
            chunk_texts = [chunk.content for chunk in chunks]
            vectors = self.embedder.encode(chunk_texts, show_progress=False)
            
            # 4. 准备元数据
            payloads = []
            chunk_ids = []
            
            for i, chunk in enumerate(chunks):
                chunk_id = self._generate_chunk_id(str(file_path), i)
                chunk_ids.append(chunk_id)
                
                payload = {
                    "chunk_id": chunk_id,
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "chunk_index": i,
                    "content": chunk.content,
                    "content_length": len(chunk.content),
                    "chunk_strategy": chunk_strategy,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "processed_at": datetime.now().isoformat(),
                    "doc_metadata": {
                        "title": doc_content.metadata.title,
                        "author": doc_content.metadata.author,
                        "page_count": doc_content.metadata.page_count,
                        "file_size": doc_content.metadata.file_size
                    }
                }
                
                # 添加chunk元数据
                if hasattr(chunk, 'metadata') and chunk.metadata:
                    payload["chunk_metadata"] = chunk.metadata.__dict__
                
                payloads.append(payload)
            
            # 5. 入库
            success = self.vector_store.insert_vectors(
                collection_name=self.collection_name,
                vectors=vectors,
                payloads=payloads,
                ids=chunk_ids
            )
            
            if not success:
                return {"error": "向量入库失败"}
            
            # 返回处理结果
            result = {
                "file_path": str(file_path),
                "file_name": file_path.name,
                "chunks_count": len(chunks),
                "vectors_count": len(vectors),
                "total_content_length": len(doc_content.content),
                "avg_chunk_length": sum(len(chunk.content) for chunk in chunks) / len(chunks),
                "chunk_strategy": chunk_strategy,
                "processed_at": datetime.now().isoformat(),
                "success": True
            }
            
            logger.info(f"文档处理完成: {file_path.name} ({len(chunks)} chunks)")
            return result
            
        except Exception as e:
            logger.error(f"处理文档失败 {file_path}: {e}")
            return {
                "file_path": str(file_path),
                "error": str(e),
                "success": False
            }
    
    def process_directory(self, 
                         directory: Path,
                         chunk_strategy: str = "sentence",
                         chunk_size: int = 500,
                         chunk_overlap: int = 50,
                         file_patterns: List[str] = None) -> Dict[str, Any]:
        """
        批量处理目录中的文档
        
        Args:
            directory: 目录路径
            chunk_strategy: 分块策略
            chunk_size: 分块大小
            chunk_overlap: 分块重叠
            file_patterns: 文件模式列表
            
        Returns:
            批量处理结果
        """
        if file_patterns is None:
            file_patterns = ['*.pdf', '*.docx', '*.txt', '*.md']
        
        # 收集文件
        files = []
        for pattern in file_patterns:
            files.extend(directory.glob(pattern))
        
        if not files:
            return {
                "error": f"目录中未找到匹配的文件: {directory}",
                "patterns": file_patterns
            }
        
        logger.info(f"开始批量处理 {len(files)} 个文件")
        
        # 处理统计
        results = []
        success_count = 0
        total_chunks = 0
        total_vectors = 0
        
        # 逐个处理文件
        for file_path in tqdm(files, desc="处理文档"):
            result = self.process_document(
                file_path=file_path,
                chunk_strategy=chunk_strategy,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            results.append(result)
            
            if result.get("success", False):
                success_count += 1
                total_chunks += result.get("chunks_count", 0)
                total_vectors += result.get("vectors_count", 0)
        
        # 汇总结果
        summary = {
            "directory": str(directory),
            "total_files": len(files),
            "success_files": success_count,
            "failed_files": len(files) - success_count,
            "total_chunks": total_chunks,
            "total_vectors": total_vectors,
            "chunk_strategy": chunk_strategy,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "processed_at": datetime.now().isoformat(),
            "file_results": results
        }
        
        logger.info(f"批量处理完成: {success_count}/{len(files)} 成功")
        return summary
    
    def search_documents(self, 
                        query: str,
                        limit: int = 10,
                        score_threshold: float = 0.5,
                        file_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        搜索相关文档
        
        Args:
            query: 查询文本
            limit: 返回结果数量
            score_threshold: 分数阈值
            file_filter: 文件过滤（文件名包含）
            
        Returns:
            搜索结果列表
        """
        # 向量化查询
        query_vector = self.embedder.encode([query])[0]
        
        # 构建过滤条件
        filter_conditions = {}
        if file_filter:
            # 注意：这里简化了过滤逻辑，实际应用中可能需要更复杂的过滤
            pass
        
        # 执行搜索
        search_results = self.vector_store.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit,
            score_threshold=score_threshold,
            filter_conditions=filter_conditions if filter_conditions else None
        )
        
        # 格式化结果
        formatted_results = []
        for result in search_results:
            formatted_result = {
                "chunk_id": result.id,
                "score": result.score,
                "content": result.payload.get("content", ""),
                "file_name": result.payload.get("file_name", ""),
                "file_path": result.payload.get("file_path", ""),
                "chunk_index": result.payload.get("chunk_index", 0),
                "content_length": result.payload.get("content_length", 0),
                "doc_metadata": result.payload.get("doc_metadata", {})
            }
            formatted_results.append(formatted_result)
        
        logger.info(f"搜索完成: 查询='{query}', 结果={len(formatted_results)}个")
        return formatted_results
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        获取集合统计信息
        
        Returns:
            统计信息字典
        """
        info = self.vector_store.get_collection_info(self.collection_name)
        if not info:
            return {"error": "无法获取集合信息"}
        
        return {
            "collection_name": self.collection_name,
            "total_vectors": info["vectors_count"],
            "total_points": info["points_count"],
            "vector_dimension": info["config"]["vector_size"],
            "distance_metric": info["config"]["distance"],
            "status": info["status"]
        }
    
    def save_processing_log(self, 
                           results: Dict[str, Any], 
                           output_file: Path):
        """
        保存处理日志
        
        Args:
            results: 处理结果
            output_file: 输出文件路径
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"处理日志已保存到: {output_file}")
```

### 3.2 创建批量向量化测试脚本

**创建 `test_batch_vectorization.py`：**
```python
"""批量文档向量化测试"""

from pathlib import Path
import json
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore
from src.vector_store.document_vectorizer import DocumentVectorizer

def create_test_documents():
    """创建测试文档"""
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # 创建测试文档
    documents = {
        "ai_overview.txt": """
人工智能概述

人工智能（Artificial Intelligence，AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。

主要领域包括：
1. 机器学习 - 让计算机从数据中学习
2. 深度学习 - 基于神经网络的学习方法
3. 自然语言处理 - 处理和理解人类语言
4. 计算机视觉 - 分析和理解图像
5. 机器人学 - 智能机器人系统

人工智能的发展历程可以分为几个阶段：
- 1950年代：AI概念提出
- 1960-1970年代：专家系统发展
- 1980-1990年代：机器学习兴起
- 2000年代至今：深度学习革命

当前AI技术在各个领域都有广泛应用，包括医疗、金融、教育、交通等。
        """,
        
        "machine_learning.txt": """
机器学习基础

机器学习是人工智能的一个重要分支，它使计算机能够在没有明确编程的情况下学习和改进。

主要类型：
1. 监督学习
   - 分类问题
   - 回归问题
   - 常用算法：线性回归、决策树、随机森林、SVM

2. 无监督学习
   - 聚类分析
   - 降维技术
   - 常用算法：K-means、PCA、t-SNE

3. 强化学习
   - 智能体与环境交互
   - 奖励机制驱动学习
   - 应用：游戏AI、机器人控制

机器学习的工作流程：
1. 数据收集和预处理
2. 特征工程
3. 模型选择和训练
4. 模型评估和优化
5. 模型部署和监控

评估指标包括准确率、精确率、召回率、F1分数等。
        """,
        
        "deep_learning.txt": """
深度学习技术

深度学习是机器学习的一个子领域，基于人工神经网络，特别是深层神经网络。

核心概念：
1. 神经网络
   - 感知机
   - 多层感知机
   - 激活函数：ReLU、Sigmoid、Tanh

2. 深度网络架构
   - 卷积神经网络（CNN）- 图像处理
   - 循环神经网络（RNN）- 序列数据
   - 长短期记忆网络（LSTM）- 长序列建模
   - Transformer - 注意力机制

3. 训练技术
   - 反向传播算法
   - 梯度下降优化
   - 正则化技术：Dropout、Batch Normalization
   - 学习率调度

深度学习的应用领域：
- 计算机视觉：图像分类、目标检测、图像生成
- 自然语言处理：机器翻译、文本生成、情感分析
- 语音识别：语音转文本、语音合成
- 推荐系统：个性化推荐

主要框架：TensorFlow、PyTorch、Keras等。
        """,
        
        "nlp_basics.txt": """
自然语言处理基础

自然语言处理（Natural Language Processing，NLP）是人工智能的一个分支，专注于计算机与人类语言之间的交互。

主要任务：
1. 文本预处理
   - 分词（Tokenization）
   - 词性标注（POS Tagging）
   - 命名实体识别（NER）
   - 句法分析

2. 语义理解
   - 词向量表示：Word2Vec、GloVe、FastText
   - 句子表示：BERT、GPT、T5
   - 语义相似度计算

3. 应用任务
   - 文本分类
   - 情感分析
   - 机器翻译
   - 问答系统
   - 文本摘要
   - 对话系统

技术发展历程：
- 规则基础方法
- 统计方法
- 机器学习方法
- 深度学习方法
- 预训练语言模型

当前热点：
- 大语言模型（LLM）
- 检索增强生成（RAG）
- 多模态理解
- 代码生成
        """
    }
    
    # 写入文件
    for filename, content in documents.items():
        file_path = test_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
    
    print(f"✅ 创建了 {len(documents)} 个测试文档在 {test_dir}")
    return test_dir

def test_single_document_processing():
    """测试单个文档处理"""
    print("\n📄 测试单个文档处理...")
    
    # 创建测试文档
    test_dir = create_test_documents()
    
    # 初始化组件
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2', batch_size=8)
    vector_store = QdrantVectorStore()
    vectorizer = DocumentVectorizer(
        embedder=embedder,
        vector_store=vector_store,
        collection_name="test_docs"
    )
    
    # 处理单个文档
    test_file = test_dir / "ai_overview.txt"
    result = vectorizer.process_document(
        file_path=test_file,
        chunk_strategy="sentence",
        chunk_size=200,
        chunk_overlap=20
    )
    
    print(f"📊 处理结果:")
    for key, value in result.items():
        if key != "error":
            print(f"  {key}: {value}")
    
    # 测试搜索
    if result.get("success", False):
        print(f"\n🔍 测试搜索功能...")
        
        queries = [
            "什么是人工智能？",
            "机器学习的应用",
            "AI发展历程"
        ]
        
        for query in queries:
            print(f"\n查询: {query}")
            search_results = vectorizer.search_documents(
                query=query,
                limit=3,
                score_threshold=0.3
            )
            
            for i, result in enumerate(search_results, 1):
                print(f"  {i}. [{result['score']:.3f}] {result['content'][:100]}...")
    
    return vectorizer

def test_batch_processing():
    """测试批量处理"""
    print("\n📚 测试批量文档处理...")
    
    # 创建测试文档
    test_dir = create_test_documents()
    
    # 初始化组件
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2', batch_size=8)
    vector_store = QdrantVectorStore()
    vectorizer = DocumentVectorizer(
        embedder=embedder,
        vector_store=vector_store,
        collection_name="batch_test_docs"
    )
    
    # 批量处理
    batch_result = vectorizer.process_directory(
        directory=test_dir,
        chunk_strategy="sentence",
        chunk_size=300,
        chunk_overlap=30,
        file_patterns=['*.txt']
    )
    
    print(f"📊 批量处理结果:")
    print(f"  总文件数: {batch_result['total_files']}")
    print(f"  成功文件数: {batch_result['success_files']}")
    print(f"  失败文件数: {batch_result['failed_files']}")
    print(f"  总分块数: {batch_result['total_chunks']}")
    print(f"  总向量数: {batch_result['total_vectors']}")
    
    # 保存处理日志
    log_file = Path("batch_processing_log.json")
    vectorizer.save_processing_log(batch_result, log_file)
    
    # 测试综合搜索
    print(f"\n🔍 测试综合搜索...")
    
    comprehensive_queries = [
        "深度学习和机器学习的区别",
        "自然语言处理的主要任务",
        "神经网络的基本概念",
        "人工智能的应用领域"
    ]
    
    for query in comprehensive_queries:
        print(f"\n查询: {query}")
        search_results = vectorizer.search_documents(
            query=query,
            limit=5,
            score_threshold=0.2
        )
        
        print(f"找到 {len(search_results)} 个相关结果:")
        for i, result in enumerate(search_results, 1):
            print(f"  {i}. [{result['score']:.3f}] {result['file_name']} - {result['content'][:80]}...")
    
    # 获取集合统计
    stats = vectorizer.get_collection_stats()
    print(f"\n📈 集合统计信息:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # 清理测试文件
    import shutil
    if test_dir.exists():
        shutil.rmtree(test_dir)
    if log_file.exists():
        log_file.unlink()
    
    # 清理测试集合
    vector_store.delete_collection("batch_test_docs")
    
    return batch_result

def test_different_chunking_strategies():
    """测试不同分块策略的效果"""
    print("\n🔄 测试不同分块策略...")
    
    # 创建测试文档
    test_dir = create_test_documents()
    test_file = test_dir / "deep_learning.txt"
    
    strategies = [
        {"name": "sentence", "chunk_size": 200, "overlap": 20},
        {"name": "sentence", "chunk_size": 400, "overlap": 40},
        {"name": "sentence", "chunk_size": 600, "overlap": 60}
    ]
    
    results = []
    
    for strategy in strategies:
        print(f"\n测试策略: {strategy['name']} (size={strategy['chunk_size']}, overlap={strategy['overlap']})")
        
        # 初始化组件
        embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
        vector_store = QdrantVectorStore()
        
        collection_name = f"test_{strategy['name']}_{strategy['chunk_size']}"
        vectorizer = DocumentVectorizer(
            embedder=embedder,
            vector_store=vector_store,
            collection_name=collection_name
        )
        
        # 处理文档
        result = vectorizer.process_document(
            file_path=test_file,
            chunk_strategy=strategy['name'],
            chunk_size=strategy['chunk_size'],
            chunk_overlap=strategy['overlap']
        )
        
        if result.get("success", False):
            print(f"  分块数量: {result['chunks_count']}")
            print(f"  平均分块长度: {result['avg_chunk_length']:.1f}")
            
            # 测试搜索效果
            query = "什么是卷积神经网络？"
            search_results = vectorizer.search_documents(
                query=query,
                limit=3,
                score_threshold=0.2
            )
            
            print(f"  搜索结果数量: {len(search_results)}")
            if search_results:
                print(f"  最高分数: {search_results[0]['score']:.3f}")
            
            results.append({
                "strategy": strategy,
                "chunks_count": result['chunks_count'],
                "avg_chunk_length": result['avg_chunk_length'],
                "search_results_count": len(search_results),
                "best_score": search_results[0]['score'] if search_results else 0
            })
        
        # 清理集合
        vector_store.delete_collection(collection_name)
    
    # 比较结果
    print(f"\n📊 策略比较结果:")
    for result in results:
        strategy = result['strategy']
        print(f"  {strategy['name']}({strategy['chunk_size']}/{strategy['overlap']}): "
              f"chunks={result['chunks_count']}, "
              f"avg_len={result['avg_chunk_length']:.1f}, "
              f"results={result['search_results_count']}, "
              f"score={result['best_score']:.3f}")
    
    # 清理测试文件
    import shutil
    if test_dir.exists():
        shutil.rmtree(test_dir)

if __name__ == "__main__":
    print("开始批量向量化测试...")
    
    try:
        # 测试单个文档处理
        vectorizer = test_single_document_processing()
        
        # 清理单个文档测试的集合
        vectorizer.vector_store.delete_collection("test_docs")
        
        # 测试批量处理
        test_batch_processing()
        
        # 测试不同分块策略
        test_different_chunking_strategies()
        
        print("\n🎉 所有批量向量化测试完成！")
        
    except Exception as e:
        print(f"\n💥 测试过程中出现错误: {e}")
        import traceback
        traceback.print_exc()
```

**运行测试：**
```bash
python test_batch_vectorization.py
```

---

## 🔬 实验四：性能优化与监控

### 4.1 创建性能监控工具

**创建 `src/utils/performance_monitor.py`：**
```python
"""性能监控工具"""

import time
import psutil
import threading
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import json
from pathlib import Path

@dataclass
class PerformanceMetrics:
    """性能指标"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    gpu_memory_mb: Optional[float] = None
    operation_time: Optional[float] = None
    operation_name: Optional[str] = None

class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self, monitor_interval: float = 1.0):
        """
        初始化性能监控器
        
        Args:
            monitor_interval: 监控间隔（秒）
        """
        self.monitor_interval = monitor_interval
        self.metrics: List[PerformanceMetrics] = []
        self.monitoring = False
        self.monitor_thread = None
        
        # 检查GPU可用性
        self.gpu_available = self._check_gpu_available()
    
    def _check_gpu_available(self) -> bool:
        """检查GPU是否可用"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _get_gpu_memory(self) -> Optional[float]:
        """获取GPU内存使用情况"""
        if not self.gpu_available:
            return None
        
        try:
            import torch
            if torch.cuda.is_available():
                return torch.cuda.memory_allocated() / 1024 / 1024  # MB
        except Exception:
            pass
        
        return None
    
    def _collect_metrics(self, operation_name: Optional[str] = None, 
                        operation_time: Optional[float] = None) -> PerformanceMetrics:
        """收集性能指标"""
        # 系统指标
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_mb = memory.used / 1024 / 1024
        
        # GPU指标
        gpu_memory_mb = self._get_gpu_memory()
        
        return PerformanceMetrics(
            timestamp=datetime.now().isoformat(),
            cpu_percent=cpu_percent,
            memory_percent=memory_percent,
            memory_used_mb=memory_used_mb,
            gpu_memory_mb=gpu_memory_mb,
            operation_time=operation_time,
            operation_name=operation_name
        )
    
    def _monitor_loop(self):
        """监控循环"""
        while self.monitoring:
            metrics = self._collect_metrics()
            self.metrics.append(metrics)
            time.sleep(self.monitor_interval)
    
    def start_monitoring(self):
        """开始监控"""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self._monitor_loop)
            self.monitor_thread.daemon = True
            self.monitor_thread.start()
            print("🔍 性能监控已启动")
    
    def stop_monitoring(self):
        """停止监控"""
        if self.monitoring:
            self.monitoring = False
            if self.monitor_thread:
                self.monitor_thread.join(timeout=2)
            print("⏹️ 性能监控已停止")
    
    def record_operation(self, operation_name: str, operation_time: float):
        """记录操作性能"""
        metrics = self._collect_metrics(operation_name, operation_time)
        self.metrics.append(metrics)
    
    def get_summary(self) -> Dict[str, Any]:
        """获取性能摘要"""
        if not self.metrics:
            return {"error": "没有性能数据"}
        
        # 计算统计信息
        cpu_values = [m.cpu_percent for m in self.metrics]
        memory_values = [m.memory_percent for m in self.metrics]
        memory_used_values = [m.memory_used_mb for m in self.metrics]
        
        summary = {
            "monitoring_duration": len(self.metrics) * self.monitor_interval,
            "total_samples": len(self.metrics),
            "cpu_usage": {
                "avg": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory_usage": {
                "avg_percent": sum(memory_values) / len(memory_values),
                "max_percent": max(memory_values),
                "min_percent": min(memory_values),
                "avg_used_mb": sum(memory_used_values) / len(memory_used_values),
                "max_used_mb": max(memory_used_values),
                "min_used_mb": min(memory_used_values)
            }
        }
        
        # GPU统计
        gpu_values = [m.gpu_memory_mb for m in self.metrics if m.gpu_memory_mb is not None]
        if gpu_values:
            summary["gpu_memory"] = {
                "avg_mb": sum(gpu_values) / len(gpu_values),
                "max_mb": max(gpu_values),
                "min_mb": min(gpu_values)
            }
        
        # 操作统计
        operations = [m for m in self.metrics if m.operation_name is not None]
        if operations:
            op_stats = {}
            for op in operations:
                if op.operation_name not in op_stats:
                    op_stats[op.operation_name] = []
                op_stats[op.operation_name].append(op.operation_time)
            
            summary["operations"] = {}
            for op_name, times in op_stats.items():
                summary["operations"][op_name] = {
                    "count": len(times),
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times),
                    "total_time": sum(times)
                }
        
        return summary
    
    def save_metrics(self, output_file: Path):
        """保存性能指标"""
        data = {
            "monitor_config": {
                "interval": self.monitor_interval,
                "gpu_available": self.gpu_available
            },
            "summary": self.get_summary(),
            "raw_metrics": [
                {
                    "timestamp": m.timestamp,
                    "cpu_percent": m.cpu_percent,
                    "memory_percent": m.memory_percent,
                    "memory_used_mb": m.memory_used_mb,
                    "gpu_memory_mb": m.gpu_memory_mb,
                    "operation_time": m.operation_time,
                    "operation_name": m.operation_name
                }
                for m in self.metrics
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"📊 性能指标已保存到: {output_file}")
    
    def clear_metrics(self):
        """清空性能指标"""
        self.metrics.clear()
        print("🧹 性能指标已清空")
    
    def print_summary(self):
        """打印性能摘要"""
        summary = self.get_summary()
        
        if "error" in summary:
            print(f"❌ {summary['error']}")
            return
        
        print("\n📊 性能监控摘要:")
        print(f"  监控时长: {summary['monitoring_duration']:.1f}秒")
        print(f"  采样次数: {summary['total_samples']}")
        
        print(f"\n💻 CPU使用率:")
        cpu = summary['cpu_usage']
        print(f"  平均: {cpu['avg']:.1f}%")
        print(f"  最大: {cpu['max']:.1f}%")
        print(f"  最小: {cpu['min']:.1f}%")
        
        print(f"\n💾 内存使用:")
        memory = summary['memory_usage']
        print(f"  平均使用率: {memory['avg_percent']:.1f}%")
        print(f"  最大使用率: {memory['max_percent']:.1f}%")
        print(f"  平均使用量: {memory['avg_used_mb']:.1f}MB")
        print(f"  最大使用量: {memory['max_used_mb']:.1f}MB")
        
        if "gpu_memory" in summary:
            print(f"\n🎮 GPU内存:")
            gpu = summary['gpu_memory']
            print(f"  平均: {gpu['avg_mb']:.1f}MB")
            print(f"  最大: {gpu['max_mb']:.1f}MB")
        
        if "operations" in summary:
            print(f"\n⚡ 操作性能:")
            for op_name, stats in summary['operations'].items():
                print(f"  {op_name}:")
                print(f"    执行次数: {stats['count']}")
                print(f"    平均耗时: {stats['avg_time']:.3f}秒")
                print(f"    最大耗时: {stats['max_time']:.3f}秒")
                print(f"    总耗时: {stats['total_time']:.3f}秒")

class OperationTimer:
    """操作计时器（上下文管理器）"""
    
    def __init__(self, monitor: PerformanceMonitor, operation_name: str):
        self.monitor = monitor
        self.operation_name = operation_name
        self.start_time = None
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time is not None:
            operation_time = time.time() - self.start_time
            self.monitor.record_operation(self.operation_name, operation_time)
```

### 4.2 创建性能测试脚本

**创建 `test_performance.py`：**
```python
"""性能测试脚本"""

from pathlib import Path
import time
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore
from src.vector_store.document_vectorizer import DocumentVectorizer
from src.utils.performance_monitor import PerformanceMonitor, OperationTimer

def create_large_test_document():
    """创建大型测试文档"""
    test_dir = Path("performance_test_docs")
    test_dir.mkdir(exist_ok=True)
    
    # 创建大型文档内容
    large_content = """
深度学习与人工智能技术详解

第一章：人工智能基础

人工智能（Artificial Intelligence，AI）是计算机科学的一个重要分支，旨在创建能够模拟人类智能行为的系统。自1956年达特茅斯会议首次提出人工智能概念以来，这一领域经历了多次发展浪潮。

人工智能的核心目标是让机器能够执行通常需要人类智能的任务，包括学习、推理、感知、理解自然语言、识别模式等。现代AI系统主要基于机器学习技术，特别是深度学习方法。

第二章：机器学习基础

机器学习是人工智能的一个子领域，专注于开发能够从数据中自动学习和改进的算法。机器学习可以分为三大类：监督学习、无监督学习和强化学习。

监督学习使用标记的训练数据来学习输入和输出之间的映射关系。常见的监督学习任务包括分类和回归。分类任务预测离散的类别标签，如图像分类、文本分类等。回归任务预测连续的数值，如房价预测、股票价格预测等。

无监督学习处理没有标签的数据，目标是发现数据中的隐藏模式和结构。聚类分析是无监督学习的典型应用，它将相似的数据点分组。降维技术如主成分分析（PCA）也属于无监督学习，用于减少数据的维度同时保持重要信息。

强化学习通过与环境的交互来学习最优策略。智能体在环境中执行动作，根据获得的奖励或惩罚来调整行为策略。强化学习在游戏AI、机器人控制、自动驾驶等领域有重要应用。

第三章：深度学习技术

深度学习是机器学习的一个分支，基于人工神经网络，特别是具有多个隐藏层的深层网络。深度学习的核心思想是通过多层非线性变换来学习数据的复杂表示。

神经网络的基本单元是神经元，它接收多个输入，通过权重加权求和，然后应用激活函数产生输出。常用的激活函数包括ReLU、Sigmoid、Tanh等。ReLU函数因其简单性和有效性而被广泛使用。

卷积神经网络（CNN）专门设计用于处理具有网格结构的数据，如图像。CNN通过卷积层、池化层和全连接层的组合来提取图像特征。卷积层使用滤波器扫描输入图像，检测局部特征。池化层减少特征图的空间尺寸，降低计算复杂度。

循环神经网络（RNN）适用于处理序列数据，如文本、语音、时间序列等。RNN具有记忆能力，能够利用之前的信息来处理当前输入。长短期记忆网络（LSTM）和门控循环单元（GRU）是RNN的改进版本，能够更好地处理长序列依赖问题。

Transformer架构引入了注意力机制，彻底改变了自然语言处理领域。注意力机制允许模型在处理序列时关注不同位置的信息，而不需要循环结构。这使得Transformer能够并行处理序列，大大提高了训练效率。

第四章：自然语言处理

自然语言处理（NLP）是人工智能的一个重要分支，专注于计算机与人类语言之间的交互。NLP的目标是让计算机能够理解、解释和生成人类语言。

文本预处理是NLP的第一步，包括分词、词性标注、命名实体识别、句法分析等任务。分词将文本分解为单词或子词单元。词性标注为每个词分配语法类别，如名词、动词、形容词等。命名实体识别识别文本中的人名、地名、组织名等实体。

词向量表示是NLP的核心技术之一。Word2Vec、GloVe、FastText等方法将词语映射到高维向量空间，使得语义相似的词在向量空间中距离较近。这些词向量可以捕获词语之间的语义关系。

预训练语言模型如BERT、GPT、T5等在NLP领域取得了突破性进展。这些模型在大规模文本语料上进行预训练，学习通用的语言表示，然后在特定任务上进行微调。BERT使用双向编码器结构，能够同时考虑上下文信息。GPT采用自回归生成模式，擅长文本生成任务。

第五章：计算机视觉

计算机视觉致力于让计算机能够理解和解释视觉信息。图像分类是计算机视觉的基础任务，目标是将图像分配到预定义的类别中。目标检测不仅要识别图像中的对象，还要定位它们的位置。语义分割为图像中的每个像素分配类别标签。

卷积神经网络在计算机视觉中发挥着核心作用。LeNet、AlexNet、VGG、ResNet、DenseNet等经典网络架构推动了计算机视觉的发展。残差连接、批量归一化、注意力机制等技术进一步提升了网络性能。

生成对抗网络（GAN）开创了图像生成的新纪元。GAN由生成器和判别器组成，通过对抗训练生成逼真的图像。变分自编码器（VAE）、扩散模型等也是重要的生成模型。

第六章：应用与未来

人工智能技术在各个领域都有广泛应用。在医疗健康领域，AI用于医学影像分析、药物发现、疾病诊断等。在金融领域，AI应用于风险评估、算法交易、欺诈检测等。在教育领域，AI支持个性化学习、智能辅导、自动评分等。

自动驾驶是AI技术的重要应用方向，涉及计算机视觉、传感器融合、路径规划、决策控制等多个技术领域。智能语音助手如Siri、Alexa、小爱同学等已经成为日常生活的一部分。

大语言模型如GPT-3、ChatGPT、GPT-4等展现了强大的语言理解和生成能力，在对话系统、文本创作、代码生成等方面表现出色。多模态AI能够同时处理文本、图像、音频等多种模态的信息。

人工智能的未来发展方向包括通用人工智能（AGI）、可解释AI、联邦学习、边缘计算等。随着技术的不断进步，AI将在更多领域发挥重要作用，同时也需要关注AI的伦理、安全、隐私等问题。

结语

人工智能技术正在快速发展，深刻改变着我们的生活和工作方式。掌握AI技术的基本原理和应用方法，对于在数字化时代保持竞争力至关重要。通过不断学习和实践，我们可以更好地利用AI技术解决实际问题，推动社会进步。
    """ * 5  # 重复5次以创建更大的文档
    
    # 创建多个大型文档
    for i in range(3):
        file_path = test_dir / f"large_document_{i+1}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"文档 {i+1}\n\n" + large_content)
    
    print(f"✅ 创建了3个大型测试文档在 {test_dir}")
    return test_dir

def test_embedding_performance():
    """测试向量化性能"""
    print("\n🚀 测试向量化性能...")
    
    # 初始化性能监控
    monitor = PerformanceMonitor(monitor_interval=0.5)
    monitor.start_monitoring()
    
    try:
        # 测试不同模型的性能
        models = [
            'all-MiniLM-L6-v2',
            'paraphrase-MiniLM-L6-v2'
        ]
        
        test_texts = [
            "这是一个测试文本，用于评估向量化性能。" * 10,
            "人工智能是计算机科学的一个分支，致力于创建智能系统。" * 10,
            "深度学习使用神经网络来学习数据的复杂模式和表示。" * 10
        ] * 20  # 创建60个测试文本
        
        for model_name in models:
            print(f"\n测试模型: {model_name}")
            
            # 初始化嵌入器
            with OperationTimer(monitor, f"init_{model_name}"):
                embedder = TextEmbedder(model_name=model_name, batch_size=16)
            
            # 测试单个文本向量化
            with OperationTimer(monitor, f"single_embed_{model_name}"):
                single_embedding = embedder.embed_text(test_texts[0])
            
            print(f"  单个文本向量维度: {len(single_embedding)}")
            
            # 测试批量向量化
            batch_sizes = [8, 16, 32]
            for batch_size in batch_sizes:
                embedder.batch_size = batch_size
                
                with OperationTimer(monitor, f"batch_embed_{model_name}_{batch_size}"):
                    batch_embeddings = embedder.embed_batch(test_texts)
                
                print(f"  批量大小 {batch_size}: 处理了 {len(batch_embeddings)} 个文本")
    
    finally:
        monitor.stop_monitoring()
    
    # 打印性能摘要
    monitor.print_summary()
    
    # 保存性能数据
    monitor.save_metrics(Path("embedding_performance.json"))
    
    return monitor

def test_vectorization_performance():
    """测试完整向量化流程性能"""
    print("\n📊 测试完整向量化流程性能...")
    
    # 创建大型测试文档
    test_dir = create_large_test_document()
    
    # 初始化性能监控
    monitor = PerformanceMonitor(monitor_interval=1.0)
    monitor.start_monitoring()
    
    try:
        # 初始化组件
        with OperationTimer(monitor, "init_components"):
            embedder = TextEmbedder(model_name='all-MiniLM-L6-v2', batch_size=16)
            vector_store = QdrantVectorStore()
            vectorizer = DocumentVectorizer(
                embedder=embedder,
                vector_store=vector_store,
                collection_name="performance_test"
            )
        
        # 测试不同分块大小的性能
        chunk_sizes = [200, 400, 800]
        
        for chunk_size in chunk_sizes:
            print(f"\n测试分块大小: {chunk_size}")
            
            collection_name = f"perf_test_{chunk_size}"
            vectorizer.collection_name = collection_name
            
            # 批量处理文档
            with OperationTimer(monitor, f"process_docs_{chunk_size}"):
                result = vectorizer.process_directory(
                    directory=test_dir,
                    chunk_strategy="sentence",
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_size // 10,
                    file_patterns=['*.txt']
                )
            
            print(f"  处理结果: {result['total_chunks']} 个分块, {result['total_vectors']} 个向量")
            
            # 测试搜索性能
            queries = [
                "什么是深度学习？",
                "人工智能的应用领域",
                "神经网络的基本原理",
                "自然语言处理技术",
                "计算机视觉的发展"
            ]
            
            for i, query in enumerate(queries):
                with OperationTimer(monitor, f"search_{chunk_size}_{i}"):
                    search_results = vectorizer.search_documents(
                        query=query,
                        limit=10,
                        score_threshold=0.1
                    )
                
                print(f"  查询 '{query[:10]}...': 找到 {len(search_results)} 个结果")
            
            # 清理集合
            vector_store.delete_collection(collection_name)
    
    finally:
        monitor.stop_monitoring()
        
        # 清理测试文件
        import shutil
        if test_dir.exists():
            shutil.rmtree(test_dir)
    
    # 打印性能摘要
    monitor.print_summary()
    
    # 保存性能数据
    monitor.save_metrics(Path("vectorization_performance.json"))
    
    return monitor

def test_memory_usage():
    """测试内存使用情况"""
    print("\n💾 测试内存使用情况...")
    
    monitor = PerformanceMonitor(monitor_interval=0.5)
    monitor.start_monitoring()
    
    try:
        # 创建大量文本数据
        large_texts = []
        for i in range(1000):
            text = f"这是第{i}个测试文本。" * 50
            large_texts.append(text)
        
        print(f"创建了 {len(large_texts)} 个文本")
        
        # 初始化嵌入器
        embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
        
        # 批量处理
        batch_size = 32
        embeddings = []
        
        for i in range(0, len(large_texts), batch_size):
            batch = large_texts[i:i+batch_size]
            
            with OperationTimer(monitor, f"batch_process_{i//batch_size}"):
                batch_embeddings = embedder.embed_batch(batch)
                embeddings.extend(batch_embeddings)
            
            if i % (batch_size * 10) == 0:
                print(f"  处理进度: {i}/{len(large_texts)}")
        
        print(f"完成处理，生成了 {len(embeddings)} 个向量")
        
        # 清理内存
        del large_texts
        del embeddings
        
    finally:
        monitor.stop_monitoring()
    
    # 打印内存使用摘要
    monitor.print_summary()
    
    return monitor

if __name__ == "__main__":
    print("开始性能测试...")
    
    try:
        # 测试向量化性能
        embedding_monitor = test_embedding_performance()
        
        print("\n" + "="*50)
        
        # 测试完整流程性能
        vectorization_monitor = test_vectorization_performance()
        
        print("\n" + "="*50)
        
        # 测试内存使用
        memory_monitor = test_memory_usage()
        
        print("\n🎉 所有性能测试完成！")
        
        # 清理性能数据文件
        performance_files = [
            "embedding_performance.json",
            "vectorization_performance.json"
        ]
        
        for file_path in performance_files:
            if Path(file_path).exists():
                Path(file_path).unlink()
        
    except Exception as e:
        print(f"\n💥 性能测试过程中出现错误: {e}")
        import traceback
        traceback.print_exc()
```

**运行性能测试：**
```bash
# 安装性能监控依赖
uv add psutil

# 运行性能测试
python test_performance.py
```

---

## 🤔 思考题

1. **模型选择与性能**：
   - 比较不同sentence-transformers模型的向量化速度和质量
   - 分析模型大小与性能的权衡关系
   - 如何根据应用场景选择合适的模型？

2. **批处理优化**：
   - 批处理大小对性能的影响是什么？
   - 如何平衡内存使用和处理速度？
   - 在什么情况下应该使用GPU加速？

3. **向量存储优化**：
   - Qdrant的不同索引类型对搜索性能有什么影响？
   - 如何优化向量维度以平衡存储空间和搜索精度？
   - 分片和复制策略如何影响系统性能？

4. **分块策略评估**：
   - 不同分块大小对搜索结果质量的影响
   - 重叠大小如何影响信息完整性？
   - 如何设计自适应分块策略？

5. **系统扩展性**：
   - 如何设计支持百万级文档的向量化系统？
   - 增量更新和删除操作如何实现？
   - 如何实现分布式向量化处理？

---

## ✅ 实验检查清单

### 环境准备
- [ ] 成功安装sentence-transformers
- [ ] Qdrant服务正常运行
- [ ] 所有依赖包安装完成
- [ ] 环境测试脚本运行正常

### 向量化功能
- [ ] TextEmbedder类正常工作
- [ ] 支持单个文本和批量文本向量化
- [ ] 模型加载和缓存机制正常
- [ ] 向量维度和格式正确

### 向量存储
- [ ] QdrantVectorStore连接正常
- [ ] 集合创建和删除功能正常
- [ ] 向量插入和搜索功能正常
- [ ] 过滤搜索功能正常

### 文档处理
- [ ] DocumentVectorizer集成正常
- [ ] 单个文档处理功能正常
- [ ] 批量文档处理功能正常
- [ ] 处理日志和统计信息正确

### 性能监控
- [ ] PerformanceMonitor正常工作
- [ ] 系统资源监控准确
- [ ] 操作计时功能正常
- [ ] 性能报告生成正确

### 测试验证
- [ ] 所有测试脚本运行成功
- [ ] 搜索结果相关性合理
- [ ] 性能指标在预期范围内
- [ ] 错误处理机制有效

---

## 🔧 常见问题解决

### 1. 模型下载问题
```bash
# 问题：模型下载缓慢或失败
# 解决：设置镜像源
export HF_ENDPOINT=https://hf-mirror.com

# 或者手动下载模型
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder='./models')
```

### 2. Qdrant连接问题
```python
# 问题：连接Qdrant失败
# 解决：检查服务状态和配置
import requests

try:
    response = requests.get('http://localhost:6333/health')
    print(f"Qdrant状态: {response.status_code}")
except Exception as e:
    print(f"连接失败: {e}")
    print("请检查Qdrant服务是否启动")
```

### 3. 内存不足问题
```python
# 问题：处理大文档时内存不足
# 解决：调整批处理大小和清理策略

# 减小批处理大小
embedder = TextEmbedder(batch_size=8)  # 默认32改为8

# 及时清理变量
import gc
del large_data
gc.collect()
```

### 4. 向量化速度慢
```python
# 问题：向量化速度太慢
# 解决：启用GPU加速（如果可用）

import torch
if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

embedder = TextEmbedder(device=device)
```

### 5. 搜索结果不准确
```python
# 问题：搜索结果相关性差
# 解决：调整搜索参数和分块策略

# 降低分数阈值
results = vectorizer.search_documents(
    query=query,
    limit=10,
    score_threshold=0.1  # 从0.3降低到0.1
)

# 调整分块大小
result = vectorizer.process_document(
    chunk_size=300,  # 增加分块大小
    chunk_overlap=50  # 增加重叠
)
```

---

## 📚 参考资料

1. **Sentence Transformers**：
   - [官方文档](https://www.sbert.net/)
   - [模型列表](https://www.sbert.net/docs/pretrained_models.html)
   - [训练自定义模型](https://www.sbert.net/docs/training/overview.html)

2. **Qdrant向量数据库**：
   - [官方文档](https://qdrant.tech/documentation/)
   - [Python客户端](https://github.com/qdrant/qdrant-client)
   - [性能优化指南](https://qdrant.tech/documentation/guides/optimization/)

3. **向量化最佳实践**：
   - [文本嵌入指南](https://huggingface.co/blog/getting-started-with-embeddings)
   - [向量搜索优化](https://www.pinecone.io/learn/vector-search-optimization/)
   - [RAG系统设计](https://www.anthropic.com/research/retrieval-augmented-generation)

4. **性能监控**：
   - [psutil文档](https://psutil.readthedocs.io/)
   - [Python性能分析](https://docs.python.org/3/library/profile.html)
   - [内存优化技巧](https://realpython.com/python-memory-management/)

---

## 📝 实验完成后的Git操作

### 为什么要进行Git提交？

完成向量化和向量存储实验后，进行Git提交非常重要：

1. **向量化代码保存**：保存TextEmbedder、QdrantVectorStore等核心组件的实现
2. **模型配置管理**：记录sentence-transformers模型选择和配置参数
3. **向量存储方案**：保存Qdrant集合配置和索引策略
4. **性能优化记录**：记录批处理优化和内存管理改进
5. **实验结果追踪**：保存向量化测试结果和性能基准
6. **团队协作**：让团队成员了解向量化实现的最新进展
7. **版本对比**：方便后续对比不同向量化策略的效果

### Git操作步骤

#### 1. 检查当前修改状态
```bash
git status
```

**预期看到的文件变更**：
- `src/core/embedder.py` - TextEmbedder实现
- `src/core/vector_store.py` - QdrantVectorStore实现
- `src/core/document_vectorizer.py` - 文档向量化器
- `src/utils/performance_monitor.py` - 性能监控工具
- `tests/test_embedding.py` - 向量化测试
- `tests/test_vector_store.py` - 向量存储测试
- `requirements.txt` - 新增sentence-transformers等依赖
- `config/embedding_config.py` - 向量化配置文件
- 可能的其他配置和测试文件

#### 2. 添加修改的文件
```bash
# 添加所有修改的文件
git add .

# 或者选择性添加特定文件
git add src/core/embedder.py
git add src/core/vector_store.py
git add src/core/document_vectorizer.py
git add src/utils/performance_monitor.py
git add tests/test_embedding.py
git add tests/test_vector_store.py
git add requirements.txt
```

#### 3. 提交更改
```bash
git commit -m "完成lesson05实验：实现向量化与向量入库功能

- 实现TextEmbedder类，支持sentence-transformers模型
- 实现QdrantVectorStore，支持向量存储和搜索
- 实现DocumentVectorizer，集成文档处理和向量化
- 添加PerformanceMonitor性能监控工具
- 支持批量处理和GPU加速
- 完成向量化和搜索功能测试
- 优化内存使用和处理性能"
```

#### 4. 查看提交历史
```bash
git log --oneline -5
```

#### 5. 推送到远程仓库（可选）
```bash
# 如果需要推送到远程仓库
git push origin lesson05-embedding

# 或推送到主分支（根据你的分支策略）
git push origin main
```

### 提交前验证清单

在提交之前，请确保：

- [ ] **向量化功能验证**：TextEmbedder能正常加载模型并生成向量
- [ ] **向量存储验证**：QdrantVectorStore能正常连接和操作
- [ ] **搜索功能验证**：向量搜索返回合理的相关性结果
- [ ] **性能监控验证**：PerformanceMonitor能正确统计资源使用
- [ ] **测试通过**：所有相关测试用例运行成功
- [ ] **依赖完整**：requirements.txt包含所有必要的依赖包
- [ ] **配置正确**：向量化和存储配置参数合理
- [ ] **文档更新**：相关文档和注释已更新

### 向量化项目特殊注意事项

1. **模型文件管理**：
   ```bash
   # 检查.gitignore是否排除了大型模型文件
   echo "models/" >> .gitignore
   echo "*.bin" >> .gitignore
   echo "*.safetensors" >> .gitignore
   ```

2. **向量数据排除**：
   ```bash
   # 排除向量数据文件（通常很大）
   echo "data/vectors/" >> .gitignore
   echo "*.npy" >> .gitignore
   echo "*.pkl" >> .gitignore
   ```

3. **敏感配置保护**：
   ```bash
   # 检查是否有敏感的API密钥或连接信息
   grep -r "api_key\|password\|secret" src/ || echo "未发现敏感信息"
   ```

### 常见问题解决

1. **大文件提交问题**：
   ```bash
   # 如果意外添加了大型模型文件
   git reset HEAD models/
   git rm --cached models/*.bin
   ```

2. **提交信息修改**：
   ```bash
   # 修改最后一次提交信息
   git commit --amend -m "新的提交信息"
   ```

3. **撤销文件添加**：
   ```bash
   # 撤销git add操作
   git reset HEAD <文件名>
   ```

### 向量化项目Git最佳实践

1. **分层提交**：
   - 先提交核心向量化功能
   - 再提交性能优化改进
   - 最后提交测试和文档

2. **配置管理**：
   - 使用配置文件管理模型参数
   - 版本控制配置变更
   - 记录性能基准数据

3. **模型版本管理**：
   - 记录使用的模型名称和版本
   - 使用标签标记重要的模型切换
   - 保持模型选择的可追溯性

4. **性能基准记录**：
   ```bash
   # 为重要的性能改进打标签
   git tag -a v1.0-embedding-optimized -m "向量化性能优化版本"
   ```

### 下一步学习指导

完成Git提交后，你可以：

1. **预习lesson06**：了解检索增强生成(RAG)的基本概念
2. **研究检索策略**：学习不同的向量检索和重排序方法
3. **探索生成模型**：了解如何集成大语言模型进行答案生成
4. **优化检索质量**：研究如何提高检索结果的相关性和准确性

记住，良好的版本控制习惯是专业开发的重要组成部分！

---

## 🎯 实验完成标志

完成本实验后，你应该能够：

1. ✅ **熟练使用sentence-transformers**：
   - 加载和使用预训练模型
   - 进行单个和批量文本向量化
   - 理解不同模型的特点和适用场景

2. ✅ **掌握Qdrant向量数据库**：
   - 创建和管理向量集合
   - 执行向量插入和搜索操作
   - 使用过滤条件进行精确搜索

3. ✅ **实现完整向量化流程**：
   - 集成文档解析、分块和向量化
   - 支持批量文档处理
   - 实现搜索和统计功能

4. ✅ **进行性能监控和优化**：
   - 监控系统资源使用情况
   - 分析操作性能瓶颈
   - 优化批处理和内存使用

5. ✅ **解决实际问题**：
   - 处理大规模文档向量化
   - 优化搜索结果质量
   - 实现高效的向量存储和检索

6. ✅ **完成Git版本控制**：
   - 所有代码变更已提交到Git仓库
   - 提交信息清晰描述了实现的功能
   - 向量化配置和测试结果已保存

**测试验证**：所有测试脚本运行成功，性能指标满足要求，能够处理实际规模的文档数据，Git提交完成。
```