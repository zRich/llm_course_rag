# Lesson 05 - Embeddingä¸å‘é‡å…¥åº“ - å­¦ç”Ÿå®éªŒæŒ‡å¯¼

## ä»£ç åŸºç¡€å‡†å¤‡

åœ¨å¼€å§‹æœ¬èŠ‚è¯¾çš„å®éªŒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åŸºäºä¸Šä¸€èŠ‚è¯¾çš„ä»£ç ç»§ç»­å¼€å‘ã€‚ç°åœ¨æˆ‘ä»¬ä½¿ç”¨Gitåˆ†æ”¯ç®¡ç†æ¥è·å–ä»£ç ã€‚

### æ­¥éª¤1ï¼šè¿›å…¥é¡¹ç›®ç›®å½•å¹¶åˆ‡æ¢åˆ†æ”¯

```bash
# è¿›å…¥rag-systemé¡¹ç›®ç›®å½•
cd rag-system

# åˆ‡æ¢åˆ°lesson05åˆ†æ”¯
git checkout lesson05

# éªŒè¯å½“å‰åˆ†æ”¯
git branch
# åº”è¯¥æ˜¾ç¤º * lesson05
```

### æ­¥éª¤2ï¼šéªŒè¯ä»£ç çŠ¶æ€

```bash
# æ£€æŸ¥é¡¹ç›®ç»“æ„
ls -la
# åº”è¯¥çœ‹åˆ°ï¼šsrc/ scripts/ test_documents/ docker-compose.yml ç­‰æ–‡ä»¶å’Œç›®å½•

# æ£€æŸ¥å‘é‡åŒ–ç›¸å…³æ–‡ä»¶
ls -la src/embedding/
# åº”è¯¥çœ‹åˆ°ï¼šembedder.py vector_store.py ç­‰æ–‡ä»¶
```

### æ­¥éª¤3ï¼šéªŒè¯PDFè§£æç¯å¢ƒ

```bash
# å¯åŠ¨ä¾èµ–æœåŠ¡
docker-compose up -d

# æµ‹è¯•PDFè§£æåŠŸèƒ½
python -c "from src.document.pdf_parser import PDFParser; print('PDFè§£æå™¨å¯¼å…¥æˆåŠŸ')"

# éªŒè¯æ–‡æ¡£è§£æ
ls test_documents/  # ç¡®è®¤æœ‰æµ‹è¯•PDFæ–‡ä»¶
```

**è¯´æ˜**ï¼šlesson05åˆ†æ”¯åŒ…å«äº†lesson04çš„æ‰€æœ‰ä»£ç ï¼Œå¹¶æ–°å¢äº†Embeddingå‘é‡åŒ–å’ŒQdrantå‘é‡æ•°æ®åº“ç›¸å…³çš„æ¨¡å—å’Œé…ç½®ã€‚

---

## ğŸ¯ å®éªŒç›®æ ‡

é€šè¿‡æœ¬å®éªŒï¼Œä½ å°†å­¦ä¼šï¼š
1. ä½¿ç”¨sentence-transformersåº“è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
2. é…ç½®å’Œä½¿ç”¨bge-m3æ¨¡å‹
3. æ“ä½œQdrantå‘é‡æ•°æ®åº“
4. å®ç°æ‰¹é‡æ–‡æ¡£å‘é‡åŒ–å’Œå…¥åº“
5. è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢å’Œæ€§èƒ½ä¼˜åŒ–

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–åŒ…

**æ›´æ–° `pyproject.toml`ï¼š**
```toml
[tool.uv.dependencies]
# ç°æœ‰ä¾èµ–...
sentence-transformers = "^2.2.2"
qdrant-client = "^1.7.0"
torch = "^2.1.0"
numpy = "^1.24.0"
scipy = "^1.11.0"
scikit-learn = "^1.3.0"
tqdm = "^4.66.0"
matplotlib = "^3.7.0"
seaborn = "^0.12.0"
```

**å®‰è£…ä¾èµ–ï¼š**
```bash
uv sync
```

### 2. å¯åŠ¨QdrantæœåŠ¡

**ç¡®ä¿ `docker-compose.yml` åŒ…å«Qdranté…ç½®ï¼š**
```yaml
services:
  # å…¶ä»–æœåŠ¡...
  
  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag_qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  # å…¶ä»–å·...
  qdrant_data:
```

**å¯åŠ¨æœåŠ¡ï¼š**
```bash
docker-compose up -d qdrant
```

**éªŒè¯QdrantæœåŠ¡ï¼š**
```bash
curl http://localhost:6333/health
```

### 3. æµ‹è¯•ç¯å¢ƒ

**åˆ›å»º `test_environment.py`ï¼š**
```python
"""æµ‹è¯•å‘é‡åŒ–ç¯å¢ƒ"""

import torch
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient

def test_torch():
    """æµ‹è¯•PyTorchç¯å¢ƒ"""
    print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸš€ CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ“± GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

def test_sentence_transformers():
    """æµ‹è¯•sentence-transformers"""
    print("\nğŸ¤– æµ‹è¯•sentence-transformers...")
    try:
        # ä½¿ç”¨è½»é‡çº§æ¨¡å‹è¿›è¡Œæµ‹è¯•
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # æµ‹è¯•ç¼–ç 
        sentences = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­", "This is a test sentence"]
        embeddings = model.encode(sentences)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"ğŸ“ å‘é‡ç»´åº¦: {embeddings.shape[1]}")
        print(f"ğŸ“Š å‘é‡å½¢çŠ¶: {embeddings.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ sentence-transformersæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_qdrant():
    """æµ‹è¯•Qdrantè¿æ¥"""
    print("\nğŸ—„ï¸ æµ‹è¯•Qdrantè¿æ¥...")
    try:
        client = QdrantClient(host="localhost", port=6333)
        
        # è·å–é›†åˆä¿¡æ¯
        collections = client.get_collections()
        print(f"âœ… Qdrantè¿æ¥æˆåŠŸ")
        print(f"ğŸ“š ç°æœ‰é›†åˆæ•°é‡: {len(collections.collections)}")
        
        return True
    except Exception as e:
        print(f"âŒ Qdrantè¿æ¥å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿QdrantæœåŠ¡æ­£åœ¨è¿è¡Œ: docker-compose up -d qdrant")
        return False

if __name__ == "__main__":
    print("ğŸ” ç¯å¢ƒæµ‹è¯•å¼€å§‹...")
    
    test_torch()
    st_ok = test_sentence_transformers()
    qdrant_ok = test_qdrant()
    
    if st_ok and qdrant_ok:
        print("\nğŸ‰ ç¯å¢ƒæµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹å®éªŒäº†ï¼")
    else:
        print("\nâš ï¸ ç¯å¢ƒæµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_environment.py
```

---

## ğŸ”¬ å®éªŒä¸€ï¼šsentence-transformersåŸºç¡€ä½¿ç”¨

### 1.1 åˆ›å»ºå‘é‡åŒ–æ¨¡å—

**åˆ›å»º `src/embedding/embedder.py`ï¼š**
```python
"""æ–‡æœ¬å‘é‡åŒ–æ¨¡å—"""

from typing import List, Union, Optional, Dict, Any
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import logging
from pathlib import Path
import json

logger = logging.getLogger(__name__)

class TextEmbedder:
    """æ–‡æœ¬å‘é‡åŒ–å™¨"""
    
    def __init__(self, 
                 model_name: str = 'BAAI/bge-m3',
                 device: Optional[str] = None,
                 max_seq_length: int = 512,
                 batch_size: int = 32):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¾å¤‡ç±»å‹ ('cuda', 'cpu', Noneä¸ºè‡ªåŠ¨é€‰æ‹©)
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.model_name = model_name
        self.batch_size = batch_size
        
        # è®¾å¤‡é€‰æ‹©
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(max_seq_length)
        
        # æ¨¡å‹ä¿¡æ¯
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"æ¨¡å‹: {model_name}, å‘é‡ç»´åº¦: {self.embedding_dim}")
    
    def _load_model(self, max_seq_length: int) -> SentenceTransformer:
        """åŠ è½½sentence-transformersæ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
            
            model = SentenceTransformer(self.model_name, device=self.device)
            
            # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
            model.max_seq_length = max_seq_length
            
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæœ€å¤§åºåˆ—é•¿åº¦: {max_seq_length}")
            return model
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°è½»é‡çº§æ¨¡å‹
            logger.info("å›é€€åˆ°è½»é‡çº§æ¨¡å‹: all-MiniLM-L6-v2")
            model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
            model.max_seq_length = max_seq_length
            return model
    
    def encode(self, 
               texts: Union[str, List[str]], 
               show_progress: bool = True,
               normalize_embeddings: bool = True) -> np.ndarray:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–ç¼–ç 
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–å‘é‡
            
        Returns:
            å‘é‡æ•°ç»„
        """
        if isinstance(texts, str):
            texts = [texts]
        
        logger.info(f"å¼€å§‹å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬")
        
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                show_progress_bar=show_progress,
                normalize_embeddings=normalize_embeddings,
                convert_to_numpy=True
            )
            
            logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå½¢çŠ¶: {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            logger.error(f"å‘é‡åŒ–å¤±è´¥: {e}")
            raise
    
    def encode_batch(self, 
                    texts: List[str], 
                    batch_size: Optional[int] = None) -> np.ndarray:
        """
        æ‰¹é‡å‘é‡åŒ–ï¼ˆæ‰‹åŠ¨æ§åˆ¶æ‰¹æ¬¡ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            å‘é‡æ•°ç»„
        """
        if batch_size is None:
            batch_size = self.batch_size
        
        embeddings_list = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(
                batch_texts,
                show_progress_bar=False,
                normalize_embeddings=True,
                convert_to_numpy=True
            )
            embeddings_list.append(batch_embeddings)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        all_embeddings = np.vstack(embeddings_list)
        logger.info(f"æ‰¹é‡å‘é‡åŒ–å®Œæˆï¼Œæ€»å½¢çŠ¶: {all_embeddings.shape}")
        
        return all_embeddings
    
    def similarity(self, 
                  embeddings1: np.ndarray, 
                  embeddings2: np.ndarray,
                  metric: str = 'cosine') -> np.ndarray:
        """
        è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
        
        Args:
            embeddings1: ç¬¬ä¸€ç»„å‘é‡
            embeddings2: ç¬¬äºŒç»„å‘é‡
            metric: ç›¸ä¼¼åº¦åº¦é‡ ('cosine', 'euclidean', 'dot')
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        if metric == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            return cosine_similarity(embeddings1, embeddings2)
        
        elif metric == 'euclidean':
            # æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            from sklearn.metrics.pairwise import euclidean_distances
            distances = euclidean_distances(embeddings1, embeddings2)
            return 1 / (1 + distances)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        
        elif metric == 'dot':
            # ç‚¹ç§¯ç›¸ä¼¼åº¦
            return np.dot(embeddings1, embeddings2.T)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡: {metric}")
    
    def save_embeddings(self, 
                       embeddings: np.ndarray, 
                       texts: List[str],
                       output_file: Path,
                       metadata: Optional[Dict[str, Any]] = None):
        """
        ä¿å­˜å‘é‡åŒ–ç»“æœ
        
        Args:
            embeddings: å‘é‡æ•°ç»„
            texts: å¯¹åº”çš„æ–‡æœ¬åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        """
        data = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'num_texts': len(texts),
            'embeddings': embeddings.tolist(),
            'texts': texts,
            'metadata': metadata or {}
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å‘é‡åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def load_embeddings(self, input_file: Path) -> tuple[np.ndarray, List[str], Dict[str, Any]]:
        """
        åŠ è½½å‘é‡åŒ–ç»“æœ
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å‘é‡æ•°ç»„, æ–‡æœ¬åˆ—è¡¨, å…ƒæ•°æ®)
        """
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        embeddings = np.array(data['embeddings'])
        texts = data['texts']
        metadata = data.get('metadata', {})
        
        logger.info(f"ä» {input_file} åŠ è½½äº† {len(texts)} ä¸ªå‘é‡")
        return embeddings, texts, metadata
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'device': self.device,
            'max_seq_length': self.model.max_seq_length,
            'batch_size': self.batch_size
        }
```

### 1.2 åˆ›å»ºå‘é‡åŒ–æµ‹è¯•è„šæœ¬

**åˆ›å»º `test_embedding.py`ï¼š**
```python
"""å‘é‡åŒ–åŠŸèƒ½æµ‹è¯•"""

from pathlib import Path
import numpy as np
from src.embedding.embedder import TextEmbedder

def test_basic_embedding():
    """æµ‹è¯•åŸºç¡€å‘é‡åŒ–åŠŸèƒ½"""
    print("ğŸ”¬ æµ‹è¯•åŸºç¡€å‘é‡åŒ–åŠŸèƒ½...")
    
    # åˆ›å»ºå‘é‡åŒ–å™¨
    embedder = TextEmbedder(
        model_name='all-MiniLM-L6-v2',  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹è¿›è¡Œæµ‹è¯•
        batch_size=4
    )
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†",
        "æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œ",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†äººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒå’Œè§†é¢‘"
    ]
    
    # å‘é‡åŒ–
    embeddings = embedder.encode(test_texts)
    
    print(f"ğŸ“Š å‘é‡åŒ–ç»“æœ:")
    print(f"  æ–‡æœ¬æ•°é‡: {len(test_texts)}")
    print(f"  å‘é‡ç»´åº¦: {embeddings.shape[1]}")
    print(f"  å‘é‡å½¢çŠ¶: {embeddings.shape}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = embedder.similarity(embeddings, embeddings)
    
    print(f"\nğŸ” ç›¸ä¼¼åº¦åˆ†æ:")
    for i, text1 in enumerate(test_texts):
        print(f"\næ–‡æœ¬ {i+1}: {text1[:30]}...")
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æœ¬ï¼ˆé™¤äº†è‡ªå·±ï¼‰
        sim_scores = similarities[i]
        sim_scores[i] = -1  # æ’é™¤è‡ªå·±
        most_similar_idx = np.argmax(sim_scores)
        
        print(f"  æœ€ç›¸ä¼¼: {test_texts[most_similar_idx][:30]}...")
        print(f"  ç›¸ä¼¼åº¦: {sim_scores[most_similar_idx]:.3f}")
    
    # ä¿å­˜ç»“æœ
    output_file = Path("test_embeddings.json")
    embedder.save_embeddings(embeddings, test_texts, output_file)
    
    # åŠ è½½æµ‹è¯•
    loaded_embeddings, loaded_texts, metadata = embedder.load_embeddings(output_file)
    print(f"\nğŸ’¾ åŠ è½½æµ‹è¯•:")
    print(f"  åŠ è½½å‘é‡å½¢çŠ¶: {loaded_embeddings.shape}")
    print(f"  åŠ è½½æ–‡æœ¬æ•°é‡: {len(loaded_texts)}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if output_file.exists():
        output_file.unlink()
    
    return embedder

def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†"""
    print("\nğŸ”„ æµ‹è¯•æ‰¹é‡å¤„ç†...")
    
    embedder = TextEmbedder(
        model_name='all-MiniLM-L6-v2',
        batch_size=2
    )
    
    # ç”Ÿæˆæ›´å¤šæµ‹è¯•æ–‡æœ¬
    test_texts = []
    topics = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰"]
    
    for i, topic in enumerate(topics):
        for j in range(3):
            test_texts.append(f"{topic}æ˜¯ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯é¢†åŸŸï¼Œç¬¬{j+1}ä¸ªç›¸å…³æè¿°ã€‚")
    
    print(f"ğŸ“ ç”Ÿæˆäº† {len(test_texts)} ä¸ªæµ‹è¯•æ–‡æœ¬")
    
    # æ‰¹é‡å‘é‡åŒ–
    embeddings = embedder.encode_batch(test_texts, batch_size=3)
    
    print(f"ğŸ“Š æ‰¹é‡å‘é‡åŒ–ç»“æœ:")
    print(f"  å‘é‡å½¢çŠ¶: {embeddings.shape}")
    
    # åˆ†ææ¯ä¸ªä¸»é¢˜çš„å‘é‡
    for i, topic in enumerate(topics):
        topic_embeddings = embeddings[i*3:(i+1)*3]
        
        # è®¡ç®—ä¸»é¢˜å†…ç›¸ä¼¼åº¦
        topic_similarities = embedder.similarity(topic_embeddings, topic_embeddings)
        avg_similarity = np.mean(topic_similarities[np.triu_indices_from(topic_similarities, k=1)])
        
        print(f"  {topic} ä¸»é¢˜å†…å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")

def test_different_models():
    """æµ‹è¯•ä¸åŒæ¨¡å‹"""
    print("\nğŸ¤– æµ‹è¯•ä¸åŒæ¨¡å‹...")
    
    models = [
        'all-MiniLM-L6-v2',
        'paraphrase-MiniLM-L6-v2'
    ]
    
    test_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œ"
    
    for model_name in models:
        try:
            print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")
            embedder = TextEmbedder(model_name=model_name)
            
            embedding = embedder.encode([test_text])
            
            print(f"  å‘é‡ç»´åº¦: {embedding.shape[1]}")
            print(f"  å‘é‡èŒƒæ•°: {np.linalg.norm(embedding[0]):.3f}")
            print(f"  å‘é‡å‰5ç»´: {embedding[0][:5]}")
            
        except Exception as e:
            print(f"  âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    print("å¼€å§‹å‘é‡åŒ–æµ‹è¯•...")
    
    try:
        embedder = test_basic_embedding()
        test_batch_processing()
        test_different_models()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        info = embedder.get_model_info()
        for key, value in info.items():
            print(f"  {key}: {value}")
            
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_embedding.py
```

---

## ğŸ”¬ å®éªŒäºŒï¼šQdrantå‘é‡æ•°æ®åº“æ“ä½œ

### 2.1 åˆ›å»ºQdrantå®¢æˆ·ç«¯æ¨¡å—

**åˆ›å»º `src/vector_store/qdrant_client.py`ï¼š**
```python
"""Qdrantå‘é‡æ•°æ®åº“å®¢æˆ·ç«¯"""

from typing import List, Dict, Any, Optional, Union
import uuid
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, Filter, 
    FieldCondition, MatchValue, SearchRequest
)
from qdrant_client.http.exceptions import ResponseHandlingException
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    id: str
    score: float
    payload: Dict[str, Any]
    vector: Optional[np.ndarray] = None

class QdrantVectorStore:
    """Qdrantå‘é‡å­˜å‚¨å®¢æˆ·ç«¯"""
    
    def __init__(self, 
                 host: str = "localhost", 
                 port: int = 6333,
                 timeout: int = 60):
        """
        åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯
        
        Args:
            host: QdrantæœåŠ¡å™¨åœ°å€
            port: QdrantæœåŠ¡å™¨ç«¯å£
            timeout: è¿æ¥è¶…æ—¶æ—¶é—´
        """
        self.host = host
        self.port = port
        
        try:
            self.client = QdrantClient(
                host=host, 
                port=port, 
                timeout=timeout
            )
            
            # æµ‹è¯•è¿æ¥
            self.client.get_collections()
            logger.info(f"Qdrantå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ: {host}:{port}")
            
        except Exception as e:
            logger.error(f"Qdrantè¿æ¥å¤±è´¥: {e}")
            raise
    
    def create_collection(self, 
                         collection_name: str, 
                         vector_size: int,
                         distance: str = "Cosine",
                         recreate: bool = False) -> bool:
        """
        åˆ›å»ºå‘é‡é›†åˆ
        
        Args:
            collection_name: é›†åˆåç§°
            vector_size: å‘é‡ç»´åº¦
            distance: è·ç¦»åº¦é‡ ("Cosine", "Euclidean", "Dot")
            recreate: æ˜¯å¦é‡æ–°åˆ›å»ºï¼ˆåˆ é™¤å·²å­˜åœ¨çš„ï¼‰
            
        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.client.get_collections()
            collection_exists = any(
                col.name == collection_name 
                for col in collections.collections
            )
            
            if collection_exists:
                if recreate:
                    logger.info(f"åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ: {collection_name}")
                    self.client.delete_collection(collection_name)
                else:
                    logger.info(f"é›†åˆå·²å­˜åœ¨: {collection_name}")
                    return True
            
            # è·ç¦»åº¦é‡æ˜ å°„
            distance_map = {
                "Cosine": Distance.COSINE,
                "Euclidean": Distance.EUCLID,
                "Dot": Distance.DOT
            }
            
            if distance not in distance_map:
                raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {distance}")
            
            # åˆ›å»ºé›†åˆ
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=distance_map[distance]
                )
            )
            
            logger.info(f"é›†åˆåˆ›å»ºæˆåŠŸ: {collection_name} (ç»´åº¦: {vector_size}, è·ç¦»: {distance})")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return False
    
    def insert_vectors(self, 
                      collection_name: str,
                      vectors: np.ndarray,
                      payloads: List[Dict[str, Any]],
                      ids: Optional[List[str]] = None) -> bool:
        """
        æ’å…¥å‘é‡
        
        Args:
            collection_name: é›†åˆåç§°
            vectors: å‘é‡æ•°ç»„
            payloads: å…ƒæ•°æ®åˆ—è¡¨
            ids: å‘é‡IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”ŸæˆUUIDï¼‰
            
        Returns:
            æ˜¯å¦æ’å…¥æˆåŠŸ
        """
        try:
            if len(vectors) != len(payloads):
                raise ValueError("å‘é‡æ•°é‡ä¸å…ƒæ•°æ®æ•°é‡ä¸åŒ¹é…")
            
            # ç”ŸæˆIDï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if ids is None:
                ids = [str(uuid.uuid4()) for _ in range(len(vectors))]
            
            # åˆ›å»ºç‚¹ç»“æ„
            points = []
            for i, (vector, payload) in enumerate(zip(vectors, payloads)):
                point = PointStruct(
                    id=ids[i],
                    vector=vector.tolist() if isinstance(vector, np.ndarray) else vector,
                    payload=payload
                )
                points.append(point)
            
            # æ‰¹é‡æ’å…¥
            self.client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            logger.info(f"æˆåŠŸæ’å…¥ {len(points)} ä¸ªå‘é‡åˆ°é›†åˆ {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"æ’å…¥å‘é‡å¤±è´¥: {e}")
            return False
    
    def search(self, 
              collection_name: str,
              query_vector: np.ndarray,
              limit: int = 10,
              score_threshold: Optional[float] = None,
              filter_conditions: Optional[Dict[str, Any]] = None) -> List[SearchResult]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢
        
        Args:
            collection_name: é›†åˆåç§°
            query_vector: æŸ¥è¯¢å‘é‡
            limit: è¿”å›ç»“æœæ•°é‡
            score_threshold: åˆ†æ•°é˜ˆå€¼
            filter_conditions: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æ„å»ºè¿‡æ»¤å™¨
            search_filter = None
            if filter_conditions:
                conditions = []
                for key, value in filter_conditions.items():
                    condition = FieldCondition(
                        key=key,
                        match=MatchValue(value=value)
                    )
                    conditions.append(condition)
                
                if conditions:
                    search_filter = Filter(must=conditions)
            
            # æ‰§è¡Œæœç´¢
            search_results = self.client.search(
                collection_name=collection_name,
                query_vector=query_vector.tolist() if isinstance(query_vector, np.ndarray) else query_vector,
                limit=limit,
                score_threshold=score_threshold,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼
            results = []
            for result in search_results:
                search_result = SearchResult(
                    id=str(result.id),
                    score=result.score,
                    payload=result.payload or {}
                )
                results.append(search_result)
            
            logger.info(f"æœç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_collection_info(self, collection_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–é›†åˆä¿¡æ¯
        
        Args:
            collection_name: é›†åˆåç§°
            
        Returns:
            é›†åˆä¿¡æ¯å­—å…¸
        """
        try:
            info = self.client.get_collection(collection_name)
            
            return {
                'name': collection_name,
                'vectors_count': info.vectors_count,
                'indexed_vectors_count': info.indexed_vectors_count,
                'points_count': info.points_count,
                'segments_count': info.segments_count,
                'config': {
                    'vector_size': info.config.params.vectors.size,
                    'distance': info.config.params.vectors.distance.name
                },
                'status': info.status.name
            }
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def delete_collection(self, collection_name: str) -> bool:
        """
        åˆ é™¤é›†åˆ
        
        Args:
            collection_name: é›†åˆåç§°
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            self.client.delete_collection(collection_name)
            logger.info(f"é›†åˆåˆ é™¤æˆåŠŸ: {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤é›†åˆå¤±è´¥: {e}")
            return False
    
    def list_collections(self) -> List[str]:
        """
        åˆ—å‡ºæ‰€æœ‰é›†åˆ
        
        Returns:
            é›†åˆåç§°åˆ—è¡¨
        """
        try:
            collections = self.client.get_collections()
            return [col.name for col in collections.collections]
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆåˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def count_points(self, collection_name: str) -> int:
        """
        ç»Ÿè®¡é›†åˆä¸­çš„ç‚¹æ•°é‡
        
        Args:
            collection_name: é›†åˆåç§°
            
        Returns:
            ç‚¹æ•°é‡
        """
        try:
            info = self.client.get_collection(collection_name)
            return info.points_count
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡ç‚¹æ•°é‡å¤±è´¥: {e}")
            return 0

### 2.2 åˆ›å»ºQdrantæµ‹è¯•è„šæœ¬

**åˆ›å»º `test_qdrant.py`ï¼š**
```python
"""Qdrantå‘é‡æ•°æ®åº“æµ‹è¯•"""

import numpy as np
from pathlib import Path
from src.vector_store.qdrant_client import QdrantVectorStore
from src.embedding.embedder import TextEmbedder

def test_qdrant_basic():
    """æµ‹è¯•QdrantåŸºç¡€åŠŸèƒ½"""
    print("ğŸ—„ï¸ æµ‹è¯•QdrantåŸºç¡€åŠŸèƒ½...")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    vector_store = QdrantVectorStore()
    
    # åˆ—å‡ºç°æœ‰é›†åˆ
    collections = vector_store.list_collections()
    print(f"ğŸ“š ç°æœ‰é›†åˆ: {collections}")
    
    # åˆ›å»ºæµ‹è¯•é›†åˆ
    collection_name = "test_collection"
    vector_size = 384  # all-MiniLM-L6-v2çš„å‘é‡ç»´åº¦
    
    success = vector_store.create_collection(
        collection_name=collection_name,
        vector_size=vector_size,
        distance="Cosine",
        recreate=True
    )
    
    if not success:
        print("âŒ é›†åˆåˆ›å»ºå¤±è´¥")
        return None
    
    # è·å–é›†åˆä¿¡æ¯
    info = vector_store.get_collection_info(collection_name)
    if info:
        print(f"ğŸ“‹ é›†åˆä¿¡æ¯:")
        for key, value in info.items():
            print(f"  {key}: {value}")
    
    return vector_store, collection_name

def test_vector_operations():
    """æµ‹è¯•å‘é‡æ“ä½œ"""
    print("\nğŸ”„ æµ‹è¯•å‘é‡æ“ä½œ...")
    
    # åˆå§‹åŒ–
    vector_store, collection_name = test_qdrant_basic()
    if not vector_store:
        return
    
    # åˆ›å»ºå‘é‡åŒ–å™¨
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
    
    # æµ‹è¯•æ–‡æ¡£
    documents = [
        {
            "text": "äººå·¥æ™ºèƒ½æ˜¯æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯",
            "category": "AI",
            "source": "doc1"
        },
        {
            "text": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
            "category": "ML", 
            "source": "doc2"
        },
        {
            "text": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ",
            "category": "DL",
            "source": "doc3"
        },
        {
            "text": "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†äººç±»è¯­è¨€",
            "category": "NLP",
            "source": "doc4"
        },
        {
            "text": "è®¡ç®—æœºè§†è§‰åˆ†æå›¾åƒå’Œè§†é¢‘",
            "category": "CV",
            "source": "doc5"
        }
    ]
    
    # å‘é‡åŒ–æ–‡æ¡£
    texts = [doc["text"] for doc in documents]
    vectors = embedder.encode(texts)
    
    print(f"ğŸ“Š å‘é‡åŒ–å®Œæˆ: {vectors.shape}")
    
    # å‡†å¤‡å…ƒæ•°æ®
    payloads = []
    for i, doc in enumerate(documents):
        payload = {
            "text": doc["text"],
            "category": doc["category"],
            "source": doc["source"],
            "length": len(doc["text"]),
            "index": i
        }
        payloads.append(payload)
    
    # æ’å…¥å‘é‡
    success = vector_store.insert_vectors(
        collection_name=collection_name,
        vectors=vectors,
        payloads=payloads
    )
    
    if not success:
        print("âŒ å‘é‡æ’å…¥å¤±è´¥")
        return
    
    # éªŒè¯æ’å…¥
    point_count = vector_store.count_points(collection_name)
    print(f"âœ… æˆåŠŸæ’å…¥ {point_count} ä¸ªå‘é‡")
    
    return vector_store, collection_name, embedder, documents

def test_vector_search():
    """æµ‹è¯•å‘é‡æœç´¢"""
    print("\nğŸ” æµ‹è¯•å‘é‡æœç´¢...")
    
    # åˆå§‹åŒ–æ•°æ®
    result = test_vector_operations()
    if not result:
        return
    
    vector_store, collection_name, embedder, documents = result
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "ç¥ç»ç½‘ç»œæ·±åº¦å­¦ä¹ ",
        "è¯­è¨€æ¨¡å‹å¤„ç†"
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vector = embedder.encode([query])[0]
        
        # æœç´¢ç›¸ä¼¼å‘é‡
        results = vector_store.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=3,
            score_threshold=0.3
        )
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
        for i, result in enumerate(results, 1):
            print(f"  {i}. [{result.score:.3f}] {result.payload['text']}")
            print(f"     ç±»åˆ«: {result.payload['category']}, æ¥æº: {result.payload['source']}")
    
    # æµ‹è¯•è¿‡æ»¤æœç´¢
    print(f"\nğŸ¯ æµ‹è¯•è¿‡æ»¤æœç´¢ (category='AI'):")
    query_vector = embedder.encode(["äººå·¥æ™ºèƒ½æŠ€æœ¯"])[0]
    
    filtered_results = vector_store.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=5,
        filter_conditions={"category": "AI"}
    )
    
    print(f"ğŸ“‹ è¿‡æ»¤ç»“æœ ({len(filtered_results)} ä¸ª):")
    for result in filtered_results:
        print(f"  [{result.score:.3f}] {result.payload['text']}")
    
    # æ¸…ç†æµ‹è¯•é›†åˆ
    print(f"\nğŸ§¹ æ¸…ç†æµ‹è¯•é›†åˆ...")
    vector_store.delete_collection(collection_name)
    print(f"âœ… é›†åˆ {collection_name} å·²åˆ é™¤")

def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\nâš¡ æµ‹è¯•æ€§èƒ½...")
    
    import time
    
    vector_store = QdrantVectorStore()
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
    
    collection_name = "performance_test"
    vector_size = 384
    
    # åˆ›å»ºé›†åˆ
    vector_store.create_collection(
        collection_name=collection_name,
        vector_size=vector_size,
        recreate=True
    )
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    num_docs = 100
    test_docs = []
    for i in range(num_docs):
        text = f"è¿™æ˜¯ç¬¬{i+1}ä¸ªæµ‹è¯•æ–‡æ¡£ï¼ŒåŒ…å«ä¸€äº›éšæœºå†…å®¹ç”¨äºæ€§èƒ½æµ‹è¯•ã€‚æ–‡æ¡£ç¼–å·ï¼š{i+1}"
        test_docs.append({
            "text": text,
            "doc_id": f"doc_{i+1}",
            "category": f"cat_{i % 5}"
        })
    
    # å‘é‡åŒ–æ€§èƒ½æµ‹è¯•
    print(f"ğŸ“Š å‘é‡åŒ– {num_docs} ä¸ªæ–‡æ¡£...")
    start_time = time.time()
    
    texts = [doc["text"] for doc in test_docs]
    vectors = embedder.encode_batch(texts, batch_size=16)
    
    embedding_time = time.time() - start_time
    print(f"â±ï¸ å‘é‡åŒ–è€—æ—¶: {embedding_time:.2f}ç§’ ({num_docs/embedding_time:.1f} docs/sec)")
    
    # æ’å…¥æ€§èƒ½æµ‹è¯•
    print(f"ğŸ’¾ æ’å…¥ {num_docs} ä¸ªå‘é‡...")
    start_time = time.time()
    
    payloads = [{
        "text": doc["text"],
        "doc_id": doc["doc_id"],
        "category": doc["category"]
    } for doc in test_docs]
    
    vector_store.insert_vectors(
        collection_name=collection_name,
        vectors=vectors,
        payloads=payloads
    )
    
    insert_time = time.time() - start_time
    print(f"â±ï¸ æ’å…¥è€—æ—¶: {insert_time:.2f}ç§’ ({num_docs/insert_time:.1f} docs/sec)")
    
    # æœç´¢æ€§èƒ½æµ‹è¯•
    print(f"ğŸ” æœç´¢æ€§èƒ½æµ‹è¯•...")
    query_text = "æµ‹è¯•æ–‡æ¡£å†…å®¹"
    query_vector = embedder.encode([query_text])[0]
    
    # å¤šæ¬¡æœç´¢æµ‹è¯•
    search_times = []
    for _ in range(10):
        start_time = time.time()
        
        results = vector_store.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=10
        )
        
        search_time = time.time() - start_time
        search_times.append(search_time)
    
    avg_search_time = np.mean(search_times)
    print(f"â±ï¸ å¹³å‡æœç´¢è€—æ—¶: {avg_search_time*1000:.1f}ms")
    print(f"ğŸ“Š æœç´¢ç»“æœæ•°é‡: {len(results)}")
    
    # æ¸…ç†
    vector_store.delete_collection(collection_name)
    print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    print("å¼€å§‹Qdrantæµ‹è¯•...")
    
    try:
        test_vector_search()
        test_performance()
        
        print("\nğŸ‰ æ‰€æœ‰Qdrantæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_qdrant.py
```

---

## ğŸ”¬ å®éªŒä¸‰ï¼šæ‰¹é‡æ–‡æ¡£å‘é‡åŒ–ä¸å…¥åº“

### 3.1 åˆ›å»ºæ–‡æ¡£å‘é‡åŒ–ç®¡ç†å™¨

**åˆ›å»º `src/vector_store/document_vectorizer.py`ï¼š**
```python
"""æ–‡æ¡£å‘é‡åŒ–ä¸å…¥åº“ç®¡ç†å™¨"""

from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import json
import hashlib
from datetime import datetime
from tqdm import tqdm
import logging

from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore
from src.chunking.chunk_manager import ChunkManager
from src.document.document_manager import DocumentManager

logger = logging.getLogger(__name__)

class DocumentVectorizer:
    """æ–‡æ¡£å‘é‡åŒ–ä¸å…¥åº“ç®¡ç†å™¨"""
    
    def __init__(self,
                 embedder: TextEmbedder,
                 vector_store: QdrantVectorStore,
                 collection_name: str = "documents"):
        """
        åˆå§‹åŒ–æ–‡æ¡£å‘é‡åŒ–å™¨
        
        Args:
            embedder: æ–‡æœ¬å‘é‡åŒ–å™¨
            vector_store: å‘é‡å­˜å‚¨å®¢æˆ·ç«¯
            collection_name: é›†åˆåç§°
        """
        self.embedder = embedder
        self.vector_store = vector_store
        self.collection_name = collection_name
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.doc_manager = DocumentManager()
        self.chunk_manager = ChunkManager()
        
        # ç¡®ä¿é›†åˆå­˜åœ¨
        self._ensure_collection()
    
    def _ensure_collection(self):
        """ç¡®ä¿å‘é‡é›†åˆå­˜åœ¨"""
        collections = self.vector_store.list_collections()
        
        if self.collection_name not in collections:
            logger.info(f"åˆ›å»ºå‘é‡é›†åˆ: {self.collection_name}")
            
            success = self.vector_store.create_collection(
                collection_name=self.collection_name,
                vector_size=self.embedder.embedding_dim,
                distance="Cosine"
            )
            
            if not success:
                raise RuntimeError(f"æ— æ³•åˆ›å»ºé›†åˆ: {self.collection_name}")
        else:
            logger.info(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {self.collection_name}")
    
    def _generate_chunk_id(self, file_path: str, chunk_index: int) -> str:
        """ç”Ÿæˆchunkçš„å”¯ä¸€ID"""
        content = f"{file_path}_{chunk_index}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def process_document(self, 
                        file_path: Path,
                        chunk_strategy: str = "sentence",
                        chunk_size: int = 500,
                        chunk_overlap: int = 50) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£ï¼šè§£æ -> åˆ†å— -> å‘é‡åŒ– -> å…¥åº“
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            chunk_strategy: åˆ†å—ç­–ç•¥
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å 
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
        
        try:
            # 1. è§£ææ–‡æ¡£
            doc_content = self.doc_manager.parse_document(file_path)
            if not doc_content:
                return {"error": "æ–‡æ¡£è§£æå¤±è´¥"}
            
            # 2. åˆ†å—å¤„ç†
            chunks = self.chunk_manager.chunk_text(
                text=doc_content.content,
                strategy=chunk_strategy,
                chunk_size=chunk_size,
                overlap_size=chunk_overlap
            )
            
            if not chunks:
                return {"error": "æ–‡æ¡£åˆ†å—å¤±è´¥"}
            
            # 3. å‘é‡åŒ–
            chunk_texts = [chunk.content for chunk in chunks]
            vectors = self.embedder.encode(chunk_texts, show_progress=False)
            
            # 4. å‡†å¤‡å…ƒæ•°æ®
            payloads = []
            chunk_ids = []
            
            for i, chunk in enumerate(chunks):
                chunk_id = self._generate_chunk_id(str(file_path), i)
                chunk_ids.append(chunk_id)
                
                payload = {
                    "chunk_id": chunk_id,
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "chunk_index": i,
                    "content": chunk.content,
                    "content_length": len(chunk.content),
                    "chunk_strategy": chunk_strategy,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "processed_at": datetime.now().isoformat(),
                    "doc_metadata": {
                        "title": doc_content.metadata.title,
                        "author": doc_content.metadata.author,
                        "page_count": doc_content.metadata.page_count,
                        "file_size": doc_content.metadata.file_size
                    }
                }
                
                # æ·»åŠ chunkå…ƒæ•°æ®
                if hasattr(chunk, 'metadata') and chunk.metadata:
                    payload["chunk_metadata"] = chunk.metadata.__dict__
                
                payloads.append(payload)
            
            # 5. å…¥åº“
            success = self.vector_store.insert_vectors(
                collection_name=self.collection_name,
                vectors=vectors,
                payloads=payloads,
                ids=chunk_ids
            )
            
            if not success:
                return {"error": "å‘é‡å…¥åº“å¤±è´¥"}
            
            # è¿”å›å¤„ç†ç»“æœ
            result = {
                "file_path": str(file_path),
                "file_name": file_path.name,
                "chunks_count": len(chunks),
                "vectors_count": len(vectors),
                "total_content_length": len(doc_content.content),
                "avg_chunk_length": sum(len(chunk.content) for chunk in chunks) / len(chunks),
                "chunk_strategy": chunk_strategy,
                "processed_at": datetime.now().isoformat(),
                "success": True
            }
            
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path.name} ({len(chunks)} chunks)")
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            return {
                "file_path": str(file_path),
                "error": str(e),
                "success": False
            }
    
    def process_directory(self, 
                         directory: Path,
                         chunk_strategy: str = "sentence",
                         chunk_size: int = 500,
                         chunk_overlap: int = 50,
                         file_patterns: List[str] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ–‡æ¡£
        
        Args:
            directory: ç›®å½•è·¯å¾„
            chunk_strategy: åˆ†å—ç­–ç•¥
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å 
            file_patterns: æ–‡ä»¶æ¨¡å¼åˆ—è¡¨
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        if file_patterns is None:
            file_patterns = ['*.pdf', '*.docx', '*.txt', '*.md']
        
        # æ”¶é›†æ–‡ä»¶
        files = []
        for pattern in file_patterns:
            files.extend(directory.glob(pattern))
        
        if not files:
            return {
                "error": f"ç›®å½•ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {directory}",
                "patterns": file_patterns
            }
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")
        
        # å¤„ç†ç»Ÿè®¡
        results = []
        success_count = 0
        total_chunks = 0
        total_vectors = 0
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶
        for file_path in tqdm(files, desc="å¤„ç†æ–‡æ¡£"):
            result = self.process_document(
                file_path=file_path,
                chunk_strategy=chunk_strategy,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            results.append(result)
            
            if result.get("success", False):
                success_count += 1
                total_chunks += result.get("chunks_count", 0)
                total_vectors += result.get("vectors_count", 0)
        
        # æ±‡æ€»ç»“æœ
        summary = {
            "directory": str(directory),
            "total_files": len(files),
            "success_files": success_count,
            "failed_files": len(files) - success_count,
            "total_chunks": total_chunks,
            "total_vectors": total_vectors,
            "chunk_strategy": chunk_strategy,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "processed_at": datetime.now().isoformat(),
            "file_results": results
        }
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(files)} æˆåŠŸ")
        return summary
    
    def search_documents(self, 
                        query: str,
                        limit: int = 10,
                        score_threshold: float = 0.5,
                        file_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: è¿”å›ç»“æœæ•°é‡
            score_threshold: åˆ†æ•°é˜ˆå€¼
            file_filter: æ–‡ä»¶è¿‡æ»¤ï¼ˆæ–‡ä»¶ååŒ…å«ï¼‰
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vector = self.embedder.encode([query])[0]
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_conditions = {}
        if file_filter:
            # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†è¿‡æ»¤é€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è¿‡æ»¤
            pass
        
        # æ‰§è¡Œæœç´¢
        search_results = self.vector_store.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit,
            score_threshold=score_threshold,
            filter_conditions=filter_conditions if filter_conditions else None
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for result in search_results:
            formatted_result = {
                "chunk_id": result.id,
                "score": result.score,
                "content": result.payload.get("content", ""),
                "file_name": result.payload.get("file_name", ""),
                "file_path": result.payload.get("file_path", ""),
                "chunk_index": result.payload.get("chunk_index", 0),
                "content_length": result.payload.get("content_length", 0),
                "doc_metadata": result.payload.get("doc_metadata", {})
            }
            formatted_results.append(formatted_result)
        
        logger.info(f"æœç´¢å®Œæˆ: æŸ¥è¯¢='{query}', ç»“æœ={len(formatted_results)}ä¸ª")
        return formatted_results
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        info = self.vector_store.get_collection_info(self.collection_name)
        if not info:
            return {"error": "æ— æ³•è·å–é›†åˆä¿¡æ¯"}
        
        return {
            "collection_name": self.collection_name,
            "total_vectors": info["vectors_count"],
            "total_points": info["points_count"],
            "vector_dimension": info["config"]["vector_size"],
            "distance_metric": info["config"]["distance"],
            "status": info["status"]
        }
    
    def save_processing_log(self, 
                           results: Dict[str, Any], 
                           output_file: Path):
        """
        ä¿å­˜å¤„ç†æ—¥å¿—
        
        Args:
            results: å¤„ç†ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {output_file}")
```

### 3.2 åˆ›å»ºæ‰¹é‡å‘é‡åŒ–æµ‹è¯•è„šæœ¬

**åˆ›å»º `test_batch_vectorization.py`ï¼š**
```python
"""æ‰¹é‡æ–‡æ¡£å‘é‡åŒ–æµ‹è¯•"""

from pathlib import Path
import json
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore
from src.vector_store.document_vectorizer import DocumentVectorizer

def create_test_documents():
    """åˆ›å»ºæµ‹è¯•æ–‡æ¡£"""
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    documents = {
        "ai_overview.txt": """
äººå·¥æ™ºèƒ½æ¦‚è¿°

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

ä¸»è¦é¢†åŸŸåŒ…æ‹¬ï¼š
1. æœºå™¨å­¦ä¹  - è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ 
2. æ·±åº¦å­¦ä¹  - åŸºäºç¥ç»ç½‘ç»œçš„å­¦ä¹ æ–¹æ³•
3. è‡ªç„¶è¯­è¨€å¤„ç† - å¤„ç†å’Œç†è§£äººç±»è¯­è¨€
4. è®¡ç®—æœºè§†è§‰ - åˆ†æå’Œç†è§£å›¾åƒ
5. æœºå™¨äººå­¦ - æ™ºèƒ½æœºå™¨äººç³»ç»Ÿ

äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µï¼š
- 1950å¹´ä»£ï¼šAIæ¦‚å¿µæå‡º
- 1960-1970å¹´ä»£ï¼šä¸“å®¶ç³»ç»Ÿå‘å±•
- 1980-1990å¹´ä»£ï¼šæœºå™¨å­¦ä¹ å…´èµ·
- 2000å¹´ä»£è‡³ä»Šï¼šæ·±åº¦å­¦ä¹ é©å‘½

å½“å‰AIæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€é‡‘èã€æ•™è‚²ã€äº¤é€šç­‰ã€‚
        """,
        
        "machine_learning.txt": """
æœºå™¨å­¦ä¹ åŸºç¡€

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚

ä¸»è¦ç±»å‹ï¼š
1. ç›‘ç£å­¦ä¹ 
   - åˆ†ç±»é—®é¢˜
   - å›å½’é—®é¢˜
   - å¸¸ç”¨ç®—æ³•ï¼šçº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€SVM

2. æ— ç›‘ç£å­¦ä¹ 
   - èšç±»åˆ†æ
   - é™ç»´æŠ€æœ¯
   - å¸¸ç”¨ç®—æ³•ï¼šK-meansã€PCAã€t-SNE

3. å¼ºåŒ–å­¦ä¹ 
   - æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’
   - å¥–åŠ±æœºåˆ¶é©±åŠ¨å­¦ä¹ 
   - åº”ç”¨ï¼šæ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶

æœºå™¨å­¦ä¹ çš„å·¥ä½œæµç¨‹ï¼š
1. æ•°æ®æ”¶é›†å’Œé¢„å¤„ç†
2. ç‰¹å¾å·¥ç¨‹
3. æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒ
4. æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–
5. æ¨¡å‹éƒ¨ç½²å’Œç›‘æ§

è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ã€‚
        """,
        
        "deep_learning.txt": """
æ·±åº¦å­¦ä¹ æŠ€æœ¯

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼ŒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œã€‚

æ ¸å¿ƒæ¦‚å¿µï¼š
1. ç¥ç»ç½‘ç»œ
   - æ„ŸçŸ¥æœº
   - å¤šå±‚æ„ŸçŸ¥æœº
   - æ¿€æ´»å‡½æ•°ï¼šReLUã€Sigmoidã€Tanh

2. æ·±åº¦ç½‘ç»œæ¶æ„
   - å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰- å›¾åƒå¤„ç†
   - å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰- åºåˆ—æ•°æ®
   - é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰- é•¿åºåˆ—å»ºæ¨¡
   - Transformer - æ³¨æ„åŠ›æœºåˆ¶

3. è®­ç»ƒæŠ€æœ¯
   - åå‘ä¼ æ’­ç®—æ³•
   - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
   - æ­£åˆ™åŒ–æŠ€æœ¯ï¼šDropoutã€Batch Normalization
   - å­¦ä¹ ç‡è°ƒåº¦

æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸï¼š
- è®¡ç®—æœºè§†è§‰ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒç”Ÿæˆ
- è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æ
- è¯­éŸ³è¯†åˆ«ï¼šè¯­éŸ³è½¬æ–‡æœ¬ã€è¯­éŸ³åˆæˆ
- æ¨èç³»ç»Ÿï¼šä¸ªæ€§åŒ–æ¨è

ä¸»è¦æ¡†æ¶ï¼šTensorFlowã€PyTorchã€Kerasç­‰ã€‚
        """,
        
        "nlp_basics.txt": """
è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚

ä¸»è¦ä»»åŠ¡ï¼š
1. æ–‡æœ¬é¢„å¤„ç†
   - åˆ†è¯ï¼ˆTokenizationï¼‰
   - è¯æ€§æ ‡æ³¨ï¼ˆPOS Taggingï¼‰
   - å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
   - å¥æ³•åˆ†æ

2. è¯­ä¹‰ç†è§£
   - è¯å‘é‡è¡¨ç¤ºï¼šWord2Vecã€GloVeã€FastText
   - å¥å­è¡¨ç¤ºï¼šBERTã€GPTã€T5
   - è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

3. åº”ç”¨ä»»åŠ¡
   - æ–‡æœ¬åˆ†ç±»
   - æƒ…æ„Ÿåˆ†æ
   - æœºå™¨ç¿»è¯‘
   - é—®ç­”ç³»ç»Ÿ
   - æ–‡æœ¬æ‘˜è¦
   - å¯¹è¯ç³»ç»Ÿ

æŠ€æœ¯å‘å±•å†ç¨‹ï¼š
- è§„åˆ™åŸºç¡€æ–¹æ³•
- ç»Ÿè®¡æ–¹æ³•
- æœºå™¨å­¦ä¹ æ–¹æ³•
- æ·±åº¦å­¦ä¹ æ–¹æ³•
- é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹

å½“å‰çƒ­ç‚¹ï¼š
- å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
- æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰
- å¤šæ¨¡æ€ç†è§£
- ä»£ç ç”Ÿæˆ
        """
    }
    
    # å†™å…¥æ–‡ä»¶
    for filename, content in documents.items():
        file_path = test_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
    
    print(f"âœ… åˆ›å»ºäº† {len(documents)} ä¸ªæµ‹è¯•æ–‡æ¡£åœ¨ {test_dir}")
    return test_dir

def test_single_document_processing():
    """æµ‹è¯•å•ä¸ªæ–‡æ¡£å¤„ç†"""
    print("\nğŸ“„ æµ‹è¯•å•ä¸ªæ–‡æ¡£å¤„ç†...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_dir = create_test_documents()
    
    # åˆå§‹åŒ–ç»„ä»¶
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2', batch_size=8)
    vector_store = QdrantVectorStore()
    vectorizer = DocumentVectorizer(
        embedder=embedder,
        vector_store=vector_store,
        collection_name="test_docs"
    )
    
    # å¤„ç†å•ä¸ªæ–‡æ¡£
    test_file = test_dir / "ai_overview.txt"
    result = vectorizer.process_document(
        file_path=test_file,
        chunk_strategy="sentence",
        chunk_size=200,
        chunk_overlap=20
    )
    
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    for key, value in result.items():
        if key != "error":
            print(f"  {key}: {value}")
    
    # æµ‹è¯•æœç´¢
    if result.get("success", False):
        print(f"\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
        
        queries = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "æœºå™¨å­¦ä¹ çš„åº”ç”¨",
            "AIå‘å±•å†ç¨‹"
        ]
        
        for query in queries:
            print(f"\næŸ¥è¯¢: {query}")
            search_results = vectorizer.search_documents(
                query=query,
                limit=3,
                score_threshold=0.3
            )
            
            for i, result in enumerate(search_results, 1):
                print(f"  {i}. [{result['score']:.3f}] {result['content'][:100]}...")
    
    return vectorizer

def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†"""
    print("\nğŸ“š æµ‹è¯•æ‰¹é‡æ–‡æ¡£å¤„ç†...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_dir = create_test_documents()
    
    # åˆå§‹åŒ–ç»„ä»¶
    embedder = TextEmbedder(model_name='all-MiniLM-L6-v2', batch_size=8)
    vector_store = QdrantVectorStore()
    vectorizer = DocumentVectorizer(
        embedder=embedder,
        vector_store=vector_store,
        collection_name="batch_test_docs"
    )
    
    # æ‰¹é‡å¤„ç†
    batch_result = vectorizer.process_directory(
        directory=test_dir,
        chunk_strategy="sentence",
        chunk_size=300,
        chunk_overlap=30,
        file_patterns=['*.txt']
    )
    
    print(f"ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœ:")
    print(f"  æ€»æ–‡ä»¶æ•°: {batch_result['total_files']}")
    print(f"  æˆåŠŸæ–‡ä»¶æ•°: {batch_result['success_files']}")
    print(f"  å¤±è´¥æ–‡ä»¶æ•°: {batch_result['failed_files']}")
    print(f"  æ€»åˆ†å—æ•°: {batch_result['total_chunks']}")
    print(f"  æ€»å‘é‡æ•°: {batch_result['total_vectors']}")
    
    # ä¿å­˜å¤„ç†æ—¥å¿—
    log_file = Path("batch_processing_log.json")
    vectorizer.save_processing_log(batch_result, log_file)
    
    # æµ‹è¯•ç»¼åˆæœç´¢
    print(f"\nğŸ” æµ‹è¯•ç»¼åˆæœç´¢...")
    
    comprehensive_queries = [
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«",
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦ä»»åŠ¡",
        "ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µ",
        "äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸ"
    ]
    
    for query in comprehensive_queries:
        print(f"\næŸ¥è¯¢: {query}")
        search_results = vectorizer.search_documents(
            query=query,
            limit=5,
            score_threshold=0.2
        )
        
        print(f"æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ç»“æœ:")
        for i, result in enumerate(search_results, 1):
            print(f"  {i}. [{result['score']:.3f}] {result['file_name']} - {result['content'][:80]}...")
    
    # è·å–é›†åˆç»Ÿè®¡
    stats = vectorizer.get_collection_stats()
    print(f"\nğŸ“ˆ é›†åˆç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    if test_dir.exists():
        shutil.rmtree(test_dir)
    if log_file.exists():
        log_file.unlink()
    
    # æ¸…ç†æµ‹è¯•é›†åˆ
    vector_store.delete_collection("batch_test_docs")
    
    return batch_result

def test_different_chunking_strategies():
    """æµ‹è¯•ä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœ"""
    print("\nğŸ”„ æµ‹è¯•ä¸åŒåˆ†å—ç­–ç•¥...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_dir = create_test_documents()
    test_file = test_dir / "deep_learning.txt"
    
    strategies = [
        {"name": "sentence", "chunk_size": 200, "overlap": 20},
        {"name": "sentence", "chunk_size": 400, "overlap": 40},
        {"name": "sentence", "chunk_size": 600, "overlap": 60}
    ]
    
    results = []
    
    for strategy in strategies:
        print(f"\næµ‹è¯•ç­–ç•¥: {strategy['name']} (size={strategy['chunk_size']}, overlap={strategy['overlap']})")
        
        # åˆå§‹åŒ–ç»„ä»¶
        embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
        vector_store = QdrantVectorStore()
        
        collection_name = f"test_{strategy['name']}_{strategy['chunk_size']}"
        vectorizer = DocumentVectorizer(
            embedder=embedder,
            vector_store=vector_store,
            collection_name=collection_name
        )
        
        # å¤„ç†æ–‡æ¡£
        result = vectorizer.process_document(
            file_path=test_file,
            chunk_strategy=strategy['name'],
            chunk_size=strategy['chunk_size'],
            chunk_overlap=strategy['overlap']
        )
        
        if result.get("success", False):
            print(f"  åˆ†å—æ•°é‡: {result['chunks_count']}")
            print(f"  å¹³å‡åˆ†å—é•¿åº¦: {result['avg_chunk_length']:.1f}")
            
            # æµ‹è¯•æœç´¢æ•ˆæœ
            query = "ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼Ÿ"
            search_results = vectorizer.search_documents(
                query=query,
                limit=3,
                score_threshold=0.2
            )
            
            print(f"  æœç´¢ç»“æœæ•°é‡: {len(search_results)}")
            if search_results:
                print(f"  æœ€é«˜åˆ†æ•°: {search_results[0]['score']:.3f}")
            
            results.append({
                "strategy": strategy,
                "chunks_count": result['chunks_count'],
                "avg_chunk_length": result['avg_chunk_length'],
                "search_results_count": len(search_results),
                "best_score": search_results[0]['score'] if search_results else 0
            })
        
        # æ¸…ç†é›†åˆ
        vector_store.delete_collection(collection_name)
    
    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š ç­–ç•¥æ¯”è¾ƒç»“æœ:")
    for result in results:
        strategy = result['strategy']
        print(f"  {strategy['name']}({strategy['chunk_size']}/{strategy['overlap']}): "
              f"chunks={result['chunks_count']}, "
              f"avg_len={result['avg_chunk_length']:.1f}, "
              f"results={result['search_results_count']}, "
              f"score={result['best_score']:.3f}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    if test_dir.exists():
        shutil.rmtree(test_dir)

if __name__ == "__main__":
    print("å¼€å§‹æ‰¹é‡å‘é‡åŒ–æµ‹è¯•...")
    
    try:
        # æµ‹è¯•å•ä¸ªæ–‡æ¡£å¤„ç†
        vectorizer = test_single_document_processing()
        
        # æ¸…ç†å•ä¸ªæ–‡æ¡£æµ‹è¯•çš„é›†åˆ
        vectorizer.vector_store.delete_collection("test_docs")
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        test_batch_processing()
        
        # æµ‹è¯•ä¸åŒåˆ†å—ç­–ç•¥
        test_different_chunking_strategies()
        
        print("\nğŸ‰ æ‰€æœ‰æ‰¹é‡å‘é‡åŒ–æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python test_batch_vectorization.py
```

---

## ğŸ”¬ å®éªŒå››ï¼šæ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### 4.1 åˆ›å»ºæ€§èƒ½ç›‘æ§å·¥å…·

**åˆ›å»º `src/utils/performance_monitor.py`ï¼š**
```python
"""æ€§èƒ½ç›‘æ§å·¥å…·"""

import time
import psutil
import threading
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import json
from pathlib import Path

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    gpu_memory_mb: Optional[float] = None
    operation_time: Optional[float] = None
    operation_name: Optional[str] = None

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, monitor_interval: float = 1.0):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        
        Args:
            monitor_interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
        """
        self.monitor_interval = monitor_interval
        self.metrics: List[PerformanceMetrics] = []
        self.monitoring = False
        self.monitor_thread = None
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        self.gpu_available = self._check_gpu_available()
    
    def _check_gpu_available(self) -> bool:
        """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _get_gpu_memory(self) -> Optional[float]:
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.gpu_available:
            return None
        
        try:
            import torch
            if torch.cuda.is_available():
                return torch.cuda.memory_allocated() / 1024 / 1024  # MB
        except Exception:
            pass
        
        return None
    
    def _collect_metrics(self, operation_name: Optional[str] = None, 
                        operation_time: Optional[float] = None) -> PerformanceMetrics:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        # ç³»ç»ŸæŒ‡æ ‡
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_mb = memory.used / 1024 / 1024
        
        # GPUæŒ‡æ ‡
        gpu_memory_mb = self._get_gpu_memory()
        
        return PerformanceMetrics(
            timestamp=datetime.now().isoformat(),
            cpu_percent=cpu_percent,
            memory_percent=memory_percent,
            memory_used_mb=memory_used_mb,
            gpu_memory_mb=gpu_memory_mb,
            operation_time=operation_time,
            operation_name=operation_name
        )
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            metrics = self._collect_metrics()
            self.metrics.append(metrics)
            time.sleep(self.monitor_interval)
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self._monitor_loop)
            self.monitor_thread.daemon = True
            self.monitor_thread.start()
            print("ğŸ” æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        if self.monitoring:
            self.monitoring = False
            if self.monitor_thread:
                self.monitor_thread.join(timeout=2)
            print("â¹ï¸ æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def record_operation(self, operation_name: str, operation_time: float):
        """è®°å½•æ“ä½œæ€§èƒ½"""
        metrics = self._collect_metrics(operation_name, operation_time)
        self.metrics.append(metrics)
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics:
            return {"error": "æ²¡æœ‰æ€§èƒ½æ•°æ®"}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        cpu_values = [m.cpu_percent for m in self.metrics]
        memory_values = [m.memory_percent for m in self.metrics]
        memory_used_values = [m.memory_used_mb for m in self.metrics]
        
        summary = {
            "monitoring_duration": len(self.metrics) * self.monitor_interval,
            "total_samples": len(self.metrics),
            "cpu_usage": {
                "avg": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory_usage": {
                "avg_percent": sum(memory_values) / len(memory_values),
                "max_percent": max(memory_values),
                "min_percent": min(memory_values),
                "avg_used_mb": sum(memory_used_values) / len(memory_used_values),
                "max_used_mb": max(memory_used_values),
                "min_used_mb": min(memory_used_values)
            }
        }
        
        # GPUç»Ÿè®¡
        gpu_values = [m.gpu_memory_mb for m in self.metrics if m.gpu_memory_mb is not None]
        if gpu_values:
            summary["gpu_memory"] = {
                "avg_mb": sum(gpu_values) / len(gpu_values),
                "max_mb": max(gpu_values),
                "min_mb": min(gpu_values)
            }
        
        # æ“ä½œç»Ÿè®¡
        operations = [m for m in self.metrics if m.operation_name is not None]
        if operations:
            op_stats = {}
            for op in operations:
                if op.operation_name not in op_stats:
                    op_stats[op.operation_name] = []
                op_stats[op.operation_name].append(op.operation_time)
            
            summary["operations"] = {}
            for op_name, times in op_stats.items():
                summary["operations"][op_name] = {
                    "count": len(times),
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times),
                    "total_time": sum(times)
                }
        
        return summary
    
    def save_metrics(self, output_file: Path):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡"""
        data = {
            "monitor_config": {
                "interval": self.monitor_interval,
                "gpu_available": self.gpu_available
            },
            "summary": self.get_summary(),
            "raw_metrics": [
                {
                    "timestamp": m.timestamp,
                    "cpu_percent": m.cpu_percent,
                    "memory_percent": m.memory_percent,
                    "memory_used_mb": m.memory_used_mb,
                    "gpu_memory_mb": m.gpu_memory_mb,
                    "operation_time": m.operation_time,
                    "operation_name": m.operation_name
                }
                for m in self.metrics
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {output_file}")
    
    def clear_metrics(self):
        """æ¸…ç©ºæ€§èƒ½æŒ‡æ ‡"""
        self.metrics.clear()
        print("ğŸ§¹ æ€§èƒ½æŒ‡æ ‡å·²æ¸…ç©º")
    
    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        summary = self.get_summary()
        
        if "error" in summary:
            print(f"âŒ {summary['error']}")
            return
        
        print("\nğŸ“Š æ€§èƒ½ç›‘æ§æ‘˜è¦:")
        print(f"  ç›‘æ§æ—¶é•¿: {summary['monitoring_duration']:.1f}ç§’")
        print(f"  é‡‡æ ·æ¬¡æ•°: {summary['total_samples']}")
        
        print(f"\nğŸ’» CPUä½¿ç”¨ç‡:")
        cpu = summary['cpu_usage']
        print(f"  å¹³å‡: {cpu['avg']:.1f}%")
        print(f"  æœ€å¤§: {cpu['max']:.1f}%")
        print(f"  æœ€å°: {cpu['min']:.1f}%")
        
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
        memory = summary['memory_usage']
        print(f"  å¹³å‡ä½¿ç”¨ç‡: {memory['avg_percent']:.1f}%")
        print(f"  æœ€å¤§ä½¿ç”¨ç‡: {memory['max_percent']:.1f}%")
        print(f"  å¹³å‡ä½¿ç”¨é‡: {memory['avg_used_mb']:.1f}MB")
        print(f"  æœ€å¤§ä½¿ç”¨é‡: {memory['max_used_mb']:.1f}MB")
        
        if "gpu_memory" in summary:
            print(f"\nğŸ® GPUå†…å­˜:")
            gpu = summary['gpu_memory']
            print(f"  å¹³å‡: {gpu['avg_mb']:.1f}MB")
            print(f"  æœ€å¤§: {gpu['max_mb']:.1f}MB")
        
        if "operations" in summary:
            print(f"\nâš¡ æ“ä½œæ€§èƒ½:")
            for op_name, stats in summary['operations'].items():
                print(f"  {op_name}:")
                print(f"    æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
                print(f"    å¹³å‡è€—æ—¶: {stats['avg_time']:.3f}ç§’")
                print(f"    æœ€å¤§è€—æ—¶: {stats['max_time']:.3f}ç§’")
                print(f"    æ€»è€—æ—¶: {stats['total_time']:.3f}ç§’")

class OperationTimer:
    """æ“ä½œè®¡æ—¶å™¨ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
    
    def __init__(self, monitor: PerformanceMonitor, operation_name: str):
        self.monitor = monitor
        self.operation_name = operation_name
        self.start_time = None
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time is not None:
            operation_time = time.time() - self.start_time
            self.monitor.record_operation(self.operation_name, operation_time)
```

### 4.2 åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬

**åˆ›å»º `test_performance.py`ï¼š**
```python
"""æ€§èƒ½æµ‹è¯•è„šæœ¬"""

from pathlib import Path
import time
from src.embedding.embedder import TextEmbedder
from src.vector_store.qdrant_client import QdrantVectorStore
from src.vector_store.document_vectorizer import DocumentVectorizer
from src.utils.performance_monitor import PerformanceMonitor, OperationTimer

def create_large_test_document():
    """åˆ›å»ºå¤§å‹æµ‹è¯•æ–‡æ¡£"""
    test_dir = Path("performance_test_docs")
    test_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºå¤§å‹æ–‡æ¡£å†…å®¹
    large_content = """
æ·±åº¦å­¦ä¹ ä¸äººå·¥æ™ºèƒ½æŠ€æœ¯è¯¦è§£

ç¬¬ä¸€ç« ï¼šäººå·¥æ™ºèƒ½åŸºç¡€

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½è¡Œä¸ºçš„ç³»ç»Ÿã€‚è‡ª1956å¹´è¾¾ç‰¹èŒ…æ–¯ä¼šè®®é¦–æ¬¡æå‡ºäººå·¥æ™ºèƒ½æ¦‚å¿µä»¥æ¥ï¼Œè¿™ä¸€é¢†åŸŸç»å†äº†å¤šæ¬¡å‘å±•æµªæ½®ã€‚

äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒç›®æ ‡æ˜¯è®©æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€ç†è§£è‡ªç„¶è¯­è¨€ã€è¯†åˆ«æ¨¡å¼ç­‰ã€‚ç°ä»£AIç³»ç»Ÿä¸»è¦åŸºäºæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚

ç¬¬äºŒç« ï¼šæœºå™¨å­¦ä¹ åŸºç¡€

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›çš„ç®—æ³•ã€‚æœºå™¨å­¦ä¹ å¯ä»¥åˆ†ä¸ºä¸‰å¤§ç±»ï¼šç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚

ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚å¸¸è§çš„ç›‘ç£å­¦ä¹ ä»»åŠ¡åŒ…æ‹¬åˆ†ç±»å’Œå›å½’ã€‚åˆ†ç±»ä»»åŠ¡é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ç­‰ã€‚å›å½’ä»»åŠ¡é¢„æµ‹è¿ç»­çš„æ•°å€¼ï¼Œå¦‚æˆ¿ä»·é¢„æµ‹ã€è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ç­‰ã€‚

æ— ç›‘ç£å­¦ä¹ å¤„ç†æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œç›®æ ‡æ˜¯å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼å’Œç»“æ„ã€‚èšç±»åˆ†ææ˜¯æ— ç›‘ç£å­¦ä¹ çš„å…¸å‹åº”ç”¨ï¼Œå®ƒå°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„ã€‚é™ç»´æŠ€æœ¯å¦‚ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¹Ÿå±äºæ— ç›‘ç£å­¦ä¹ ï¼Œç”¨äºå‡å°‘æ•°æ®çš„ç»´åº¦åŒæ—¶ä¿æŒé‡è¦ä¿¡æ¯ã€‚

å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œæ ¹æ®è·å¾—çš„å¥–åŠ±æˆ–æƒ©ç½šæ¥è°ƒæ•´è¡Œä¸ºç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚

ç¬¬ä¸‰ç« ï¼šæ·±åº¦å­¦ä¹ æŠ€æœ¯

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼ŒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯å…·æœ‰å¤šä¸ªéšè—å±‚çš„æ·±å±‚ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢æ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚

ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒæ˜¯ç¥ç»å…ƒï¼Œå®ƒæ¥æ”¶å¤šä¸ªè¾“å…¥ï¼Œé€šè¿‡æƒé‡åŠ æƒæ±‚å’Œï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°äº§ç”Ÿè¾“å‡ºã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ReLUã€Sigmoidã€Tanhç­‰ã€‚ReLUå‡½æ•°å› å…¶ç®€å•æ€§å’Œæœ‰æ•ˆæ€§è€Œè¢«å¹¿æ³›ä½¿ç”¨ã€‚

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†å…·æœ‰ç½‘æ ¼ç»“æ„çš„æ•°æ®ï¼Œå¦‚å›¾åƒã€‚CNNé€šè¿‡å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚çš„ç»„åˆæ¥æå–å›¾åƒç‰¹å¾ã€‚å·ç§¯å±‚ä½¿ç”¨æ»¤æ³¢å™¨æ‰«æè¾“å…¥å›¾åƒï¼Œæ£€æµ‹å±€éƒ¨ç‰¹å¾ã€‚æ± åŒ–å±‚å‡å°‘ç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€‚ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€æ—¶é—´åºåˆ—ç­‰ã€‚RNNå…·æœ‰è®°å¿†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåˆ©ç”¨ä¹‹å‰çš„ä¿¡æ¯æ¥å¤„ç†å½“å‰è¾“å…¥ã€‚é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰æ˜¯RNNçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿åºåˆ—ä¾èµ–é—®é¢˜ã€‚

Transformeræ¶æ„å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶å…³æ³¨ä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œè€Œä¸éœ€è¦å¾ªç¯ç»“æ„ã€‚è¿™ä½¿å¾—Transformerèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œå¤§å¤§æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

ç¬¬å››ç« ï¼šè‡ªç„¶è¯­è¨€å¤„ç†

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚NLPçš„ç›®æ ‡æ˜¯è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

æ–‡æœ¬é¢„å¤„ç†æ˜¯NLPçš„ç¬¬ä¸€æ­¥ï¼ŒåŒ…æ‹¬åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ã€å¥æ³•åˆ†æç­‰ä»»åŠ¡ã€‚åˆ†è¯å°†æ–‡æœ¬åˆ†è§£ä¸ºå•è¯æˆ–å­è¯å•å…ƒã€‚è¯æ€§æ ‡æ³¨ä¸ºæ¯ä¸ªè¯åˆ†é…è¯­æ³•ç±»åˆ«ï¼Œå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ã€‚å‘½åå®ä½“è¯†åˆ«è¯†åˆ«æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€ç»„ç»‡åç­‰å®ä½“ã€‚

è¯å‘é‡è¡¨ç¤ºæ˜¯NLPçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚Word2Vecã€GloVeã€FastTextç­‰æ–¹æ³•å°†è¯è¯­æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ã€‚è¿™äº›è¯å‘é‡å¯ä»¥æ•è·è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚

é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚BERTã€GPTã€T5ç­‰åœ¨NLPé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚è¿™äº›æ¨¡å‹åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œç„¶ååœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚BERTä½¿ç”¨åŒå‘ç¼–ç å™¨ç»“æ„ï¼Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚GPTé‡‡ç”¨è‡ªå›å½’ç”Ÿæˆæ¨¡å¼ï¼Œæ“…é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

ç¬¬äº”ç« ï¼šè®¡ç®—æœºè§†è§‰

è®¡ç®—æœºè§†è§‰è‡´åŠ›äºè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè§£é‡Šè§†è§‰ä¿¡æ¯ã€‚å›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰çš„åŸºç¡€ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å°†å›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚ç›®æ ‡æ£€æµ‹ä¸ä»…è¦è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ï¼Œè¿˜è¦å®šä½å®ƒä»¬çš„ä½ç½®ã€‚è¯­ä¹‰åˆ†å‰²ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾ã€‚

å·ç§¯ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ã€‚LeNetã€AlexNetã€VGGã€ResNetã€DenseNetç­‰ç»å…¸ç½‘ç»œæ¶æ„æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰çš„å‘å±•ã€‚æ®‹å·®è¿æ¥ã€æ‰¹é‡å½’ä¸€åŒ–ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰æŠ€æœ¯è¿›ä¸€æ­¥æå‡äº†ç½‘ç»œæ€§èƒ½ã€‚

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å¼€åˆ›äº†å›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒã€‚GANç”±ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç»„æˆï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€æ‰©æ•£æ¨¡å‹ç­‰ä¹Ÿæ˜¯é‡è¦çš„ç”Ÿæˆæ¨¡å‹ã€‚

ç¬¬å…­ç« ï¼šåº”ç”¨ä¸æœªæ¥

äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚åœ¨åŒ»ç–—å¥åº·é¢†åŸŸï¼ŒAIç”¨äºåŒ»å­¦å½±åƒåˆ†æã€è¯ç‰©å‘ç°ã€ç–¾ç—…è¯Šæ–­ç­‰ã€‚åœ¨é‡‘èé¢†åŸŸï¼ŒAIåº”ç”¨äºé£é™©è¯„ä¼°ã€ç®—æ³•äº¤æ˜“ã€æ¬ºè¯ˆæ£€æµ‹ç­‰ã€‚åœ¨æ•™è‚²é¢†åŸŸï¼ŒAIæ”¯æŒä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¾…å¯¼ã€è‡ªåŠ¨è¯„åˆ†ç­‰ã€‚

è‡ªåŠ¨é©¾é©¶æ˜¯AIæŠ€æœ¯çš„é‡è¦åº”ç”¨æ–¹å‘ï¼Œæ¶‰åŠè®¡ç®—æœºè§†è§‰ã€ä¼ æ„Ÿå™¨èåˆã€è·¯å¾„è§„åˆ’ã€å†³ç­–æ§åˆ¶ç­‰å¤šä¸ªæŠ€æœ¯é¢†åŸŸã€‚æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹å¦‚Siriã€Alexaã€å°çˆ±åŒå­¦ç­‰å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»çš„ä¸€éƒ¨åˆ†ã€‚

å¤§è¯­è¨€æ¨¡å‹å¦‚GPT-3ã€ChatGPTã€GPT-4ç­‰å±•ç°äº†å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬åˆ›ä½œã€ä»£ç ç”Ÿæˆç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å¤šæ¨¡æ€AIèƒ½å¤ŸåŒæ—¶å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„ä¿¡æ¯ã€‚

äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘åŒ…æ‹¬é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰ã€å¯è§£é‡ŠAIã€è”é‚¦å­¦ä¹ ã€è¾¹ç¼˜è®¡ç®—ç­‰ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒæ—¶ä¹Ÿéœ€è¦å…³æ³¨AIçš„ä¼¦ç†ã€å®‰å…¨ã€éšç§ç­‰é—®é¢˜ã€‚

ç»“è¯­

äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åˆ»æ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚æŒæ¡AIæŠ€æœ¯çš„åŸºæœ¬åŸç†å’Œåº”ç”¨æ–¹æ³•ï¼Œå¯¹äºåœ¨æ•°å­—åŒ–æ—¶ä»£ä¿æŒç«äº‰åŠ›è‡³å…³é‡è¦ã€‚é€šè¿‡ä¸æ–­å­¦ä¹ å’Œå®è·µï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨AIæŠ€æœ¯è§£å†³å®é™…é—®é¢˜ï¼Œæ¨åŠ¨ç¤¾ä¼šè¿›æ­¥ã€‚
    """ * 5  # é‡å¤5æ¬¡ä»¥åˆ›å»ºæ›´å¤§çš„æ–‡æ¡£
    
    # åˆ›å»ºå¤šä¸ªå¤§å‹æ–‡æ¡£
    for i in range(3):
        file_path = test_dir / f"large_document_{i+1}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"æ–‡æ¡£ {i+1}\n\n" + large_content)
    
    print(f"âœ… åˆ›å»ºäº†3ä¸ªå¤§å‹æµ‹è¯•æ–‡æ¡£åœ¨ {test_dir}")
    return test_dir

def test_embedding_performance():
    """æµ‹è¯•å‘é‡åŒ–æ€§èƒ½"""
    print("\nğŸš€ æµ‹è¯•å‘é‡åŒ–æ€§èƒ½...")
    
    # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
    monitor = PerformanceMonitor(monitor_interval=0.5)
    monitor.start_monitoring()
    
    try:
        # æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ€§èƒ½
        models = [
            'all-MiniLM-L6-v2',
            'paraphrase-MiniLM-L6-v2'
        ]
        
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºè¯„ä¼°å‘é‡åŒ–æ€§èƒ½ã€‚" * 10,
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºæ™ºèƒ½ç³»ç»Ÿã€‚" * 10,
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼å’Œè¡¨ç¤ºã€‚" * 10
        ] * 20  # åˆ›å»º60ä¸ªæµ‹è¯•æ–‡æœ¬
        
        for model_name in models:
            print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")
            
            # åˆå§‹åŒ–åµŒå…¥å™¨
            with OperationTimer(monitor, f"init_{model_name}"):
                embedder = TextEmbedder(model_name=model_name, batch_size=16)
            
            # æµ‹è¯•å•ä¸ªæ–‡æœ¬å‘é‡åŒ–
            with OperationTimer(monitor, f"single_embed_{model_name}"):
                single_embedding = embedder.embed_text(test_texts[0])
            
            print(f"  å•ä¸ªæ–‡æœ¬å‘é‡ç»´åº¦: {len(single_embedding)}")
            
            # æµ‹è¯•æ‰¹é‡å‘é‡åŒ–
            batch_sizes = [8, 16, 32]
            for batch_size in batch_sizes:
                embedder.batch_size = batch_size
                
                with OperationTimer(monitor, f"batch_embed_{model_name}_{batch_size}"):
                    batch_embeddings = embedder.embed_batch(test_texts)
                
                print(f"  æ‰¹é‡å¤§å° {batch_size}: å¤„ç†äº† {len(batch_embeddings)} ä¸ªæ–‡æœ¬")
    
    finally:
        monitor.stop_monitoring()
    
    # æ‰“å°æ€§èƒ½æ‘˜è¦
    monitor.print_summary()
    
    # ä¿å­˜æ€§èƒ½æ•°æ®
    monitor.save_metrics(Path("embedding_performance.json"))
    
    return monitor

def test_vectorization_performance():
    """æµ‹è¯•å®Œæ•´å‘é‡åŒ–æµç¨‹æ€§èƒ½"""
    print("\nğŸ“Š æµ‹è¯•å®Œæ•´å‘é‡åŒ–æµç¨‹æ€§èƒ½...")
    
    # åˆ›å»ºå¤§å‹æµ‹è¯•æ–‡æ¡£
    test_dir = create_large_test_document()
    
    # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
    monitor = PerformanceMonitor(monitor_interval=1.0)
    monitor.start_monitoring()
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        with OperationTimer(monitor, "init_components"):
            embedder = TextEmbedder(model_name='all-MiniLM-L6-v2', batch_size=16)
            vector_store = QdrantVectorStore()
            vectorizer = DocumentVectorizer(
                embedder=embedder,
                vector_store=vector_store,
                collection_name="performance_test"
            )
        
        # æµ‹è¯•ä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½
        chunk_sizes = [200, 400, 800]
        
        for chunk_size in chunk_sizes:
            print(f"\næµ‹è¯•åˆ†å—å¤§å°: {chunk_size}")
            
            collection_name = f"perf_test_{chunk_size}"
            vectorizer.collection_name = collection_name
            
            # æ‰¹é‡å¤„ç†æ–‡æ¡£
            with OperationTimer(monitor, f"process_docs_{chunk_size}"):
                result = vectorizer.process_directory(
                    directory=test_dir,
                    chunk_strategy="sentence",
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_size // 10,
                    file_patterns=['*.txt']
                )
            
            print(f"  å¤„ç†ç»“æœ: {result['total_chunks']} ä¸ªåˆ†å—, {result['total_vectors']} ä¸ªå‘é‡")
            
            # æµ‹è¯•æœç´¢æ€§èƒ½
            queries = [
                "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                "äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸ",
                "ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†",
                "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
                "è®¡ç®—æœºè§†è§‰çš„å‘å±•"
            ]
            
            for i, query in enumerate(queries):
                with OperationTimer(monitor, f"search_{chunk_size}_{i}"):
                    search_results = vectorizer.search_documents(
                        query=query,
                        limit=10,
                        score_threshold=0.1
                    )
                
                print(f"  æŸ¥è¯¢ '{query[:10]}...': æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
            
            # æ¸…ç†é›†åˆ
            vector_store.delete_collection(collection_name)
    
    finally:
        monitor.stop_monitoring()
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        if test_dir.exists():
            shutil.rmtree(test_dir)
    
    # æ‰“å°æ€§èƒ½æ‘˜è¦
    monitor.print_summary()
    
    # ä¿å­˜æ€§èƒ½æ•°æ®
    monitor.save_metrics(Path("vectorization_performance.json"))
    
    return monitor

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
    
    monitor = PerformanceMonitor(monitor_interval=0.5)
    monitor.start_monitoring()
    
    try:
        # åˆ›å»ºå¤§é‡æ–‡æœ¬æ•°æ®
        large_texts = []
        for i in range(1000):
            text = f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚" * 50
            large_texts.append(text)
        
        print(f"åˆ›å»ºäº† {len(large_texts)} ä¸ªæ–‡æœ¬")
        
        # åˆå§‹åŒ–åµŒå…¥å™¨
        embedder = TextEmbedder(model_name='all-MiniLM-L6-v2')
        
        # æ‰¹é‡å¤„ç†
        batch_size = 32
        embeddings = []
        
        for i in range(0, len(large_texts), batch_size):
            batch = large_texts[i:i+batch_size]
            
            with OperationTimer(monitor, f"batch_process_{i//batch_size}"):
                batch_embeddings = embedder.embed_batch(batch)
                embeddings.extend(batch_embeddings)
            
            if i % (batch_size * 10) == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(large_texts)}")
        
        print(f"å®Œæˆå¤„ç†ï¼Œç”Ÿæˆäº† {len(embeddings)} ä¸ªå‘é‡")
        
        # æ¸…ç†å†…å­˜
        del large_texts
        del embeddings
        
    finally:
        monitor.stop_monitoring()
    
    # æ‰“å°å†…å­˜ä½¿ç”¨æ‘˜è¦
    monitor.print_summary()
    
    return monitor

if __name__ == "__main__":
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    
    try:
        # æµ‹è¯•å‘é‡åŒ–æ€§èƒ½
        embedding_monitor = test_embedding_performance()
        
        print("\n" + "="*50)
        
        # æµ‹è¯•å®Œæ•´æµç¨‹æ€§èƒ½
        vectorization_monitor = test_vectorization_performance()
        
        print("\n" + "="*50)
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        memory_monitor = test_memory_usage()
        
        print("\nğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        
        # æ¸…ç†æ€§èƒ½æ•°æ®æ–‡ä»¶
        performance_files = [
            "embedding_performance.json",
            "vectorization_performance.json"
        ]
        
        for file_path in performance_files:
            if Path(file_path).exists():
                Path(file_path).unlink()
        
    except Exception as e:
        print(f"\nğŸ’¥ æ€§èƒ½æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
```

**è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼š**
```bash
# å®‰è£…æ€§èƒ½ç›‘æ§ä¾èµ–
uv add psutil

# è¿è¡Œæ€§èƒ½æµ‹è¯•
python test_performance.py
```

---

## ğŸ¤” æ€è€ƒé¢˜

1. **æ¨¡å‹é€‰æ‹©ä¸æ€§èƒ½**ï¼š
   - æ¯”è¾ƒä¸åŒsentence-transformersæ¨¡å‹çš„å‘é‡åŒ–é€Ÿåº¦å’Œè´¨é‡
   - åˆ†ææ¨¡å‹å¤§å°ä¸æ€§èƒ½çš„æƒè¡¡å…³ç³»
   - å¦‚ä½•æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ

2. **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼š
   - æ‰¹å¤„ç†å¤§å°å¯¹æ€§èƒ½çš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿ
   - å¦‚ä½•å¹³è¡¡å†…å­˜ä½¿ç”¨å’Œå¤„ç†é€Ÿåº¦ï¼Ÿ
   - åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ä½¿ç”¨GPUåŠ é€Ÿï¼Ÿ

3. **å‘é‡å­˜å‚¨ä¼˜åŒ–**ï¼š
   - Qdrantçš„ä¸åŒç´¢å¼•ç±»å‹å¯¹æœç´¢æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ
   - å¦‚ä½•ä¼˜åŒ–å‘é‡ç»´åº¦ä»¥å¹³è¡¡å­˜å‚¨ç©ºé—´å’Œæœç´¢ç²¾åº¦ï¼Ÿ
   - åˆ†ç‰‡å’Œå¤åˆ¶ç­–ç•¥å¦‚ä½•å½±å“ç³»ç»Ÿæ€§èƒ½ï¼Ÿ

4. **åˆ†å—ç­–ç•¥è¯„ä¼°**ï¼š
   - ä¸åŒåˆ†å—å¤§å°å¯¹æœç´¢ç»“æœè´¨é‡çš„å½±å“
   - é‡å å¤§å°å¦‚ä½•å½±å“ä¿¡æ¯å®Œæ•´æ€§ï¼Ÿ
   - å¦‚ä½•è®¾è®¡è‡ªé€‚åº”åˆ†å—ç­–ç•¥ï¼Ÿ

5. **ç³»ç»Ÿæ‰©å±•æ€§**ï¼š
   - å¦‚ä½•è®¾è®¡æ”¯æŒç™¾ä¸‡çº§æ–‡æ¡£çš„å‘é‡åŒ–ç³»ç»Ÿï¼Ÿ
   - å¢é‡æ›´æ–°å’Œåˆ é™¤æ“ä½œå¦‚ä½•å®ç°ï¼Ÿ
   - å¦‚ä½•å®ç°åˆ†å¸ƒå¼å‘é‡åŒ–å¤„ç†ï¼Ÿ

---

## âœ… å®éªŒæ£€æŸ¥æ¸…å•

### ç¯å¢ƒå‡†å¤‡
- [ ] æˆåŠŸå®‰è£…sentence-transformers
- [ ] QdrantæœåŠ¡æ­£å¸¸è¿è¡Œ
- [ ] æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆ
- [ ] ç¯å¢ƒæµ‹è¯•è„šæœ¬è¿è¡Œæ­£å¸¸

### å‘é‡åŒ–åŠŸèƒ½
- [ ] TextEmbedderç±»æ­£å¸¸å·¥ä½œ
- [ ] æ”¯æŒå•ä¸ªæ–‡æœ¬å’Œæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
- [ ] æ¨¡å‹åŠ è½½å’Œç¼“å­˜æœºåˆ¶æ­£å¸¸
- [ ] å‘é‡ç»´åº¦å’Œæ ¼å¼æ­£ç¡®

### å‘é‡å­˜å‚¨
- [ ] QdrantVectorStoreè¿æ¥æ­£å¸¸
- [ ] é›†åˆåˆ›å»ºå’Œåˆ é™¤åŠŸèƒ½æ­£å¸¸
- [ ] å‘é‡æ’å…¥å’Œæœç´¢åŠŸèƒ½æ­£å¸¸
- [ ] è¿‡æ»¤æœç´¢åŠŸèƒ½æ­£å¸¸

### æ–‡æ¡£å¤„ç†
- [ ] DocumentVectorizeré›†æˆæ­£å¸¸
- [ ] å•ä¸ªæ–‡æ¡£å¤„ç†åŠŸèƒ½æ­£å¸¸
- [ ] æ‰¹é‡æ–‡æ¡£å¤„ç†åŠŸèƒ½æ­£å¸¸
- [ ] å¤„ç†æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯æ­£ç¡®

### æ€§èƒ½ç›‘æ§
- [ ] PerformanceMonitoræ­£å¸¸å·¥ä½œ
- [ ] ç³»ç»Ÿèµ„æºç›‘æ§å‡†ç¡®
- [ ] æ“ä½œè®¡æ—¶åŠŸèƒ½æ­£å¸¸
- [ ] æ€§èƒ½æŠ¥å‘Šç”Ÿæˆæ­£ç¡®

### æµ‹è¯•éªŒè¯
- [ ] æ‰€æœ‰æµ‹è¯•è„šæœ¬è¿è¡ŒæˆåŠŸ
- [ ] æœç´¢ç»“æœç›¸å…³æ€§åˆç†
- [ ] æ€§èƒ½æŒ‡æ ‡åœ¨é¢„æœŸèŒƒå›´å†…
- [ ] é”™è¯¯å¤„ç†æœºåˆ¶æœ‰æ•ˆ

---

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

### 1. æ¨¡å‹ä¸‹è½½é—®é¢˜
```bash
# é—®é¢˜ï¼šæ¨¡å‹ä¸‹è½½ç¼“æ…¢æˆ–å¤±è´¥
# è§£å†³ï¼šè®¾ç½®é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder='./models')
```

### 2. Qdrantè¿æ¥é—®é¢˜
```python
# é—®é¢˜ï¼šè¿æ¥Qdrantå¤±è´¥
# è§£å†³ï¼šæ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œé…ç½®
import requests

try:
    response = requests.get('http://localhost:6333/health')
    print(f"QdrantçŠ¶æ€: {response.status_code}")
except Exception as e:
    print(f"è¿æ¥å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥QdrantæœåŠ¡æ˜¯å¦å¯åŠ¨")
```

### 3. å†…å­˜ä¸è¶³é—®é¢˜
```python
# é—®é¢˜ï¼šå¤„ç†å¤§æ–‡æ¡£æ—¶å†…å­˜ä¸è¶³
# è§£å†³ï¼šè°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œæ¸…ç†ç­–ç•¥

# å‡å°æ‰¹å¤„ç†å¤§å°
embedder = TextEmbedder(batch_size=8)  # é»˜è®¤32æ”¹ä¸º8

# åŠæ—¶æ¸…ç†å˜é‡
import gc
del large_data
gc.collect()
```

### 4. å‘é‡åŒ–é€Ÿåº¦æ…¢
```python
# é—®é¢˜ï¼šå‘é‡åŒ–é€Ÿåº¦å¤ªæ…¢
# è§£å†³ï¼šå¯ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰

import torch
if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

embedder = TextEmbedder(device=device)
```

### 5. æœç´¢ç»“æœä¸å‡†ç¡®
```python
# é—®é¢˜ï¼šæœç´¢ç»“æœç›¸å…³æ€§å·®
# è§£å†³ï¼šè°ƒæ•´æœç´¢å‚æ•°å’Œåˆ†å—ç­–ç•¥

# é™ä½åˆ†æ•°é˜ˆå€¼
results = vectorizer.search_documents(
    query=query,
    limit=10,
    score_threshold=0.1  # ä»0.3é™ä½åˆ°0.1
)

# è°ƒæ•´åˆ†å—å¤§å°
result = vectorizer.process_document(
    chunk_size=300,  # å¢åŠ åˆ†å—å¤§å°
    chunk_overlap=50  # å¢åŠ é‡å 
)
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **Sentence Transformers**ï¼š
   - [å®˜æ–¹æ–‡æ¡£](https://www.sbert.net/)
   - [æ¨¡å‹åˆ—è¡¨](https://www.sbert.net/docs/pretrained_models.html)
   - [è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹](https://www.sbert.net/docs/training/overview.html)

2. **Qdrantå‘é‡æ•°æ®åº“**ï¼š
   - [å®˜æ–¹æ–‡æ¡£](https://qdrant.tech/documentation/)
   - [Pythonå®¢æˆ·ç«¯](https://github.com/qdrant/qdrant-client)
   - [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://qdrant.tech/documentation/guides/optimization/)

3. **å‘é‡åŒ–æœ€ä½³å®è·µ**ï¼š
   - [æ–‡æœ¬åµŒå…¥æŒ‡å—](https://huggingface.co/blog/getting-started-with-embeddings)
   - [å‘é‡æœç´¢ä¼˜åŒ–](https://www.pinecone.io/learn/vector-search-optimization/)
   - [RAGç³»ç»Ÿè®¾è®¡](https://www.anthropic.com/research/retrieval-augmented-generation)

4. **æ€§èƒ½ç›‘æ§**ï¼š
   - [psutilæ–‡æ¡£](https://psutil.readthedocs.io/)
   - [Pythonæ€§èƒ½åˆ†æ](https://docs.python.org/3/library/profile.html)
   - [å†…å­˜ä¼˜åŒ–æŠ€å·§](https://realpython.com/python-memory-management/)

---

## ğŸ“ å®éªŒå®Œæˆåçš„Gitæ“ä½œ

### ä¸ºä»€ä¹ˆè¦è¿›è¡ŒGitæäº¤ï¼Ÿ

å®Œæˆå‘é‡åŒ–å’Œå‘é‡å­˜å‚¨å®éªŒåï¼Œè¿›è¡ŒGitæäº¤éå¸¸é‡è¦ï¼š

1. **å‘é‡åŒ–ä»£ç ä¿å­˜**ï¼šä¿å­˜TextEmbedderã€QdrantVectorStoreç­‰æ ¸å¿ƒç»„ä»¶çš„å®ç°
2. **æ¨¡å‹é…ç½®ç®¡ç†**ï¼šè®°å½•sentence-transformersæ¨¡å‹é€‰æ‹©å’Œé…ç½®å‚æ•°
3. **å‘é‡å­˜å‚¨æ–¹æ¡ˆ**ï¼šä¿å­˜Qdranté›†åˆé…ç½®å’Œç´¢å¼•ç­–ç•¥
4. **æ€§èƒ½ä¼˜åŒ–è®°å½•**ï¼šè®°å½•æ‰¹å¤„ç†ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†æ”¹è¿›
5. **å®éªŒç»“æœè¿½è¸ª**ï¼šä¿å­˜å‘é‡åŒ–æµ‹è¯•ç»“æœå’Œæ€§èƒ½åŸºå‡†
6. **å›¢é˜Ÿåä½œ**ï¼šè®©å›¢é˜Ÿæˆå‘˜äº†è§£å‘é‡åŒ–å®ç°çš„æœ€æ–°è¿›å±•
7. **ç‰ˆæœ¬å¯¹æ¯”**ï¼šæ–¹ä¾¿åç»­å¯¹æ¯”ä¸åŒå‘é‡åŒ–ç­–ç•¥çš„æ•ˆæœ

### Gitæ“ä½œæ­¥éª¤

#### 1. æ£€æŸ¥å½“å‰ä¿®æ”¹çŠ¶æ€
```bash
git status
```

**é¢„æœŸçœ‹åˆ°çš„æ–‡ä»¶å˜æ›´**ï¼š
- `src/core/embedder.py` - TextEmbedderå®ç°
- `src/core/vector_store.py` - QdrantVectorStoreå®ç°
- `src/core/document_vectorizer.py` - æ–‡æ¡£å‘é‡åŒ–å™¨
- `src/utils/performance_monitor.py` - æ€§èƒ½ç›‘æ§å·¥å…·
- `tests/test_embedding.py` - å‘é‡åŒ–æµ‹è¯•
- `tests/test_vector_store.py` - å‘é‡å­˜å‚¨æµ‹è¯•
- `requirements.txt` - æ–°å¢sentence-transformersç­‰ä¾èµ–
- `config/embedding_config.py` - å‘é‡åŒ–é…ç½®æ–‡ä»¶
- å¯èƒ½çš„å…¶ä»–é…ç½®å’Œæµ‹è¯•æ–‡ä»¶

#### 2. æ·»åŠ ä¿®æ”¹çš„æ–‡ä»¶
```bash
# æ·»åŠ æ‰€æœ‰ä¿®æ”¹çš„æ–‡ä»¶
git add .

# æˆ–è€…é€‰æ‹©æ€§æ·»åŠ ç‰¹å®šæ–‡ä»¶
git add src/core/embedder.py
git add src/core/vector_store.py
git add src/core/document_vectorizer.py
git add src/utils/performance_monitor.py
git add tests/test_embedding.py
git add tests/test_vector_store.py
git add requirements.txt
```

#### 3. æäº¤æ›´æ”¹
```bash
git commit -m "å®Œæˆlesson05å®éªŒï¼šå®ç°å‘é‡åŒ–ä¸å‘é‡å…¥åº“åŠŸèƒ½

- å®ç°TextEmbedderç±»ï¼Œæ”¯æŒsentence-transformersæ¨¡å‹
- å®ç°QdrantVectorStoreï¼Œæ”¯æŒå‘é‡å­˜å‚¨å’Œæœç´¢
- å®ç°DocumentVectorizerï¼Œé›†æˆæ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–
- æ·»åŠ PerformanceMonitoræ€§èƒ½ç›‘æ§å·¥å…·
- æ”¯æŒæ‰¹é‡å¤„ç†å’ŒGPUåŠ é€Ÿ
- å®Œæˆå‘é‡åŒ–å’Œæœç´¢åŠŸèƒ½æµ‹è¯•
- ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œå¤„ç†æ€§èƒ½"
```

#### 4. æŸ¥çœ‹æäº¤å†å²
```bash
git log --oneline -5
```

#### 5. æ¨é€åˆ°è¿œç¨‹ä»“åº“ï¼ˆå¯é€‰ï¼‰
```bash
# å¦‚æœéœ€è¦æ¨é€åˆ°è¿œç¨‹ä»“åº“
git push origin lesson05-embedding

# æˆ–æ¨é€åˆ°ä¸»åˆ†æ”¯ï¼ˆæ ¹æ®ä½ çš„åˆ†æ”¯ç­–ç•¥ï¼‰
git push origin main
```

### æäº¤å‰éªŒè¯æ¸…å•

åœ¨æäº¤ä¹‹å‰ï¼Œè¯·ç¡®ä¿ï¼š

- [ ] **å‘é‡åŒ–åŠŸèƒ½éªŒè¯**ï¼šTextEmbedderèƒ½æ­£å¸¸åŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆå‘é‡
- [ ] **å‘é‡å­˜å‚¨éªŒè¯**ï¼šQdrantVectorStoreèƒ½æ­£å¸¸è¿æ¥å’Œæ“ä½œ
- [ ] **æœç´¢åŠŸèƒ½éªŒè¯**ï¼šå‘é‡æœç´¢è¿”å›åˆç†çš„ç›¸å…³æ€§ç»“æœ
- [ ] **æ€§èƒ½ç›‘æ§éªŒè¯**ï¼šPerformanceMonitorèƒ½æ­£ç¡®ç»Ÿè®¡èµ„æºä½¿ç”¨
- [ ] **æµ‹è¯•é€šè¿‡**ï¼šæ‰€æœ‰ç›¸å…³æµ‹è¯•ç”¨ä¾‹è¿è¡ŒæˆåŠŸ
- [ ] **ä¾èµ–å®Œæ•´**ï¼šrequirements.txtåŒ…å«æ‰€æœ‰å¿…è¦çš„ä¾èµ–åŒ…
- [ ] **é…ç½®æ­£ç¡®**ï¼šå‘é‡åŒ–å’Œå­˜å‚¨é…ç½®å‚æ•°åˆç†
- [ ] **æ–‡æ¡£æ›´æ–°**ï¼šç›¸å…³æ–‡æ¡£å’Œæ³¨é‡Šå·²æ›´æ–°

### å‘é‡åŒ–é¡¹ç›®ç‰¹æ®Šæ³¨æ„äº‹é¡¹

1. **æ¨¡å‹æ–‡ä»¶ç®¡ç†**ï¼š
   ```bash
   # æ£€æŸ¥.gitignoreæ˜¯å¦æ’é™¤äº†å¤§å‹æ¨¡å‹æ–‡ä»¶
   echo "models/" >> .gitignore
   echo "*.bin" >> .gitignore
   echo "*.safetensors" >> .gitignore
   ```

2. **å‘é‡æ•°æ®æ’é™¤**ï¼š
   ```bash
   # æ’é™¤å‘é‡æ•°æ®æ–‡ä»¶ï¼ˆé€šå¸¸å¾ˆå¤§ï¼‰
   echo "data/vectors/" >> .gitignore
   echo "*.npy" >> .gitignore
   echo "*.pkl" >> .gitignore
   ```

3. **æ•æ„Ÿé…ç½®ä¿æŠ¤**ï¼š
   ```bash
   # æ£€æŸ¥æ˜¯å¦æœ‰æ•æ„Ÿçš„APIå¯†é’¥æˆ–è¿æ¥ä¿¡æ¯
   grep -r "api_key\|password\|secret" src/ || echo "æœªå‘ç°æ•æ„Ÿä¿¡æ¯"
   ```

### å¸¸è§é—®é¢˜è§£å†³

1. **å¤§æ–‡ä»¶æäº¤é—®é¢˜**ï¼š
   ```bash
   # å¦‚æœæ„å¤–æ·»åŠ äº†å¤§å‹æ¨¡å‹æ–‡ä»¶
   git reset HEAD models/
   git rm --cached models/*.bin
   ```

2. **æäº¤ä¿¡æ¯ä¿®æ”¹**ï¼š
   ```bash
   # ä¿®æ”¹æœ€åä¸€æ¬¡æäº¤ä¿¡æ¯
   git commit --amend -m "æ–°çš„æäº¤ä¿¡æ¯"
   ```

3. **æ’¤é”€æ–‡ä»¶æ·»åŠ **ï¼š
   ```bash
   # æ’¤é”€git addæ“ä½œ
   git reset HEAD <æ–‡ä»¶å>
   ```

### å‘é‡åŒ–é¡¹ç›®Gitæœ€ä½³å®è·µ

1. **åˆ†å±‚æäº¤**ï¼š
   - å…ˆæäº¤æ ¸å¿ƒå‘é‡åŒ–åŠŸèƒ½
   - å†æäº¤æ€§èƒ½ä¼˜åŒ–æ”¹è¿›
   - æœ€åæäº¤æµ‹è¯•å’Œæ–‡æ¡£

2. **é…ç½®ç®¡ç†**ï¼š
   - ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†æ¨¡å‹å‚æ•°
   - ç‰ˆæœ¬æ§åˆ¶é…ç½®å˜æ›´
   - è®°å½•æ€§èƒ½åŸºå‡†æ•°æ®

3. **æ¨¡å‹ç‰ˆæœ¬ç®¡ç†**ï¼š
   - è®°å½•ä½¿ç”¨çš„æ¨¡å‹åç§°å’Œç‰ˆæœ¬
   - ä½¿ç”¨æ ‡ç­¾æ ‡è®°é‡è¦çš„æ¨¡å‹åˆ‡æ¢
   - ä¿æŒæ¨¡å‹é€‰æ‹©çš„å¯è¿½æº¯æ€§

4. **æ€§èƒ½åŸºå‡†è®°å½•**ï¼š
   ```bash
   # ä¸ºé‡è¦çš„æ€§èƒ½æ”¹è¿›æ‰“æ ‡ç­¾
   git tag -a v1.0-embedding-optimized -m "å‘é‡åŒ–æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"
   ```

### ä¸‹ä¸€æ­¥å­¦ä¹ æŒ‡å¯¼

å®ŒæˆGitæäº¤åï¼Œä½ å¯ä»¥ï¼š

1. **é¢„ä¹ lesson06**ï¼šäº†è§£æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„åŸºæœ¬æ¦‚å¿µ
2. **ç ”ç©¶æ£€ç´¢ç­–ç•¥**ï¼šå­¦ä¹ ä¸åŒçš„å‘é‡æ£€ç´¢å’Œé‡æ’åºæ–¹æ³•
3. **æ¢ç´¢ç”Ÿæˆæ¨¡å‹**ï¼šäº†è§£å¦‚ä½•é›†æˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆç”Ÿæˆ
4. **ä¼˜åŒ–æ£€ç´¢è´¨é‡**ï¼šç ”ç©¶å¦‚ä½•æé«˜æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§

è®°ä½ï¼Œè‰¯å¥½çš„ç‰ˆæœ¬æ§åˆ¶ä¹ æƒ¯æ˜¯ä¸“ä¸šå¼€å‘çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼

---

## ğŸ¯ å®éªŒå®Œæˆæ ‡å¿—

å®Œæˆæœ¬å®éªŒåï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

1. âœ… **ç†Ÿç»ƒä½¿ç”¨sentence-transformers**ï¼š
   - åŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
   - è¿›è¡Œå•ä¸ªå’Œæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
   - ç†è§£ä¸åŒæ¨¡å‹çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯

2. âœ… **æŒæ¡Qdrantå‘é‡æ•°æ®åº“**ï¼š
   - åˆ›å»ºå’Œç®¡ç†å‘é‡é›†åˆ
   - æ‰§è¡Œå‘é‡æ’å…¥å’Œæœç´¢æ“ä½œ
   - ä½¿ç”¨è¿‡æ»¤æ¡ä»¶è¿›è¡Œç²¾ç¡®æœç´¢

3. âœ… **å®ç°å®Œæ•´å‘é‡åŒ–æµç¨‹**ï¼š
   - é›†æˆæ–‡æ¡£è§£æã€åˆ†å—å’Œå‘é‡åŒ–
   - æ”¯æŒæ‰¹é‡æ–‡æ¡£å¤„ç†
   - å®ç°æœç´¢å’Œç»Ÿè®¡åŠŸèƒ½

4. âœ… **è¿›è¡Œæ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–**ï¼š
   - ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
   - åˆ†ææ“ä½œæ€§èƒ½ç“¶é¢ˆ
   - ä¼˜åŒ–æ‰¹å¤„ç†å’Œå†…å­˜ä½¿ç”¨

5. âœ… **è§£å†³å®é™…é—®é¢˜**ï¼š
   - å¤„ç†å¤§è§„æ¨¡æ–‡æ¡£å‘é‡åŒ–
   - ä¼˜åŒ–æœç´¢ç»“æœè´¨é‡
   - å®ç°é«˜æ•ˆçš„å‘é‡å­˜å‚¨å’Œæ£€ç´¢

6. âœ… **å®ŒæˆGitç‰ˆæœ¬æ§åˆ¶**ï¼š
   - æ‰€æœ‰ä»£ç å˜æ›´å·²æäº¤åˆ°Gitä»“åº“
   - æäº¤ä¿¡æ¯æ¸…æ™°æè¿°äº†å®ç°çš„åŠŸèƒ½
   - å‘é‡åŒ–é…ç½®å’Œæµ‹è¯•ç»“æœå·²ä¿å­˜

**æµ‹è¯•éªŒè¯**ï¼šæ‰€æœ‰æµ‹è¯•è„šæœ¬è¿è¡ŒæˆåŠŸï¼Œæ€§èƒ½æŒ‡æ ‡æ»¡è¶³è¦æ±‚ï¼Œèƒ½å¤Ÿå¤„ç†å®é™…è§„æ¨¡çš„æ–‡æ¡£æ•°æ®ï¼ŒGitæäº¤å®Œæˆã€‚
```