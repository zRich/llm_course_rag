# ç¬¬15èŠ‚è¯¾ï¼šæ‰¹é‡/æ–­ç‚¹ç»­ä¼  - å­¦ç”Ÿå®éªŒæŒ‡å¯¼

## å®éªŒç›®æ ‡
é€šè¿‡æœ¬å®éªŒï¼Œå­¦ç”Ÿå°†ï¼š
1. æŒæ¡å¼‚æ­¥æ‰¹é‡å¤„ç†çš„å®ç°æ–¹æ³•
2. ç†è§£æ–­ç‚¹ç»­ä¼ æœºåˆ¶çš„è®¾è®¡åŸç†
3. å­¦ä¼šå®ç°è¿›åº¦è·Ÿè¸ªå’Œé”™è¯¯æ¢å¤
4. æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ‰¹é‡æ–‡æ¡£å¤„ç†ç³»ç»Ÿ

## å®éªŒç¯å¢ƒå‡†å¤‡

### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- FastAPI
- asyncio
- aiofiles
- å‰é¢è¯¾ç¨‹çš„RAGç³»ç»Ÿä»£ç 

### ä¾èµ–å®‰è£…
```bash
# è¿›å…¥rag-systemç›®å½•
cd rag-system

# å®‰è£…æ–°çš„ä¾èµ–
uv add aiofiles
uv add python-multipart
```

## å®éªŒæ­¥éª¤

### æ­¥éª¤1ï¼šåˆ›å»ºæ‰¹é‡å¤„ç†æ ¸å¿ƒæ¨¡å—

åˆ›å»º `batch_processor.py` æ–‡ä»¶ï¼š

```python
import asyncio
import json
import logging
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
import aiofiles

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    doc_id: str
    success: bool
    error_message: Optional[str] = None
    processing_time: float = 0.0
    timestamp: str = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()

@dataclass
class BatchState:
    """æ‰¹å¤„ç†çŠ¶æ€æ•°æ®ç±»"""
    batch_id: str
    total_docs: int
    processed_docs: List[str]
    failed_docs: List[str]
    current_progress: int
    start_time: str
    last_checkpoint: str
    
    def __post_init__(self):
        if self.start_time is None:
            self.start_time = datetime.now().isoformat()
        if self.last_checkpoint is None:
            self.last_checkpoint = datetime.now().isoformat()

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, total_items: int, batch_id: str):
        self.total_items = total_items
        self.completed_items = 0
        self.failed_items = 0
        self.batch_id = batch_id
        self.start_time = datetime.now()
        self.callbacks = []
    
    def add_callback(self, callback):
        """æ·»åŠ è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°"""
        self.callbacks.append(callback)
    
    async def update_progress(self, success: bool = True, doc_id: str = None):
        """æ›´æ–°è¿›åº¦"""
        if success:
            self.completed_items += 1
        else:
            self.failed_items += 1
        
        progress_percent = (self.completed_items + self.failed_items) / self.total_items * 100
        
        progress_info = {
            'batch_id': self.batch_id,
            'progress_percent': progress_percent,
            'completed': self.completed_items,
            'failed': self.failed_items,
            'total': self.total_items,
            'eta': self.get_eta(),
            'doc_id': doc_id
        }
        
        logger.info(f"Progress: {progress_percent:.1f}% ({self.completed_items}/{self.total_items})")
        
        # è°ƒç”¨æ‰€æœ‰å›è°ƒå‡½æ•°
        for callback in self.callbacks:
            try:
                await callback(progress_info)
            except Exception as e:
                logger.error(f"Progress callback error: {e}")
    
    def get_eta(self) -> str:
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if self.completed_items == 0:
            return "Unknown"
        
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.completed_items / elapsed
        remaining_items = self.total_items - self.completed_items - self.failed_items
        eta_seconds = remaining_items / rate if rate > 0 else 0
        return f"{eta_seconds:.0f} seconds"

class CheckpointManager:
    """æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
    
    async def save_checkpoint(self, state: BatchState):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / f"{state.batch_id}.json"
        state.last_checkpoint = datetime.now().isoformat()
        
        async with aiofiles.open(checkpoint_file, 'w') as f:
            await f.write(json.dumps(asdict(state), indent=2))
        
        logger.info(f"Checkpoint saved for batch {state.batch_id}")
    
    async def load_checkpoint(self, batch_id: str) -> Optional[BatchState]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / f"{batch_id}.json"
        
        if not checkpoint_file.exists():
            return None
        
        try:
            async with aiofiles.open(checkpoint_file, 'r') as f:
                data = json.loads(await f.read())
            
            state = BatchState(**data)
            logger.info(f"Checkpoint loaded for batch {batch_id}")
            return state
        except Exception as e:
            logger.error(f"Failed to load checkpoint for batch {batch_id}: {e}")
            return None
    
    async def cleanup_checkpoint(self, batch_id: str):
        """æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        checkpoint_file = self.checkpoint_dir / f"{batch_id}.json"
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            logger.info(f"Checkpoint cleaned up for batch {batch_id}")

class ErrorRecoveryManager:
    """é”™è¯¯æ¢å¤ç®¡ç†å™¨"""
    
    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
    
    async def execute_with_retry(self, func, *args, **kwargs) -> Any:
        """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°æ‰§è¡Œ"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                
                if attempt < self.max_retries:
                    delay = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"All {self.max_retries + 1} attempts failed")
                    raise last_exception

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self, max_concurrent: int = 10, checkpoint_interval: int = 10):
        self.max_concurrent = max_concurrent
        self.checkpoint_interval = checkpoint_interval
        self.checkpoint_manager = CheckpointManager()
        self.error_recovery = ErrorRecoveryManager()
        self.active_batches = {}
    
    async def process_document(self, doc_path: str) -> ProcessingResult:
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        start_time = datetime.now()
        
        try:
            # æ¨¡æ‹Ÿæ–‡æ¡£å¤„ç†æ—¶é—´
            await asyncio.sleep(0.1)
            
            # æ¨¡æ‹Ÿå¶å‘é”™è¯¯ï¼ˆ10%æ¦‚ç‡ï¼‰
            import random
            if random.random() < 0.1:
                raise Exception(f"Simulated processing error for {doc_path}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return ProcessingResult(
                doc_id=doc_path,
                success=True,
                processing_time=processing_time
            )
        
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            return ProcessingResult(
                doc_id=doc_path,
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    async def process_batch(self, doc_paths: List[str], batch_id: str = None, 
                          resume: bool = False) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        if batch_id is None:
            batch_id = str(uuid.uuid4())
        
        # å°è¯•æ¢å¤ä¹‹å‰çš„çŠ¶æ€
        if resume:
            state = await self.checkpoint_manager.load_checkpoint(batch_id)
            if state:
                # è¿‡æ»¤æ‰å·²å¤„ç†çš„æ–‡æ¡£
                remaining_docs = [doc for doc in doc_paths 
                                if doc not in state.processed_docs and doc not in state.failed_docs]
                logger.info(f"Resuming batch {batch_id}, {len(remaining_docs)} documents remaining")
            else:
                logger.warning(f"No checkpoint found for batch {batch_id}, starting fresh")
                remaining_docs = doc_paths
                state = BatchState(
                    batch_id=batch_id,
                    total_docs=len(doc_paths),
                    processed_docs=[],
                    failed_docs=[],
                    current_progress=0,
                    start_time=datetime.now().isoformat(),
                    last_checkpoint=datetime.now().isoformat()
                )
        else:
            remaining_docs = doc_paths
            state = BatchState(
                batch_id=batch_id,
                total_docs=len(doc_paths),
                processed_docs=[],
                failed_docs=[],
                current_progress=0,
                start_time=datetime.now().isoformat(),
                last_checkpoint=datetime.now().isoformat()
            )
        
        # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
        progress_tracker = ProgressTracker(len(doc_paths), batch_id)
        progress_tracker.completed_items = len(state.processed_docs)
        progress_tracker.failed_items = len(state.failed_docs)
        
        # æ³¨å†Œæ‰¹å¤„ç†ä»»åŠ¡
        self.active_batches[batch_id] = {
            'state': state,
            'progress_tracker': progress_tracker,
            'start_time': datetime.now()
        }
        
        try:
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            async def process_with_semaphore(doc_path: str) -> ProcessingResult:
                async with semaphore:
                    return await self.error_recovery.execute_with_retry(
                        self.process_document, doc_path
                    )
            
            # å¤„ç†å‰©ä½™æ–‡æ¡£
            results = []
            checkpoint_counter = 0
            
            for doc_path in remaining_docs:
                result = await process_with_semaphore(doc_path)
                results.append(result)
                
                # æ›´æ–°çŠ¶æ€
                if result.success:
                    state.processed_docs.append(result.doc_id)
                else:
                    state.failed_docs.append(result.doc_id)
                
                state.current_progress = len(state.processed_docs) + len(state.failed_docs)
                
                # æ›´æ–°è¿›åº¦
                await progress_tracker.update_progress(result.success, result.doc_id)
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                checkpoint_counter += 1
                if checkpoint_counter >= self.checkpoint_interval:
                    await self.checkpoint_manager.save_checkpoint(state)
                    checkpoint_counter = 0
            
            # æœ€ç»ˆæ£€æŸ¥ç‚¹
            await self.checkpoint_manager.save_checkpoint(state)
            
            # å¤„ç†å®Œæˆï¼Œæ¸…ç†æ£€æŸ¥ç‚¹
            await self.checkpoint_manager.cleanup_checkpoint(batch_id)
            
            # ç»Ÿè®¡ç»“æœ
            total_docs = len(doc_paths)
            successful_docs = len(state.processed_docs)
            failed_docs = len(state.failed_docs)
            
            batch_result = {
                'batch_id': batch_id,
                'total_docs': total_docs,
                'successful_docs': successful_docs,
                'failed_docs': failed_docs,
                'success_rate': successful_docs / total_docs * 100,
                'processing_time': (datetime.now() - self.active_batches[batch_id]['start_time']).total_seconds(),
                'results': results
            }
            
            logger.info(f"Batch {batch_id} completed: {successful_docs}/{total_docs} successful")
            return batch_result
        
        finally:
            # æ¸…ç†æ´»è·ƒæ‰¹å¤„ç†è®°å½•
            if batch_id in self.active_batches:
                del self.active_batches[batch_id]
    
    def get_batch_status(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ‰¹å¤„ç†çŠ¶æ€"""
        if batch_id not in self.active_batches:
            return None
        
        batch_info = self.active_batches[batch_id]
        state = batch_info['state']
        progress_tracker = batch_info['progress_tracker']
        
        return {
            'batch_id': batch_id,
            'total_docs': state.total_docs,
            'processed_docs': len(state.processed_docs),
            'failed_docs': len(state.failed_docs),
            'progress_percent': (len(state.processed_docs) + len(state.failed_docs)) / state.total_docs * 100,
            'eta': progress_tracker.get_eta(),
            'start_time': state.start_time,
            'last_checkpoint': state.last_checkpoint
        }
```

### æ­¥éª¤2ï¼šåˆ›å»ºFastAPIæ¥å£

åˆ›å»º `batch_api.py` æ–‡ä»¶ï¼š

```python
from fastapi import FastAPI, BackgroundTasks, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
from typing import List, Optional
import os
import tempfile
from pathlib import Path

from batch_processor import BatchProcessor

app = FastAPI(title="æ‰¹é‡æ–‡æ¡£å¤„ç†API")
batch_processor = BatchProcessor(max_concurrent=5)

@app.post("/batch/upload")
async def upload_and_process_batch(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    batch_id: Optional[str] = None
):
    """ä¸Šä¼ æ–‡ä»¶å¹¶å¯åŠ¨æ‰¹é‡å¤„ç†"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_dir = Path(tempfile.mkdtemp())
        doc_paths = []
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        for file in files:
            file_path = temp_dir / file.filename
            with open(file_path, "wb") as f:
                content = await file.read()
                f.write(content)
            doc_paths.append(str(file_path))
        
        # å¯åŠ¨åå°æ‰¹é‡å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            batch_processor.process_batch,
            doc_paths,
            batch_id
        )
        
        return {
            "message": "æ‰¹é‡å¤„ç†ä»»åŠ¡å·²å¯åŠ¨",
            "batch_id": batch_id,
            "total_files": len(files)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/process")
async def start_batch_processing(
    background_tasks: BackgroundTasks,
    doc_paths: List[str],
    batch_id: Optional[str] = None
):
    """å¯åŠ¨æ‰¹é‡å¤„ç†ä»»åŠ¡"""
    try:
        # éªŒè¯æ–‡ä»¶è·¯å¾„
        for path in doc_paths:
            if not os.path.exists(path):
                raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        # å¯åŠ¨åå°æ‰¹é‡å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            batch_processor.process_batch,
            doc_paths,
            batch_id
        )
        
        return {
            "message": "æ‰¹é‡å¤„ç†ä»»åŠ¡å·²å¯åŠ¨",
            "batch_id": batch_id,
            "total_files": len(doc_paths)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/resume/{batch_id}")
async def resume_batch_processing(
    batch_id: str,
    background_tasks: BackgroundTasks,
    doc_paths: List[str]
):
    """æ¢å¤æ‰¹é‡å¤„ç†ä»»åŠ¡"""
    try:
        # å¯åŠ¨æ¢å¤ä»»åŠ¡
        background_tasks.add_task(
            batch_processor.process_batch,
            doc_paths,
            batch_id,
            True  # resume=True
        )
        
        return {
            "message": f"æ‰¹é‡å¤„ç†ä»»åŠ¡ {batch_id} å·²æ¢å¤",
            "batch_id": batch_id
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/batch/status/{batch_id}")
async def get_batch_status(batch_id: str):
    """è·å–æ‰¹é‡å¤„ç†çŠ¶æ€"""
    status = batch_processor.get_batch_status(batch_id)
    
    if status is None:
        raise HTTPException(status_code=404, detail="æ‰¹å¤„ç†ä»»åŠ¡ä¸å­˜åœ¨")
    
    return status

@app.get("/batch/active")
async def get_active_batches():
    """è·å–æ‰€æœ‰æ´»è·ƒçš„æ‰¹å¤„ç†ä»»åŠ¡"""
    active_batches = []
    
    for batch_id in batch_processor.active_batches.keys():
        status = batch_processor.get_batch_status(batch_id)
        if status:
            active_batches.append(status)
    
    return {
        "active_batches": active_batches,
        "total_count": len(active_batches)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### æ­¥éª¤3ï¼šåˆ›å»ºæµ‹è¯•è„šæœ¬

åˆ›å»º `test_batch_processing.py` æ–‡ä»¶ï¼š

```python
import asyncio
import tempfile
import os
from pathlib import Path
import requests
import time

from batch_processor import BatchProcessor

def create_test_files(count: int = 20) -> list:
    """åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
    temp_dir = Path(tempfile.mkdtemp())
    test_files = []
    
    for i in range(count):
        file_path = temp_dir / f"test_doc_{i:03d}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"è¿™æ˜¯æµ‹è¯•æ–‡æ¡£ {i}\n" * 100)
        test_files.append(str(file_path))
    
    print(f"åˆ›å»ºäº† {count} ä¸ªæµ‹è¯•æ–‡ä»¶åœ¨ {temp_dir}")
    return test_files

async def test_basic_batch_processing():
    """æµ‹è¯•åŸºæœ¬æ‰¹é‡å¤„ç†åŠŸèƒ½"""
    print("\n=== æµ‹è¯•åŸºæœ¬æ‰¹é‡å¤„ç†åŠŸèƒ½ ===")
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_files = create_test_files(10)
    
    # åˆ›å»ºæ‰¹å¤„ç†å™¨
    processor = BatchProcessor(max_concurrent=3)
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    result = await processor.process_batch(test_files)
    
    print(f"æ‰¹å¤„ç†ç»“æœ:")
    print(f"- æ‰¹æ¬¡ID: {result['batch_id']}")
    print(f"- æ€»æ–‡æ¡£æ•°: {result['total_docs']}")
    print(f"- æˆåŠŸå¤„ç†: {result['successful_docs']}")
    print(f"- å¤„ç†å¤±è´¥: {result['failed_docs']}")
    print(f"- æˆåŠŸç‡: {result['success_rate']:.1f}%")
    print(f"- å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")

async def test_checkpoint_recovery():
    """æµ‹è¯•æ–­ç‚¹ç»­ä¼ åŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ–­ç‚¹ç»­ä¼ åŠŸèƒ½ ===")
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_files = create_test_files(15)
    
    # åˆ›å»ºæ‰¹å¤„ç†å™¨
    processor = BatchProcessor(max_concurrent=2, checkpoint_interval=3)
    
    batch_id = "test_checkpoint_batch"
    
    try:
        # æ¨¡æ‹Ÿå¤„ç†ä¸­æ–­ï¼ˆå¤„ç†ä¸€éƒ¨åˆ†ååœæ­¢ï¼‰
        print("å¼€å§‹ç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆæ¨¡æ‹Ÿä¸­æ–­ï¼‰...")
        
        # åªå¤„ç†å‰5ä¸ªæ–‡ä»¶æ¥æ¨¡æ‹Ÿéƒ¨åˆ†å®Œæˆ
        partial_files = test_files[:5]
        result1 = await processor.process_batch(partial_files, batch_id)
        
        print(f"ç¬¬ä¸€æ¬¡å¤„ç†å®Œæˆ: {result1['successful_docs']}/{result1['total_docs']}")
        
        # æ¨¡æ‹Ÿä»æ–­ç‚¹æ¢å¤
        print("\nä»æ–­ç‚¹æ¢å¤å¤„ç†...")
        result2 = await processor.process_batch(test_files, batch_id, resume=True)
        
        print(f"æ¢å¤å¤„ç†ç»“æœ:")
        print(f"- æ€»æ–‡æ¡£æ•°: {result2['total_docs']}")
        print(f"- æˆåŠŸå¤„ç†: {result2['successful_docs']}")
        print(f"- å¤„ç†å¤±è´¥: {result2['failed_docs']}")
        print(f"- æˆåŠŸç‡: {result2['success_rate']:.1f}%")
        
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

def test_api_endpoints():
    """æµ‹è¯•APIæ¥å£"""
    print("\n=== æµ‹è¯•APIæ¥å£ ===")
    
    # å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆéœ€è¦åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ python batch_api.pyï¼‰
    base_url = "http://localhost:8000"
    
    try:
        # æµ‹è¯•å¯åŠ¨æ‰¹é‡å¤„ç†
        test_files = create_test_files(5)
        
        response = requests.post(
            f"{base_url}/batch/process",
            json=test_files
        )
        
        if response.status_code == 200:
            result = response.json()
            batch_id = result.get('batch_id')
            print(f"æ‰¹é‡å¤„ç†ä»»åŠ¡å·²å¯åŠ¨: {batch_id}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´åæŸ¥è¯¢çŠ¶æ€
            time.sleep(2)
            
            status_response = requests.get(f"{base_url}/batch/status/{batch_id}")
            if status_response.status_code == 200:
                status = status_response.json()
                print(f"ä»»åŠ¡çŠ¶æ€: {status}")
            else:
                print(f"è·å–çŠ¶æ€å¤±è´¥: {status_response.text}")
        else:
            print(f"å¯åŠ¨æ‰¹é‡å¤„ç†å¤±è´¥: {response.text}")
    
    except requests.exceptions.ConnectionError:
        print("æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        print("è¿è¡Œå‘½ä»¤: python batch_api.py")

async def test_error_recovery():
    """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
    print("\n=== æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶ ===")
    
    from batch_processor import ErrorRecoveryManager
    
    recovery_manager = ErrorRecoveryManager(max_retries=3, retry_delay=0.1)
    
    # æ¨¡æ‹Ÿä¼šå¤±è´¥çš„å‡½æ•°
    attempt_count = 0
    async def failing_function():
        nonlocal attempt_count
        attempt_count += 1
        if attempt_count < 3:
            raise Exception(f"æ¨¡æ‹Ÿå¤±è´¥ (å°è¯• {attempt_count})")
        return f"æˆåŠŸ (å°è¯• {attempt_count})"
    
    try:
        result = await recovery_manager.execute_with_retry(failing_function)
        print(f"é”™è¯¯æ¢å¤æµ‹è¯•ç»“æœ: {result}")
    except Exception as e:
        print(f"é”™è¯¯æ¢å¤å¤±è´¥: {e}")

async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹æ‰¹é‡å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ åŠŸèƒ½æµ‹è¯•")
    
    await test_basic_batch_processing()
    await test_checkpoint_recovery()
    await test_error_recovery()
    
    print("\næ³¨æ„: APIæµ‹è¯•éœ€è¦å•ç‹¬è¿è¡ŒæœåŠ¡å™¨")
    print("è¿è¡Œå‘½ä»¤: python batch_api.py")
    print("ç„¶åè¿è¡Œ: python -c 'from test_batch_processing import test_api_endpoints; test_api_endpoints()'")
    
    print("\næ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
```

### æ­¥éª¤4ï¼šåˆ›å»ºWebç•Œé¢ï¼ˆå¯é€‰ï¼‰

åˆ›å»º `batch_web_interface.py` æ–‡ä»¶ï¼š

```python
import streamlit as st
import requests
import time
import json
from pathlib import Path

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="æ‰¹é‡æ–‡æ¡£å¤„ç†ç³»ç»Ÿ",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ æ‰¹é‡æ–‡æ¡£å¤„ç†ç³»ç»Ÿ")
st.markdown("---")

# APIåŸºç¡€URL
API_BASE_URL = "http://localhost:8000"

# ä¾§è¾¹æ 
st.sidebar.title("åŠŸèƒ½é€‰æ‹©")
function_choice = st.sidebar.selectbox(
    "é€‰æ‹©åŠŸèƒ½",
    ["æ–‡ä»¶ä¸Šä¼ å¤„ç†", "æ‰¹é‡å¤„ç†ç®¡ç†", "ä»»åŠ¡çŠ¶æ€ç›‘æ§"]
)

if function_choice == "æ–‡ä»¶ä¸Šä¼ å¤„ç†":
    st.header("ğŸ“¤ æ–‡ä»¶ä¸Šä¼ å¤„ç†")
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶",
        accept_multiple_files=True,
        type=['txt', 'pdf', 'docx']
    )
    
    if uploaded_files:
        st.write(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶:")
        for file in uploaded_files:
            st.write(f"- {file.name} ({file.size} bytes)")
        
        if st.button("å¼€å§‹æ‰¹é‡å¤„ç†"):
            try:
                # å‡†å¤‡æ–‡ä»¶æ•°æ®
                files_data = []
                for file in uploaded_files:
                    files_data.append(('files', (file.name, file.read(), file.type)))
                
                # å‘é€è¯·æ±‚
                response = requests.post(
                    f"{API_BASE_URL}/batch/upload",
                    files=files_data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    st.success(f"æ‰¹é‡å¤„ç†ä»»åŠ¡å·²å¯åŠ¨ï¼")
                    st.json(result)
                    
                    # ä¿å­˜batch_idåˆ°session state
                    st.session_state.current_batch_id = result.get('batch_id')
                else:
                    st.error(f"å¯åŠ¨å¤±è´¥: {response.text}")
            
            except Exception as e:
                st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

elif function_choice == "æ‰¹é‡å¤„ç†ç®¡ç†":
    st.header("âš™ï¸ æ‰¹é‡å¤„ç†ç®¡ç†")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("å¯åŠ¨æ–°çš„æ‰¹é‡å¤„ç†")
        
        # æ–‡ä»¶è·¯å¾„è¾“å…¥
        file_paths_text = st.text_area(
            "è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            height=150,
            placeholder="/path/to/file1.txt\n/path/to/file2.txt"
        )
        
        batch_id = st.text_input("æ‰¹æ¬¡IDï¼ˆå¯é€‰ï¼‰", placeholder="ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ")
        
        if st.button("å¯åŠ¨æ‰¹é‡å¤„ç†"):
            if file_paths_text.strip():
                file_paths = [path.strip() for path in file_paths_text.strip().split('\n') if path.strip()]
                
                try:
                    data = file_paths
                    if batch_id:
                        # å¦‚æœæä¾›äº†batch_idï¼Œéœ€è¦ä½¿ç”¨ä¸åŒçš„ç«¯ç‚¹
                        pass
                    
                    response = requests.post(
                        f"{API_BASE_URL}/batch/process",
                        json=data
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        st.success("æ‰¹é‡å¤„ç†ä»»åŠ¡å·²å¯åŠ¨ï¼")
                        st.json(result)
                        st.session_state.current_batch_id = result.get('batch_id')
                    else:
                        st.error(f"å¯åŠ¨å¤±è´¥: {response.text}")
                
                except Exception as e:
                    st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            else:
                st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæ–‡ä»¶è·¯å¾„")
    
    with col2:
        st.subheader("æ¢å¤æ‰¹é‡å¤„ç†")
        
        resume_batch_id = st.text_input("è¦æ¢å¤çš„æ‰¹æ¬¡ID")
        resume_file_paths_text = st.text_area(
            "åŸå§‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨",
            height=100,
            placeholder="/path/to/file1.txt\n/path/to/file2.txt"
        )
        
        if st.button("æ¢å¤å¤„ç†"):
            if resume_batch_id and resume_file_paths_text.strip():
                file_paths = [path.strip() for path in resume_file_paths_text.strip().split('\n') if path.strip()]
                
                try:
                    response = requests.post(
                        f"{API_BASE_URL}/batch/resume/{resume_batch_id}",
                        json=file_paths
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        st.success("æ‰¹é‡å¤„ç†ä»»åŠ¡å·²æ¢å¤ï¼")
                        st.json(result)
                    else:
                        st.error(f"æ¢å¤å¤±è´¥: {response.text}")
                
                except Exception as e:
                    st.error(f"æ¢å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            else:
                st.warning("è¯·è¾“å…¥æ‰¹æ¬¡IDå’Œæ–‡ä»¶è·¯å¾„")

elif function_choice == "ä»»åŠ¡çŠ¶æ€ç›‘æ§":
    st.header("ğŸ“Š ä»»åŠ¡çŠ¶æ€ç›‘æ§")
    
    # è‡ªåŠ¨åˆ·æ–°é€‰é¡¹
    auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–° (5ç§’)", value=False)
    
    if auto_refresh:
        time.sleep(5)
        st.rerun()
    
    # æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®
    if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€"):
        st.rerun()
    
    try:
        # è·å–æ´»è·ƒä»»åŠ¡åˆ—è¡¨
        response = requests.get(f"{API_BASE_URL}/batch/active")
        
        if response.status_code == 200:
            data = response.json()
            active_batches = data.get('active_batches', [])
            
            if active_batches:
                st.subheader(f"æ´»è·ƒä»»åŠ¡ ({len(active_batches)} ä¸ª)")
                
                for batch in active_batches:
                    with st.expander(f"æ‰¹æ¬¡ {batch['batch_id'][:8]}..."):
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("æ€»æ–‡æ¡£æ•°", batch['total_docs'])
                            st.metric("å·²å¤„ç†", batch['processed_docs'])
                        
                        with col2:
                            st.metric("å¤„ç†å¤±è´¥", batch['failed_docs'])
                            st.metric("è¿›åº¦", f"{batch['progress_percent']:.1f}%")
                        
                        with col3:
                            st.metric("é¢„è®¡å‰©ä½™æ—¶é—´", batch['eta'])
                        
                        # è¿›åº¦æ¡
                        progress = batch['progress_percent'] / 100
                        st.progress(progress)
                        
                        # è¯¦ç»†ä¿¡æ¯
                        st.json(batch)
            else:
                st.info("å½“å‰æ²¡æœ‰æ´»è·ƒçš„æ‰¹é‡å¤„ç†ä»»åŠ¡")
        else:
            st.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.text}")
    
    except requests.exceptions.ConnectionError:
        st.error("æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        st.code("python batch_api.py")
    
    # å•ä¸ªä»»åŠ¡çŠ¶æ€æŸ¥è¯¢
    st.markdown("---")
    st.subheader("æŸ¥è¯¢ç‰¹å®šä»»åŠ¡çŠ¶æ€")
    
    query_batch_id = st.text_input("è¾“å…¥æ‰¹æ¬¡ID")
    
    if st.button("æŸ¥è¯¢çŠ¶æ€") and query_batch_id:
        try:
            response = requests.get(f"{API_BASE_URL}/batch/status/{query_batch_id}")
            
            if response.status_code == 200:
                status = response.json()
                st.success("æŸ¥è¯¢æˆåŠŸï¼")
                st.json(status)
            elif response.status_code == 404:
                st.warning("æœªæ‰¾åˆ°æŒ‡å®šçš„æ‰¹æ¬¡ID")
            else:
                st.error(f"æŸ¥è¯¢å¤±è´¥: {response.text}")
        
        except Exception as e:
            st.error(f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

# é¡µé¢åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown(
    """
    **ä½¿ç”¨è¯´æ˜:**
    1. ç¡®ä¿APIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ: `python batch_api.py`
    2. ä¸Šä¼ æ–‡ä»¶æˆ–è¾“å…¥æ–‡ä»¶è·¯å¾„å¼€å§‹æ‰¹é‡å¤„ç†
    3. ä½¿ç”¨ä»»åŠ¡çŠ¶æ€ç›‘æ§æŸ¥çœ‹å¤„ç†è¿›åº¦
    4. å¦‚æœå¤„ç†ä¸­æ–­ï¼Œå¯ä»¥ä½¿ç”¨æ¢å¤åŠŸèƒ½ç»§ç»­å¤„ç†
    """
)
```

## å®éªŒä»»åŠ¡

### ä»»åŠ¡1ï¼šåŸºç¡€åŠŸèƒ½æµ‹è¯•
1. è¿è¡ŒåŸºç¡€æ‰¹é‡å¤„ç†æµ‹è¯•
2. è§‚å¯Ÿå¤„ç†è¿›åº¦å’Œæ—¥å¿—è¾“å‡º
3. åˆ†ææˆåŠŸç‡å’Œå¤„ç†æ—¶é—´

### ä»»åŠ¡2ï¼šæ–­ç‚¹ç»­ä¼ æµ‹è¯•
1. å¯åŠ¨ä¸€ä¸ªæ‰¹é‡å¤„ç†ä»»åŠ¡
2. åœ¨å¤„ç†è¿‡ç¨‹ä¸­æ‰‹åŠ¨ä¸­æ–­
3. ä½¿ç”¨ç›¸åŒçš„batch_idæ¢å¤å¤„ç†
4. éªŒè¯å·²å¤„ç†çš„æ–‡æ¡£ä¸ä¼šé‡å¤å¤„ç†

### ä»»åŠ¡3ï¼šAPIæ¥å£æµ‹è¯•
1. å¯åŠ¨FastAPIæœåŠ¡å™¨
2. ä½¿ç”¨APIæ¥å£å¯åŠ¨æ‰¹é‡å¤„ç†
3. æŸ¥è¯¢å¤„ç†çŠ¶æ€å’Œè¿›åº¦
4. æµ‹è¯•æ¢å¤åŠŸèƒ½

### ä»»åŠ¡4ï¼šWebç•Œé¢æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
1. å¯åŠ¨Streamlit Webç•Œé¢
2. ä¸Šä¼ æ–‡ä»¶å¹¶å¯åŠ¨å¤„ç†
3. å®æ—¶ç›‘æ§å¤„ç†è¿›åº¦
4. æµ‹è¯•ä»»åŠ¡ç®¡ç†åŠŸèƒ½

## å®éªŒæŠ¥å‘Šè¦æ±‚

è¯·å®Œæˆå®éªŒåï¼Œæäº¤åŒ…å«ä»¥ä¸‹å†…å®¹çš„å®éªŒæŠ¥å‘Šï¼š

1. **å®éªŒç¯å¢ƒ**ï¼šPythonç‰ˆæœ¬ã€ä¾èµ–åŒ…ç‰ˆæœ¬ç­‰
2. **åŠŸèƒ½æµ‹è¯•ç»“æœ**ï¼šå„ä¸ªåŠŸèƒ½æ¨¡å—çš„æµ‹è¯•ç»“æœæˆªå›¾
3. **æ€§èƒ½åˆ†æ**ï¼šä¸åŒå¹¶å‘æ•°é‡ä¸‹çš„å¤„ç†æ€§èƒ½å¯¹æ¯”
4. **é”™è¯¯å¤„ç†**ï¼šæµ‹è¯•å„ç§å¼‚å¸¸æƒ…å†µçš„å¤„ç†æ•ˆæœ
5. **æ”¹è¿›å»ºè®®**ï¼šå¯¹ç³»ç»Ÿçš„ä¼˜åŒ–å»ºè®®å’Œæ‰©å±•æƒ³æ³•

## æ€è€ƒé¢˜

1. **å†…å­˜ç®¡ç†**ï¼šå¦‚ä½•é¿å…æ‰¹é‡å¤„ç†å¤§æ–‡ä»¶æ—¶çš„å†…å­˜æº¢å‡ºï¼Ÿ
2. **åˆ†å¸ƒå¼å¤„ç†**ï¼šå¦‚ä½•å°†æ‰¹é‡å¤„ç†æ‰©å±•åˆ°å¤šå°æœåŠ¡å™¨ï¼Ÿ
3. **ä¼˜å…ˆçº§è°ƒåº¦**ï¼šå¦‚ä½•å®ç°ä¸åŒä¼˜å…ˆçº§çš„æ‰¹é‡å¤„ç†ä»»åŠ¡ï¼Ÿ
4. **ç›‘æ§å‘Šè­¦**ï¼šå¦‚ä½•è®¾è®¡å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶ï¼Ÿ
5. **æ•°æ®ä¸€è‡´æ€§**ï¼šåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹å¦‚ä½•ä¿è¯æ–­ç‚¹ç»­ä¼ çš„æ•°æ®ä¸€è‡´æ€§ï¼Ÿ

## æ‰©å±•ç»ƒä¹ 

1. **é›†æˆåˆ°RAGç³»ç»Ÿ**ï¼šå°†æ‰¹é‡å¤„ç†åŠŸèƒ½é›†æˆåˆ°ç°æœ‰çš„RAGç³»ç»Ÿä¸­
2. **æ·»åŠ æ–‡ä»¶ç±»å‹æ”¯æŒ**ï¼šæ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼çš„æ‰¹é‡å¤„ç†
3. **å®ç°ä»»åŠ¡é˜Ÿåˆ—**ï¼šä½¿ç”¨Redisæˆ–RabbitMQå®ç°ä»»åŠ¡é˜Ÿåˆ—
4. **æ·»åŠ ç”¨æˆ·æƒé™**ï¼šå®ç°å¤šç”¨æˆ·çš„æ‰¹é‡å¤„ç†ä»»åŠ¡ç®¡ç†
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šä¼˜åŒ–å¤„ç†ç®—æ³•ï¼Œæé«˜å¤„ç†é€Ÿåº¦