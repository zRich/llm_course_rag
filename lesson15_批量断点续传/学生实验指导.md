# 第15节课：批量/断点续传 - 学生实验指导

## 实验目标
通过本实验，学生将：
1. 掌握异步批量处理的实现方法
2. 理解断点续传机制的设计原理
3. 学会实现进度跟踪和错误恢复
4. 构建一个完整的批量文档处理系统

## 实验环境准备

### 环境要求
- Python 3.8+
- FastAPI
- asyncio
- aiofiles
- 前面课程的RAG系统代码

### 依赖安装
```bash
# 进入rag-system目录
cd rag-system

# 安装新的依赖
uv add aiofiles
uv add python-multipart
```

## 实验步骤

### 步骤1：创建批量处理核心模块

创建 `batch_processor.py` 文件：

```python
import asyncio
import json
import logging
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
import aiofiles

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """处理结果数据类"""
    doc_id: str
    success: bool
    error_message: Optional[str] = None
    processing_time: float = 0.0
    timestamp: str = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()

@dataclass
class BatchState:
    """批处理状态数据类"""
    batch_id: str
    total_docs: int
    processed_docs: List[str]
    failed_docs: List[str]
    current_progress: int
    start_time: str
    last_checkpoint: str
    
    def __post_init__(self):
        if self.start_time is None:
            self.start_time = datetime.now().isoformat()
        if self.last_checkpoint is None:
            self.last_checkpoint = datetime.now().isoformat()

class ProgressTracker:
    """进度跟踪器"""
    
    def __init__(self, total_items: int, batch_id: str):
        self.total_items = total_items
        self.completed_items = 0
        self.failed_items = 0
        self.batch_id = batch_id
        self.start_time = datetime.now()
        self.callbacks = []
    
    def add_callback(self, callback):
        """添加进度更新回调函数"""
        self.callbacks.append(callback)
    
    async def update_progress(self, success: bool = True, doc_id: str = None):
        """更新进度"""
        if success:
            self.completed_items += 1
        else:
            self.failed_items += 1
        
        progress_percent = (self.completed_items + self.failed_items) / self.total_items * 100
        
        progress_info = {
            'batch_id': self.batch_id,
            'progress_percent': progress_percent,
            'completed': self.completed_items,
            'failed': self.failed_items,
            'total': self.total_items,
            'eta': self.get_eta(),
            'doc_id': doc_id
        }
        
        logger.info(f"Progress: {progress_percent:.1f}% ({self.completed_items}/{self.total_items})")
        
        # 调用所有回调函数
        for callback in self.callbacks:
            try:
                await callback(progress_info)
            except Exception as e:
                logger.error(f"Progress callback error: {e}")
    
    def get_eta(self) -> str:
        """估算剩余时间"""
        if self.completed_items == 0:
            return "Unknown"
        
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.completed_items / elapsed
        remaining_items = self.total_items - self.completed_items - self.failed_items
        eta_seconds = remaining_items / rate if rate > 0 else 0
        return f"{eta_seconds:.0f} seconds"

class CheckpointManager:
    """断点续传管理器"""
    
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
    
    async def save_checkpoint(self, state: BatchState):
        """保存检查点"""
        checkpoint_file = self.checkpoint_dir / f"{state.batch_id}.json"
        state.last_checkpoint = datetime.now().isoformat()
        
        async with aiofiles.open(checkpoint_file, 'w') as f:
            await f.write(json.dumps(asdict(state), indent=2))
        
        logger.info(f"Checkpoint saved for batch {state.batch_id}")
    
    async def load_checkpoint(self, batch_id: str) -> Optional[BatchState]:
        """加载检查点"""
        checkpoint_file = self.checkpoint_dir / f"{batch_id}.json"
        
        if not checkpoint_file.exists():
            return None
        
        try:
            async with aiofiles.open(checkpoint_file, 'r') as f:
                data = json.loads(await f.read())
            
            state = BatchState(**data)
            logger.info(f"Checkpoint loaded for batch {batch_id}")
            return state
        except Exception as e:
            logger.error(f"Failed to load checkpoint for batch {batch_id}: {e}")
            return None
    
    async def cleanup_checkpoint(self, batch_id: str):
        """清理检查点文件"""
        checkpoint_file = self.checkpoint_dir / f"{batch_id}.json"
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            logger.info(f"Checkpoint cleaned up for batch {batch_id}")

class ErrorRecoveryManager:
    """错误恢复管理器"""
    
    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
    
    async def execute_with_retry(self, func, *args, **kwargs) -> Any:
        """带重试机制的函数执行"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                
                if attempt < self.max_retries:
                    delay = self.retry_delay * (2 ** attempt)  # 指数退避
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"All {self.max_retries + 1} attempts failed")
                    raise last_exception

class BatchProcessor:
    """批量处理器主类"""
    
    def __init__(self, max_concurrent: int = 10, checkpoint_interval: int = 10):
        self.max_concurrent = max_concurrent
        self.checkpoint_interval = checkpoint_interval
        self.checkpoint_manager = CheckpointManager()
        self.error_recovery = ErrorRecoveryManager()
        self.active_batches = {}
    
    async def process_document(self, doc_path: str) -> ProcessingResult:
        """处理单个文档（模拟实现）"""
        start_time = datetime.now()
        
        try:
            # 模拟文档处理时间
            await asyncio.sleep(0.1)
            
            # 模拟偶发错误（10%概率）
            import random
            if random.random() < 0.1:
                raise Exception(f"Simulated processing error for {doc_path}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return ProcessingResult(
                doc_id=doc_path,
                success=True,
                processing_time=processing_time
            )
        
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            return ProcessingResult(
                doc_id=doc_path,
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    async def process_batch(self, doc_paths: List[str], batch_id: str = None, 
                          resume: bool = False) -> Dict[str, Any]:
        """批量处理文档"""
        if batch_id is None:
            batch_id = str(uuid.uuid4())
        
        # 尝试恢复之前的状态
        if resume:
            state = await self.checkpoint_manager.load_checkpoint(batch_id)
            if state:
                # 过滤掉已处理的文档
                remaining_docs = [doc for doc in doc_paths 
                                if doc not in state.processed_docs and doc not in state.failed_docs]
                logger.info(f"Resuming batch {batch_id}, {len(remaining_docs)} documents remaining")
            else:
                logger.warning(f"No checkpoint found for batch {batch_id}, starting fresh")
                remaining_docs = doc_paths
                state = BatchState(
                    batch_id=batch_id,
                    total_docs=len(doc_paths),
                    processed_docs=[],
                    failed_docs=[],
                    current_progress=0,
                    start_time=datetime.now().isoformat(),
                    last_checkpoint=datetime.now().isoformat()
                )
        else:
            remaining_docs = doc_paths
            state = BatchState(
                batch_id=batch_id,
                total_docs=len(doc_paths),
                processed_docs=[],
                failed_docs=[],
                current_progress=0,
                start_time=datetime.now().isoformat(),
                last_checkpoint=datetime.now().isoformat()
            )
        
        # 创建进度跟踪器
        progress_tracker = ProgressTracker(len(doc_paths), batch_id)
        progress_tracker.completed_items = len(state.processed_docs)
        progress_tracker.failed_items = len(state.failed_docs)
        
        # 注册批处理任务
        self.active_batches[batch_id] = {
            'state': state,
            'progress_tracker': progress_tracker,
            'start_time': datetime.now()
        }
        
        try:
            # 创建信号量控制并发
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            async def process_with_semaphore(doc_path: str) -> ProcessingResult:
                async with semaphore:
                    return await self.error_recovery.execute_with_retry(
                        self.process_document, doc_path
                    )
            
            # 处理剩余文档
            results = []
            checkpoint_counter = 0
            
            for doc_path in remaining_docs:
                result = await process_with_semaphore(doc_path)
                results.append(result)
                
                # 更新状态
                if result.success:
                    state.processed_docs.append(result.doc_id)
                else:
                    state.failed_docs.append(result.doc_id)
                
                state.current_progress = len(state.processed_docs) + len(state.failed_docs)
                
                # 更新进度
                await progress_tracker.update_progress(result.success, result.doc_id)
                
                # 定期保存检查点
                checkpoint_counter += 1
                if checkpoint_counter >= self.checkpoint_interval:
                    await self.checkpoint_manager.save_checkpoint(state)
                    checkpoint_counter = 0
            
            # 最终检查点
            await self.checkpoint_manager.save_checkpoint(state)
            
            # 处理完成，清理检查点
            await self.checkpoint_manager.cleanup_checkpoint(batch_id)
            
            # 统计结果
            total_docs = len(doc_paths)
            successful_docs = len(state.processed_docs)
            failed_docs = len(state.failed_docs)
            
            batch_result = {
                'batch_id': batch_id,
                'total_docs': total_docs,
                'successful_docs': successful_docs,
                'failed_docs': failed_docs,
                'success_rate': successful_docs / total_docs * 100,
                'processing_time': (datetime.now() - self.active_batches[batch_id]['start_time']).total_seconds(),
                'results': results
            }
            
            logger.info(f"Batch {batch_id} completed: {successful_docs}/{total_docs} successful")
            return batch_result
        
        finally:
            # 清理活跃批处理记录
            if batch_id in self.active_batches:
                del self.active_batches[batch_id]
    
    def get_batch_status(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """获取批处理状态"""
        if batch_id not in self.active_batches:
            return None
        
        batch_info = self.active_batches[batch_id]
        state = batch_info['state']
        progress_tracker = batch_info['progress_tracker']
        
        return {
            'batch_id': batch_id,
            'total_docs': state.total_docs,
            'processed_docs': len(state.processed_docs),
            'failed_docs': len(state.failed_docs),
            'progress_percent': (len(state.processed_docs) + len(state.failed_docs)) / state.total_docs * 100,
            'eta': progress_tracker.get_eta(),
            'start_time': state.start_time,
            'last_checkpoint': state.last_checkpoint
        }
```

### 步骤2：创建FastAPI接口

创建 `batch_api.py` 文件：

```python
from fastapi import FastAPI, BackgroundTasks, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
from typing import List, Optional
import os
import tempfile
from pathlib import Path

from batch_processor import BatchProcessor

app = FastAPI(title="批量文档处理API")
batch_processor = BatchProcessor(max_concurrent=5)

@app.post("/batch/upload")
async def upload_and_process_batch(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    batch_id: Optional[str] = None
):
    """上传文件并启动批量处理"""
    try:
        # 创建临时目录保存上传的文件
        temp_dir = Path(tempfile.mkdtemp())
        doc_paths = []
        
        # 保存上传的文件
        for file in files:
            file_path = temp_dir / file.filename
            with open(file_path, "wb") as f:
                content = await file.read()
                f.write(content)
            doc_paths.append(str(file_path))
        
        # 启动后台批量处理任务
        background_tasks.add_task(
            batch_processor.process_batch,
            doc_paths,
            batch_id
        )
        
        return {
            "message": "批量处理任务已启动",
            "batch_id": batch_id,
            "total_files": len(files)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/process")
async def start_batch_processing(
    background_tasks: BackgroundTasks,
    doc_paths: List[str],
    batch_id: Optional[str] = None
):
    """启动批量处理任务"""
    try:
        # 验证文件路径
        for path in doc_paths:
            if not os.path.exists(path):
                raise HTTPException(status_code=400, detail=f"文件不存在: {path}")
        
        # 启动后台批量处理任务
        background_tasks.add_task(
            batch_processor.process_batch,
            doc_paths,
            batch_id
        )
        
        return {
            "message": "批量处理任务已启动",
            "batch_id": batch_id,
            "total_files": len(doc_paths)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/resume/{batch_id}")
async def resume_batch_processing(
    batch_id: str,
    background_tasks: BackgroundTasks,
    doc_paths: List[str]
):
    """恢复批量处理任务"""
    try:
        # 启动恢复任务
        background_tasks.add_task(
            batch_processor.process_batch,
            doc_paths,
            batch_id,
            True  # resume=True
        )
        
        return {
            "message": f"批量处理任务 {batch_id} 已恢复",
            "batch_id": batch_id
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/batch/status/{batch_id}")
async def get_batch_status(batch_id: str):
    """获取批量处理状态"""
    status = batch_processor.get_batch_status(batch_id)
    
    if status is None:
        raise HTTPException(status_code=404, detail="批处理任务不存在")
    
    return status

@app.get("/batch/active")
async def get_active_batches():
    """获取所有活跃的批处理任务"""
    active_batches = []
    
    for batch_id in batch_processor.active_batches.keys():
        status = batch_processor.get_batch_status(batch_id)
        if status:
            active_batches.append(status)
    
    return {
        "active_batches": active_batches,
        "total_count": len(active_batches)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 步骤3：创建测试脚本

创建 `test_batch_processing.py` 文件：

```python
import asyncio
import tempfile
import os
from pathlib import Path
import requests
import time

from batch_processor import BatchProcessor

def create_test_files(count: int = 20) -> list:
    """创建测试文件"""
    temp_dir = Path(tempfile.mkdtemp())
    test_files = []
    
    for i in range(count):
        file_path = temp_dir / f"test_doc_{i:03d}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"这是测试文档 {i}\n" * 100)
        test_files.append(str(file_path))
    
    print(f"创建了 {count} 个测试文件在 {temp_dir}")
    return test_files

async def test_basic_batch_processing():
    """测试基本批量处理功能"""
    print("\n=== 测试基本批量处理功能 ===")
    
    # 创建测试文件
    test_files = create_test_files(10)
    
    # 创建批处理器
    processor = BatchProcessor(max_concurrent=3)
    
    # 执行批量处理
    result = await processor.process_batch(test_files)
    
    print(f"批处理结果:")
    print(f"- 批次ID: {result['batch_id']}")
    print(f"- 总文档数: {result['total_docs']}")
    print(f"- 成功处理: {result['successful_docs']}")
    print(f"- 处理失败: {result['failed_docs']}")
    print(f"- 成功率: {result['success_rate']:.1f}%")
    print(f"- 处理时间: {result['processing_time']:.2f}秒")

async def test_checkpoint_recovery():
    """测试断点续传功能"""
    print("\n=== 测试断点续传功能 ===")
    
    # 创建测试文件
    test_files = create_test_files(15)
    
    # 创建批处理器
    processor = BatchProcessor(max_concurrent=2, checkpoint_interval=3)
    
    batch_id = "test_checkpoint_batch"
    
    try:
        # 模拟处理中断（处理一部分后停止）
        print("开始第一次处理（模拟中断）...")
        
        # 只处理前5个文件来模拟部分完成
        partial_files = test_files[:5]
        result1 = await processor.process_batch(partial_files, batch_id)
        
        print(f"第一次处理完成: {result1['successful_docs']}/{result1['total_docs']}")
        
        # 模拟从断点恢复
        print("\n从断点恢复处理...")
        result2 = await processor.process_batch(test_files, batch_id, resume=True)
        
        print(f"恢复处理结果:")
        print(f"- 总文档数: {result2['total_docs']}")
        print(f"- 成功处理: {result2['successful_docs']}")
        print(f"- 处理失败: {result2['failed_docs']}")
        print(f"- 成功率: {result2['success_rate']:.1f}%")
        
    except Exception as e:
        print(f"测试过程中出现错误: {e}")

def test_api_endpoints():
    """测试API接口"""
    print("\n=== 测试API接口 ===")
    
    # 启动API服务器（需要在另一个终端运行 python batch_api.py）
    base_url = "http://localhost:8000"
    
    try:
        # 测试启动批量处理
        test_files = create_test_files(5)
        
        response = requests.post(
            f"{base_url}/batch/process",
            json=test_files
        )
        
        if response.status_code == 200:
            result = response.json()
            batch_id = result.get('batch_id')
            print(f"批量处理任务已启动: {batch_id}")
            
            # 等待一段时间后查询状态
            time.sleep(2)
            
            status_response = requests.get(f"{base_url}/batch/status/{batch_id}")
            if status_response.status_code == 200:
                status = status_response.json()
                print(f"任务状态: {status}")
            else:
                print(f"获取状态失败: {status_response.text}")
        else:
            print(f"启动批量处理失败: {response.text}")
    
    except requests.exceptions.ConnectionError:
        print("无法连接到API服务器，请确保服务器正在运行")
        print("运行命令: python batch_api.py")

async def test_error_recovery():
    """测试错误恢复机制"""
    print("\n=== 测试错误恢复机制 ===")
    
    from batch_processor import ErrorRecoveryManager
    
    recovery_manager = ErrorRecoveryManager(max_retries=3, retry_delay=0.1)
    
    # 模拟会失败的函数
    attempt_count = 0
    async def failing_function():
        nonlocal attempt_count
        attempt_count += 1
        if attempt_count < 3:
            raise Exception(f"模拟失败 (尝试 {attempt_count})")
        return f"成功 (尝试 {attempt_count})"
    
    try:
        result = await recovery_manager.execute_with_retry(failing_function)
        print(f"错误恢复测试结果: {result}")
    except Exception as e:
        print(f"错误恢复失败: {e}")

async def main():
    """运行所有测试"""
    print("开始批量处理和断点续传功能测试")
    
    await test_basic_batch_processing()
    await test_checkpoint_recovery()
    await test_error_recovery()
    
    print("\n注意: API测试需要单独运行服务器")
    print("运行命令: python batch_api.py")
    print("然后运行: python -c 'from test_batch_processing import test_api_endpoints; test_api_endpoints()'")
    
    print("\n所有测试完成！")

if __name__ == "__main__":
    asyncio.run(main())
```

### 步骤4：创建Web界面（可选）

创建 `batch_web_interface.py` 文件：

```python
import streamlit as st
import requests
import time
import json
from pathlib import Path

# 配置页面
st.set_page_config(
    page_title="批量文档处理系统",
    page_icon="📄",
    layout="wide"
)

st.title("📄 批量文档处理系统")
st.markdown("---")

# API基础URL
API_BASE_URL = "http://localhost:8000"

# 侧边栏
st.sidebar.title("功能选择")
function_choice = st.sidebar.selectbox(
    "选择功能",
    ["文件上传处理", "批量处理管理", "任务状态监控"]
)

if function_choice == "文件上传处理":
    st.header("📤 文件上传处理")
    
    # 文件上传
    uploaded_files = st.file_uploader(
        "选择要处理的文件",
        accept_multiple_files=True,
        type=['txt', 'pdf', 'docx']
    )
    
    if uploaded_files:
        st.write(f"已选择 {len(uploaded_files)} 个文件:")
        for file in uploaded_files:
            st.write(f"- {file.name} ({file.size} bytes)")
        
        if st.button("开始批量处理"):
            try:
                # 准备文件数据
                files_data = []
                for file in uploaded_files:
                    files_data.append(('files', (file.name, file.read(), file.type)))
                
                # 发送请求
                response = requests.post(
                    f"{API_BASE_URL}/batch/upload",
                    files=files_data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    st.success(f"批量处理任务已启动！")
                    st.json(result)
                    
                    # 保存batch_id到session state
                    st.session_state.current_batch_id = result.get('batch_id')
                else:
                    st.error(f"启动失败: {response.text}")
            
            except Exception as e:
                st.error(f"处理过程中出现错误: {str(e)}")

elif function_choice == "批量处理管理":
    st.header("⚙️ 批量处理管理")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("启动新的批量处理")
        
        # 文件路径输入
        file_paths_text = st.text_area(
            "输入文件路径（每行一个）",
            height=150,
            placeholder="/path/to/file1.txt\n/path/to/file2.txt"
        )
        
        batch_id = st.text_input("批次ID（可选）", placeholder="留空自动生成")
        
        if st.button("启动批量处理"):
            if file_paths_text.strip():
                file_paths = [path.strip() for path in file_paths_text.strip().split('\n') if path.strip()]
                
                try:
                    data = file_paths
                    if batch_id:
                        # 如果提供了batch_id，需要使用不同的端点
                        pass
                    
                    response = requests.post(
                        f"{API_BASE_URL}/batch/process",
                        json=data
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        st.success("批量处理任务已启动！")
                        st.json(result)
                        st.session_state.current_batch_id = result.get('batch_id')
                    else:
                        st.error(f"启动失败: {response.text}")
                
                except Exception as e:
                    st.error(f"处理过程中出现错误: {str(e)}")
            else:
                st.warning("请输入至少一个文件路径")
    
    with col2:
        st.subheader("恢复批量处理")
        
        resume_batch_id = st.text_input("要恢复的批次ID")
        resume_file_paths_text = st.text_area(
            "原始文件路径列表",
            height=100,
            placeholder="/path/to/file1.txt\n/path/to/file2.txt"
        )
        
        if st.button("恢复处理"):
            if resume_batch_id and resume_file_paths_text.strip():
                file_paths = [path.strip() for path in resume_file_paths_text.strip().split('\n') if path.strip()]
                
                try:
                    response = requests.post(
                        f"{API_BASE_URL}/batch/resume/{resume_batch_id}",
                        json=file_paths
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        st.success("批量处理任务已恢复！")
                        st.json(result)
                    else:
                        st.error(f"恢复失败: {response.text}")
                
                except Exception as e:
                    st.error(f"恢复过程中出现错误: {str(e)}")
            else:
                st.warning("请输入批次ID和文件路径")

elif function_choice == "任务状态监控":
    st.header("📊 任务状态监控")
    
    # 自动刷新选项
    auto_refresh = st.checkbox("自动刷新 (5秒)", value=False)
    
    if auto_refresh:
        time.sleep(5)
        st.rerun()
    
    # 手动刷新按钮
    if st.button("🔄 刷新状态"):
        st.rerun()
    
    try:
        # 获取活跃任务列表
        response = requests.get(f"{API_BASE_URL}/batch/active")
        
        if response.status_code == 200:
            data = response.json()
            active_batches = data.get('active_batches', [])
            
            if active_batches:
                st.subheader(f"活跃任务 ({len(active_batches)} 个)")
                
                for batch in active_batches:
                    with st.expander(f"批次 {batch['batch_id'][:8]}..."):
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("总文档数", batch['total_docs'])
                            st.metric("已处理", batch['processed_docs'])
                        
                        with col2:
                            st.metric("处理失败", batch['failed_docs'])
                            st.metric("进度", f"{batch['progress_percent']:.1f}%")
                        
                        with col3:
                            st.metric("预计剩余时间", batch['eta'])
                        
                        # 进度条
                        progress = batch['progress_percent'] / 100
                        st.progress(progress)
                        
                        # 详细信息
                        st.json(batch)
            else:
                st.info("当前没有活跃的批量处理任务")
        else:
            st.error(f"获取任务状态失败: {response.text}")
    
    except requests.exceptions.ConnectionError:
        st.error("无法连接到API服务器，请确保服务器正在运行")
        st.code("python batch_api.py")
    
    # 单个任务状态查询
    st.markdown("---")
    st.subheader("查询特定任务状态")
    
    query_batch_id = st.text_input("输入批次ID")
    
    if st.button("查询状态") and query_batch_id:
        try:
            response = requests.get(f"{API_BASE_URL}/batch/status/{query_batch_id}")
            
            if response.status_code == 200:
                status = response.json()
                st.success("查询成功！")
                st.json(status)
            elif response.status_code == 404:
                st.warning("未找到指定的批次ID")
            else:
                st.error(f"查询失败: {response.text}")
        
        except Exception as e:
            st.error(f"查询过程中出现错误: {str(e)}")

# 页面底部信息
st.markdown("---")
st.markdown(
    """
    **使用说明:**
    1. 确保API服务器正在运行: `python batch_api.py`
    2. 上传文件或输入文件路径开始批量处理
    3. 使用任务状态监控查看处理进度
    4. 如果处理中断，可以使用恢复功能继续处理
    """
)
```

## 实验任务

### 任务1：基础功能测试
1. 运行基础批量处理测试
2. 观察处理进度和日志输出
3. 分析成功率和处理时间

### 任务2：断点续传测试
1. 启动一个批量处理任务
2. 在处理过程中手动中断
3. 使用相同的batch_id恢复处理
4. 验证已处理的文档不会重复处理

### 任务3：API接口测试
1. 启动FastAPI服务器
2. 使用API接口启动批量处理
3. 查询处理状态和进度
4. 测试恢复功能

### 任务4：Web界面测试（可选）
1. 启动Streamlit Web界面
2. 上传文件并启动处理
3. 实时监控处理进度
4. 测试任务管理功能

## 实验报告要求

请完成实验后，提交包含以下内容的实验报告：

1. **实验环境**：Python版本、依赖包版本等
2. **功能测试结果**：各个功能模块的测试结果截图
3. **性能分析**：不同并发数量下的处理性能对比
4. **错误处理**：测试各种异常情况的处理效果
5. **改进建议**：对系统的优化建议和扩展想法

## 思考题

1. **内存管理**：如何避免批量处理大文件时的内存溢出？
2. **分布式处理**：如何将批量处理扩展到多台服务器？
3. **优先级调度**：如何实现不同优先级的批量处理任务？
4. **监控告警**：如何设计完善的监控和告警机制？
5. **数据一致性**：在分布式环境下如何保证断点续传的数据一致性？

## 扩展练习

1. **集成到RAG系统**：将批量处理功能集成到现有的RAG系统中
2. **添加文件类型支持**：支持更多文件格式的批量处理
3. **实现任务队列**：使用Redis或RabbitMQ实现任务队列
4. **添加用户权限**：实现多用户的批量处理任务管理
5. **性能优化**：优化处理算法，提高处理速度