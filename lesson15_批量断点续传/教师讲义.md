# 第15节课：批量/断点续传

## 课程信息
- **课程时长**：45分钟
- **课程目标**：实现大规模文档的批量处理与断点续传，提高系统稳定性
- **前置知识**：Python异步编程、FastAPI基础、文件处理

## 1. 课程概述（5分钟）

### 1.1 为什么需要批量处理和断点续传？
- **大规模数据处理挑战**：单个文档处理vs批量处理的效率差异
- **系统稳定性问题**：网络中断、服务重启、内存不足等场景
- **用户体验考虑**：长时间处理任务的进度反馈和恢复能力

### 1.2 本节课要解决的问题
- 如何高效处理大量文档
- 如何在处理中断后继续未完成的任务
- 如何跟踪处理进度并提供用户反馈
- 如何设计错误恢复机制

## 2. 技术原理讲解（15分钟）

### 2.1 异步批量处理架构
```python
# 核心概念示例
import asyncio
from fastapi import BackgroundTasks

# 异步处理单个文档
async def process_document(doc_id: str):
    # 模拟文档处理时间
    await asyncio.sleep(0.1)
    return f"Processed {doc_id}"

# 批量处理管理器
async def batch_process_manager(doc_ids: list):
    # 控制并发数量，避免资源耗尽
    semaphore = asyncio.Semaphore(10)  # 最多10个并发
    
    async def process_with_semaphore(doc_id):
        async with semaphore:
            return await process_document(doc_id)
    
    # 创建任务并等待完成
    tasks = [process_with_semaphore(doc_id) for doc_id in doc_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

### 2.2 断点续传机制设计
```python
# 进度状态管理
class ProcessingState:
    def __init__(self, batch_id: str):
        self.batch_id = batch_id
        self.total_docs = 0
        self.processed_docs = []
        self.failed_docs = []
        self.current_progress = 0
    
    def save_checkpoint(self):
        """保存当前处理状态到持久化存储"""
        state_data = {
            'batch_id': self.batch_id,
            'total_docs': self.total_docs,
            'processed_docs': self.processed_docs,
            'failed_docs': self.failed_docs,
            'current_progress': self.current_progress
        }
        # 保存到文件或数据库
        with open(f'checkpoints/{self.batch_id}.json', 'w') as f:
            json.dump(state_data, f)
    
    @classmethod
    def load_checkpoint(cls, batch_id: str):
        """从持久化存储加载处理状态"""
        try:
            with open(f'checkpoints/{batch_id}.json', 'r') as f:
                state_data = json.load(f)
            state = cls(batch_id)
            state.total_docs = state_data['total_docs']
            state.processed_docs = state_data['processed_docs']
            state.failed_docs = state_data['failed_docs']
            state.current_progress = state_data['current_progress']
            return state
        except FileNotFoundError:
            return cls(batch_id)
```

### 2.3 进度跟踪与日志记录
```python
import logging
from datetime import datetime

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_processing.log'),
        logging.StreamHandler()
    ]
)

class ProgressTracker:
    def __init__(self, total_items: int):
        self.total_items = total_items
        self.completed_items = 0
        self.failed_items = 0
        self.start_time = datetime.now()
        self.logger = logging.getLogger(__name__)
    
    def update_progress(self, success: bool = True):
        if success:
            self.completed_items += 1
        else:
            self.failed_items += 1
        
        progress_percent = (self.completed_items + self.failed_items) / self.total_items * 100
        self.logger.info(f"Progress: {progress_percent:.1f}% ({self.completed_items}/{self.total_items})")
    
    def get_eta(self):
        """估算剩余时间"""
        if self.completed_items == 0:
            return "Unknown"
        
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.completed_items / elapsed
        remaining_items = self.total_items - self.completed_items - self.failed_items
        eta_seconds = remaining_items / rate if rate > 0 else 0
        return f"{eta_seconds:.0f} seconds"
```

### 2.4 错误恢复策略
```python
class ErrorRecoveryManager:
    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.logger = logging.getLogger(__name__)
    
    async def execute_with_retry(self, func, *args, **kwargs):
        """带重试机制的函数执行"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                self.logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                
                if attempt < self.max_retries:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))  # 指数退避
                else:
                    self.logger.error(f"All {self.max_retries + 1} attempts failed")
                    raise last_exception
```

## 3. 实际应用场景（10分钟）

### 3.1 RAG系统中的批量文档处理
- **文档上传场景**：用户一次性上传大量PDF、Word文档
- **定期更新场景**：从外部数据源同步最新文档
- **系统迁移场景**：从旧系统迁移历史文档数据

### 3.2 性能优化考虑
- **内存管理**：避免同时加载过多文档到内存
- **并发控制**：合理设置并发数量，平衡效率和资源消耗
- **资源监控**：实时监控CPU、内存、磁盘使用情况

### 3.3 用户体验设计
- **实时进度显示**：WebSocket推送处理进度
- **任务管理界面**：查看历史任务、重启失败任务
- **错误报告**：详细的错误信息和处理建议

## 4. 最佳实践与注意事项（10分钟）

### 4.1 设计原则
1. **幂等性**：重复执行相同操作应该产生相同结果
2. **可观测性**：充分的日志记录和监控指标
3. **容错性**：优雅处理各种异常情况
4. **可扩展性**：支持水平扩展和负载均衡

### 4.2 常见陷阱
- **内存泄漏**：未正确释放处理完的文档对象
- **死锁问题**：不当的锁使用导致任务卡死
- **状态不一致**：并发更新导致的数据竞争
- **资源耗尽**：无限制的并发导致系统崩溃

### 4.3 监控指标
- **处理速度**：每秒处理文档数量
- **成功率**：成功处理的文档比例
- **资源使用**：CPU、内存、磁盘IO使用率
- **错误分布**：不同类型错误的发生频率

## 5. 课堂总结（5分钟）

### 5.1 核心要点回顾
- 异步批量处理提高系统吞吐量
- 断点续传机制保证任务可恢复性
- 进度跟踪提升用户体验
- 错误恢复策略增强系统稳定性

### 5.2 下节课预告
下节课将学习"增量更新"，探讨如何高效处理文档变更，避免重复处理未修改的内容。

### 5.3 课后思考
1. 在你的实际项目中，哪些场景需要批量处理和断点续传？
2. 如何设计一个通用的批量处理框架，支持不同类型的任务？
3. 在分布式环境下，如何实现跨节点的断点续传？

## 6. 参考资料
- [Python asyncio官方文档](https://docs.python.org/3/library/asyncio.html)
- [FastAPI Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/)
- [分布式系统设计模式](https://microservices.io/patterns/)
- [系统可靠性工程](https://sre.google/books/)